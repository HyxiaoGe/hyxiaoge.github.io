<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Go 实战入门（一）：从零搭建 Web API 项目</title>
      <link href="/2025/11/12/go-shi-zhan-ru-men-yi-cong-ling-da-jian-web-api-xiang-mu/"/>
      <url>/2025/11/12/go-shi-zhan-ru-men-yi-cong-ling-da-jian-web-api-xiang-mu/</url>
      
        <content type="html"><![CDATA[<h1 id="Go-实战入门（一）：从零搭建-Web-API-项目"><a href="#Go-实战入门（一）：从零搭建-Web-API-项目" class="headerlink" title="Go 实战入门（一）：从零搭建 Web API 项目"></a>Go 实战入门（一）：从零搭建 Web API 项目</h1><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>作为一名后端开发者，学习一门新语言最痛苦的莫过于——看完语法教程，依然不知道怎么写项目。</p><p>本系列选择另一条路：<strong>直接从实战项目入手</strong>，在做项目的过程中学习 Go。不讲语法细节，只关注工程实践。</p><p>本篇是系列第一篇，目标是搭建一个完整的 Web API 项目骨架，包含：配置管理、日志系统、数据库操作、HTTP 路由、JWT 认证等核心模块。</p><p>完整代码见：<a href="https://github.com/HyxiaoGe/go-practical-roadmap">go-practical-roadmap</a></p><hr><h2 id="二、项目结构"><a href="#二、项目结构" class="headerlink" title="二、项目结构"></a>二、项目结构</h2><p>Go 项目没有强制的目录规范，但社区有一套约定俗成的布局：</p><pre><code>01-web-api-template/├── cmd/server/main.go        # 程序入口├── internal/                 # 内部包（不对外暴露）│   ├── api/                  # HTTP 处理层│   ├── service/              # 业务逻辑层│   ├── repository/           # 数据访问层│   ├── model/                # 数据模型│   ├── config/               # 配置定义│   ├── middleware/           # 中间件│   └── app/                  # 应用组装├── pkg/                      # 可复用的公共包│   ├── logger/               # 日志封装│   └── db/                   # 数据库封装├── configs/config.yaml       # 配置文件└── Makefile                  # 构建脚本</code></pre><p>几个关键点：</p><ul><li><strong>cmd&#x2F;</strong> 放可执行程序入口，每个子目录对应一个二进制文件</li><li><strong>internal&#x2F;</strong> 是 Go 的特殊目录，里面的包无法被外部项目导入</li><li><strong>pkg&#x2F;</strong> 放可以被其他项目复用的代码</li></ul><hr><h2 id="三、程序入口与生命周期"><a href="#三、程序入口与生命周期" class="headerlink" title="三、程序入口与生命周期"></a>三、程序入口与生命周期</h2><h3 id="main-go"><a href="#main-go" class="headerlink" title="main.go"></a>main.go</h3><pre class="line-numbers language-language-go"><code class="language-language-go">package mainimport (    "fmt"    "log"    "go-practical-roadmap/01-web-api-template/internal/app")func main() &#123;    // 创建应用实例    application, err := app.NewApp()    if err != nil &#123;        log.Fatalf("Failed to create app: %v", err)    &#125;    // 在 goroutine 中启动应用    go func() &#123;        if err := application.Run(); err != nil &#123;            log.Fatalf("Failed to run app: %v", err)        &#125;    &#125;()    fmt.Println("Server is running on :8080")    // 等待中断信号    application.WaitForInterrupt()    // 停止应用    if err := application.Stop(); err != nil &#123;        log.Fatalf("Failed to stop app: %v", err)    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="为什么需要显式阻塞？"><a href="#为什么需要显式阻塞？" class="headerlink" title="为什么需要显式阻塞？"></a>为什么需要显式阻塞？</h3><p>注意 <code>application.WaitForInterrupt()</code> 这行。Go 有个特性：<strong>main 函数结束 &#x3D; 程序退出</strong>，不管还有多少 goroutine 在运行。</p><p>所以必须用某种方式”挡住” main 函数，否则 HTTP 服务器刚启动程序就退出了。</p><p><code>WaitForInterrupt</code> 的实现很简单：</p><pre class="line-numbers language-language-go"><code class="language-language-go">func (a *App) WaitForInterrupt() &#123;    quit := make(chan os.Signal, 1)    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)    <-quit  // 阻塞，直到收到 Ctrl+C&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="优雅停机"><a href="#优雅停机" class="headerlink" title="优雅停机"></a>优雅停机</h3><p>收到停止信号后，不能直接退出，要给正在处理的请求一个收尾的机会：</p><pre class="line-numbers language-language-go"><code class="language-language-go">func (a *App) Stop() error &#123;    // 创建 30 秒超时的 context    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)    defer cancel()    // 优雅关闭 HTTP 服务（等待请求处理完）    if err := a.server.Shutdown(ctx); err != nil &#123;        return err    &#125;    // 关闭数据库连接    db.Close()    // 刷新日志缓冲    logger.GlobalLogger.Sync()    return nil&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>context.WithTimeout</code> 是 Go 的超时控制机制。<code>Shutdown</code> 会等待所有请求处理完毕，但最多等 30 秒，超时就强制关闭。</p><hr><h2 id="四、配置管理（Viper）"><a href="#四、配置管理（Viper）" class="headerlink" title="四、配置管理（Viper）"></a>四、配置管理（Viper）</h2><p>Viper 是 Go 最流行的配置库，支持 YAML、JSON、环境变量等多种来源。</p><h3 id="配置结构体"><a href="#配置结构体" class="headerlink" title="配置结构体"></a>配置结构体</h3><pre class="line-numbers language-language-go"><code class="language-language-go">type Config struct &#123;    Server   ServerConfig   `mapstructure:"server"`    Database DatabaseConfig `mapstructure:"database"`    JWT      JWTConfig      `mapstructure:"jwt"`    Logger   LoggerConfig   `mapstructure:"logger"`&#125;type ServerConfig struct &#123;    Port int    `mapstructure:"port"`    Host string `mapstructure:"host"`    Mode string `mapstructure:"mode"`&#125;type DatabaseConfig struct &#123;    Driver       string `mapstructure:"driver"`    DSN          string `mapstructure:"dsn"`    MaxIdleConns int    `mapstructure:"max_idle_conns"`    MaxOpenConns int    `mapstructure:"max_open_conns"`&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>mapstructure</code> 标签告诉 Viper 如何将 YAML 字段映射到结构体。</p><h3 id="加载配置"><a href="#加载配置" class="headerlink" title="加载配置"></a>加载配置</h3><pre class="line-numbers language-language-go"><code class="language-language-go">func LoadConfig() (*Config, error) &#123;    viper.SetConfigName("config")    viper.SetConfigType("yaml")    viper.AddConfigPath("./configs")    // 设置默认值    viper.SetDefault("server.port", 8080)    viper.SetDefault("database.max_idle_conns", 10)    // 环境变量自动覆盖（前缀 APP_）    viper.SetEnvPrefix("APP")    viper.AutomaticEnv()    viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))    // 读取配置文件    if err := viper.ReadInConfig(); err != nil &#123;        return nil, err    &#125;    // 反序列化到结构体    var config Config    if err := viper.Unmarshal(&config); err != nil &#123;        return nil, err    &#125;    return &config, nil&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对应的 <code>config.yaml</code>：</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">server:  port: 8080  host: localhost  mode: debugdatabase:  driver: sqlite  dsn: ./data/app.db  max_idle_conns: 10  max_open_conns: 100jwt:  secret: your-secret-key  access_token_exp: 3600<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>环境变量 <code>APP_DATABASE_DSN</code> 会自动覆盖 <code>database.dsn</code> 的值，方便在不同环境部署。</p><hr><h2 id="五、日志系统（Zap）"><a href="#五、日志系统（Zap）" class="headerlink" title="五、日志系统（Zap）"></a>五、日志系统（Zap）</h2><p>Zap 是 Uber 开源的高性能日志库，特点是零内存分配和结构化日志。</p><h3 id="初始化日志"><a href="#初始化日志" class="headerlink" title="初始化日志"></a>初始化日志</h3><pre class="line-numbers language-language-go"><code class="language-language-go">func NewLogger(level string, format string, outputPath string) (Logger, error) &#123;    // 解析日志级别    var zapLevel zapcore.Level    switch level &#123;    case "debug":        zapLevel = zap.DebugLevel    case "info":        zapLevel = zap.InfoLevel    case "error":        zapLevel = zap.ErrorLevel    default:        zapLevel = zap.InfoLevel    &#125;    // 配置编码器    encoderConfig := zap.NewProductionEncoderConfig()    encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder    var encoder zapcore.Encoder    if format == "json" &#123;        encoder = zapcore.NewJSONEncoder(encoderConfig)    &#125; else &#123;        encoder = zapcore.NewConsoleEncoder(encoderConfig)    &#125;    // 配置输出（支持日志轮转）    var output zapcore.WriteSyncer    if outputPath == "stdout" &#123;        output = zapcore.AddSync(os.Stdout)    &#125; else &#123;        output = zapcore.AddSync(&lumberjack.Logger&#123;            Filename:   outputPath,            MaxSize:    100,  // MB            MaxBackups: 10,            MaxAge:     30,   // days        &#125;)    &#125;    core := zapcore.NewCore(encoder, output, zapLevel)    logger := zap.New(core, zap.AddCaller())    return &zapLogger&#123;logger: logger&#125;, nil&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><pre class="line-numbers language-language-go"><code class="language-language-go">// 结构化日志logger.Info("User login",    zap.String("username", "sean"),    zap.Int("user_id", 123),    zap.Duration("latency", time.Millisecond*50))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出（JSON 格式）：</p><pre class="line-numbers language-language-json"><code class="language-language-json">&#123;"level":"INFO","ts":"2024-01-01T12:00:00Z","caller":"api/routes.go:42","msg":"User login","username":"sean","user_id":123,"latency":"50ms"&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>结构化日志的好处是方便后续用 ELK、Loki 等工具分析检索。</p><hr><h2 id="六、数据库（Gorm）"><a href="#六、数据库（Gorm）" class="headerlink" title="六、数据库（Gorm）"></a>六、数据库（Gorm）</h2><p>Gorm 是 Go 最流行的 ORM，支持 MySQL、PostgreSQL、SQLite 等数据库。</p><h3 id="连接数据库"><a href="#连接数据库" class="headerlink" title="连接数据库"></a>连接数据库</h3><pre class="line-numbers language-language-go"><code class="language-language-go">var globalDB *gorm.DBfunc Connect(dsn string, driver string) error &#123;    var err error    switch driver &#123;    case "mysql":        globalDB, err = gorm.Open(mysql.Open(dsn), &gorm.Config&#123;&#125;)    case "postgres":        globalDB, err = gorm.Open(postgres.Open(dsn), &gorm.Config&#123;&#125;)    case "sqlite":        globalDB, err = gorm.Open(sqlite.Open(dsn), &gorm.Config&#123;&#125;)    &#125;    if err != nil &#123;        return err    &#125;    // 配置连接池    sqlDB, _ := globalDB.DB()    sqlDB.SetMaxIdleConns(10)    sqlDB.SetMaxOpenConns(100)    sqlDB.SetConnMaxLifetime(time.Hour)    return nil&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Model-定义"><a href="#Model-定义" class="headerlink" title="Model 定义"></a>Model 定义</h3><pre class="line-numbers language-language-go"><code class="language-language-go">type User struct &#123;    ID        uint           `gorm:"primaryKey" json:"id"`    CreatedAt time.Time      `json:"created_at"`    UpdatedAt time.Time      `json:"updated_at"`    DeletedAt gorm.DeletedAt `gorm:"index" json:"deleted_at,omitempty"`    Username  string         `gorm:"uniqueIndex;size:50;not null" json:"username"`    Email     string         `gorm:"uniqueIndex;size:100;not null" json:"email"`    Password  string         `gorm:"size:255;not null" json:"-"`    IsActive  bool           `gorm:"default:true" json:"is_active"`&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>几个要点：</p><ul><li><code>gorm</code> 标签定义数据库约束（主键、索引、长度等）</li><li><code>json</code> 标签定义 JSON 序列化行为，<code>json:&quot;-&quot;</code> 表示忽略该字段</li><li><code>gorm.DeletedAt</code> 实现软删除，删除记录时只更新该字段</li></ul><h3 id="Repository-层"><a href="#Repository-层" class="headerlink" title="Repository 层"></a>Repository 层</h3><pre class="line-numbers language-language-go"><code class="language-language-go">type UserRepository interface &#123;    Create(user *model.User) error    GetByID(id uint) (*model.User, error)    GetByUsername(username string) (*model.User, error)    Update(user *model.User) error    Delete(id uint) error&#125;type userRepository struct &#123;    db *gorm.DB&#125;func NewUserRepository(db *gorm.DB) UserRepository &#123;    return &userRepository&#123;db: db&#125;&#125;func (r *userRepository) Create(user *model.User) error &#123;    return r.db.Create(user).Error&#125;func (r *userRepository) GetByUsername(username string) (*model.User, error) &#123;    var user model.User    err := r.db.Where("username = ?", username).First(&user).Error    if err != nil &#123;        return nil, err    &#125;    return &user, nil&#125;func (r *userRepository) Delete(id uint) error &#123;    return r.db.Delete(&model.User&#123;&#125;, id).Error&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接口 + 实现分离的设计，方便单元测试时 mock。</p><hr><h2 id="七、HTTP-服务（Gin）"><a href="#七、HTTP-服务（Gin）" class="headerlink" title="七、HTTP 服务（Gin）"></a>七、HTTP 服务（Gin）</h2><p>Gin 是 Go 最流行的 Web 框架，性能优异且 API 简洁。</p><h3 id="路由注册"><a href="#路由注册" class="headerlink" title="路由注册"></a>路由注册</h3><pre class="line-numbers language-language-go"><code class="language-language-go">func SetupRoutes(userService service.UserService) *gin.Engine &#123;    r := gin.New()    // 内置中间件    r.Use(gin.Logger())    r.Use(gin.Recovery())    // 自定义中间件    r.Use(middleware.RequestTracerMiddleware())    r.Use(middleware.CORSMiddleware())    // 公开路由    r.GET("/health", healthCheck)    r.POST("/api/v1/register", func(c *gin.Context) &#123;        registerHandler(c, userService)    &#125;)    r.POST("/api/v1/login", func(c *gin.Context) &#123;        loginHandler(c, userService)    &#125;)    // 需要认证的路由组    authorized := r.Group("/")    authorized.Use(middleware.JWTAuthMiddleware)    &#123;        authorized.GET("/api/v1/profile", profileHandler)    &#125;    return r&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h3><pre class="line-numbers language-language-go"><code class="language-language-go">func registerHandler(c *gin.Context, userService service.UserService) &#123;    var req dto.RegisterRequest    // 绑定 JSON 请求体到结构体    if err := c.ShouldBindJSON(&req); err != nil &#123;        c.JSON(http.StatusBadRequest, gin.H&#123;"error": err.Error()&#125;)        return    &#125;    // 调用业务层    user, err := userService.Register(&req)    if err != nil &#123;        c.JSON(http.StatusBadRequest, gin.H&#123;"error": err.Error()&#125;)        return    &#125;    // 返回响应    c.JSON(http.StatusCreated, gin.H&#123;        "message": "User registered successfully",        "data":    user,    &#125;)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>c.ShouldBindJSON</code> 会自动解析请求体并校验参数（配合 validator 标签），省去大量样板代码。</p><hr><h2 id="八、中间件"><a href="#八、中间件" class="headerlink" title="八、中间件"></a>八、中间件</h2><h3 id="JWT-认证"><a href="#JWT-认证" class="headerlink" title="JWT 认证"></a>JWT 认证</h3><pre class="line-numbers language-language-go"><code class="language-language-go">type Claims struct &#123;    UserID   uint   `json:"user_id"`    Username string `json:"username"`    jwt.RegisteredClaims&#125;// 生成 Tokenfunc GenerateToken(userID uint, username string) (string, error) &#123;    expirationTime := time.Now().Add(time.Duration(config.GlobalConfig.JWT.AccessTokenExp) * time.Second)    claims := &Claims&#123;        UserID:   userID,        Username: username,        RegisteredClaims: jwt.RegisteredClaims&#123;            ExpiresAt: jwt.NewNumericDate(expirationTime),            IssuedAt:  jwt.NewNumericDate(time.Now()),        &#125;,    &#125;    token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)    return token.SignedString([]byte(config.GlobalConfig.JWT.Secret))&#125;// 认证中间件func JWTAuthMiddleware(c *gin.Context) &#123;    authHeader := c.GetHeader("Authorization")    if authHeader == "" &#123;        c.JSON(http.StatusUnauthorized, gin.H&#123;"error": "Missing Authorization header"&#125;)        c.Abort()        return    &#125;    if !strings.HasPrefix(authHeader, "Bearer ") &#123;        c.JSON(http.StatusUnauthorized, gin.H&#123;"error": "Invalid Authorization format"&#125;)        c.Abort()        return    &#125;    tokenString := strings.TrimPrefix(authHeader, "Bearer ")    claims, err := ValidateToken(tokenString)    if err != nil &#123;        c.JSON(http.StatusUnauthorized, gin.H&#123;"error": "Invalid token"&#125;)        c.Abort()        return    &#125;    // 将用户信息存入 context，后续 handler 可以取用    c.Set("user_id", claims.UserID)    c.Set("username", claims.Username)    c.Next()&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>c.Abort()</code> 会终止后续中间件和 handler 的执行，直接返回响应。</p><hr><h2 id="九、业务分层"><a href="#九、业务分层" class="headerlink" title="九、业务分层"></a>九、业务分层</h2><h3 id="Service-层"><a href="#Service-层" class="headerlink" title="Service 层"></a>Service 层</h3><pre class="line-numbers language-language-go"><code class="language-language-go">type UserService interface &#123;    Register(req *dto.RegisterRequest) (*dto.UserProfileResponse, error)    Login(req *dto.LoginRequest) (string, error)&#125;type userService struct &#123;    userRepo repository.UserRepository&#125;func NewUserService(userRepo repository.UserRepository) UserService &#123;    return &userService&#123;userRepo: userRepo&#125;&#125;func (s *userService) Register(req *dto.RegisterRequest) (*dto.UserProfileResponse, error) &#123;    // 检查用户名是否已存在    if _, err := s.userRepo.GetByUsername(req.Username); err == nil &#123;        return nil, errors.New("username already exists")    &#125;    // 密码加密    hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost)    if err != nil &#123;        return nil, err    &#125;    // 创建用户    user := &model.User&#123;        Username: req.Username,        Email:    req.Email,        Password: string(hashedPassword),    &#125;    if err := s.userRepo.Create(user); err != nil &#123;        return nil, err    &#125;    // 返回 DTO（不含密码）    return &dto.UserProfileResponse&#123;        ID:       user.ID,        Username: user.Username,        Email:    user.Email,    &#125;, nil&#125;func (s *userService) Login(req *dto.LoginRequest) (string, error) &#123;    user, err := s.userRepo.GetByUsername(req.Username)    if err != nil &#123;        return "", errors.New("invalid username or password")    &#125;    if err := bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(req.Password)); err != nil &#123;        return "", errors.New("invalid username or password")    &#125;    return middleware.GenerateToken(user.ID, user.Username)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="分层职责"><a href="#分层职责" class="headerlink" title="分层职责"></a>分层职责</h3><table><thead><tr><th>层</th><th>职责</th></tr></thead><tbody><tr><td>API&#x2F;Handler</td><td>处理 HTTP 请求响应，参数校验</td></tr><tr><td>Service</td><td>业务逻辑，事务控制</td></tr><tr><td>Repository</td><td>数据访问，SQL 操作</td></tr><tr><td>Model</td><td>数据库实体</td></tr><tr><td>DTO</td><td>接口传输对象，与 Model 分离</td></tr></tbody></table><p>DTO 和 Model 分离的好处：</p><ul><li>接口返回的字段可以灵活控制，不暴露敏感信息（如密码）</li><li>数据库结构变化不影响 API 契约</li></ul><hr><h2 id="十、小结"><a href="#十、小结" class="headerlink" title="十、小结"></a>十、小结</h2><p>本篇完成了一个 Web API 项目的基础骨架：</p><ul><li><strong>程序入口</strong>：理解 Go 的启动机制和优雅停机</li><li><strong>配置管理</strong>：使用 Viper 加载 YAML 配置和环境变量</li><li><strong>日志系统</strong>：使用 Zap 输出结构化日志</li><li><strong>数据库</strong>：使用 Gorm 定义 Model 和 Repository</li><li><strong>HTTP 服务</strong>：使用 Gin 注册路由和处理请求</li><li><strong>中间件</strong>：实现 JWT 认证</li><li><strong>业务分层</strong>：Service 层处理业务逻辑</li></ul>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> Web </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型时代：从概念到未来</title>
      <link href="/2025/09/07/da-mo-xing-shi-dai-cong-gai-nian-dao-wei-lai/"/>
      <url>/2025/09/07/da-mo-xing-shi-dai-cong-gai-nian-dao-wei-lai/</url>
      
        <content type="html"><![CDATA[<p>简单来说，大模型（<strong>Large Language Model, LLM</strong>）是一种基于海量数据训练的、参数规模及其庞大的深度学习模型。它本质上是一个极其复杂的“概率预测机器”，但其表现出的能力已经远远超出了简单的预测，更像是一个能够理解、生成和推理知识的通用信息处理系统。</p><h2 id="1-核心定义：是什么？"><a href="#1-核心定义：是什么？" class="headerlink" title="1. 核心定义：是什么？"></a>1. 核心定义：是什么？</h2><ul><li><strong>“大”的核心在于参数（Parameters）</strong>：这里的“大”主要指模型的参数数量极其庞大（通常是数十亿、数百亿甚至万亿级别），如7B，30B，250B等等（这里的B指的是Billion，也就是十亿）。参数可以理解为模型从数据中学到的“内部知识”或“连接强度”。参数越多，模型能存储和理解的模式就越复杂、越细微。</li><li><strong>“模型”的核心是神经网络（Neural Network）</strong>：大模型通常基于<strong>Transformer</strong>架构（尤其是其中的“注意力机制”）。这个架构让它能够高效地处理序列数据（如文本），并理解上下文中不同词汇之间的长远依赖关系。</li></ul><h2 id="2-核心能力：能做什么？"><a href="#2-核心能力：能做什么？" class="headerlink" title="2. 核心能力：能做什么？"></a>2. 核心能力：能做什么？</h2><p>大模型的核心能力可以概括为以下几个“理解”与“生成”：</p><ul><li><strong>自然语言理解（NLU）</strong>：阅读一段文字，不仅能明白字面意思，还能理解其背后的意图、情感和隐喻。例如，理解一句反讽或笑话。</li><li><strong>自然语言生成（NLG）</strong>：根据给定的提示（Prompt），生成流畅、连贯且符合逻辑和语境的新文本。这包括写文章、编故事、写邮件、写代码等。</li><li><strong>知识存储与回忆</strong>：在训练过程中，模型将海量知识以参数的形式“记忆”下来。当被问及时，它能够回忆并组织这些知识来回答问题，像一个强大的知识库。</li><li><strong>逻辑推理与思维链（Chain-of-Thought）</strong>：能够进行一定程度的逻辑推理、数学运算和常识推理。通过“一步一步地想”的提示方式，它可以展现出更复杂的推理能力。</li><li><strong>代码理解与生成</strong>：许多大模型在代码数据上进行了训练，使其能够编写、解释、调试和翻译不同编程语言的代码。</li></ul><h2 id="3-工作原理：如何做到的？"><a href="#3-工作原理：如何做到的？" class="headerlink" title="3. 工作原理：如何做到的？"></a>3. 工作原理：如何做到的？</h2><p>其核心工作原理是 <strong>“基于上下文预测下一个词”</strong>。</p><ul><li><strong>训练阶段</strong>：模型通过阅读数以万亿计的单词（来自互联网文本、书籍、代码等），不断玩一个“填空游戏”：给定前面所有的词，预测下一个最可能出现的词是什么。通过无数次这样的预测和调整，模型逐渐学会了语法、句法、事实知识以及世界运行的复杂模式。</li><li><strong>推理&#x2F;生成阶段</strong>：当你输入一个提示（例如：“法国的首都是”）时，模型会根据它学到的模式，计算出概率最高的下一个词是“巴黎”。然后，它会把“巴黎”作为新输入的一部分，继续预测下一个词，如此循环，直到生成完整的回答。</li></ul><p>这个过程看似简单，但因为模型的参数规模巨大，它捕捉到的模式极其复杂和抽象，从而涌现出了令人惊叹的能力。</p><h2 id="4-训练过程是怎样的？"><a href="#4-训练过程是怎样的？" class="headerlink" title="4. 训练过程是怎样的？"></a>4. 训练过程是怎样的？</h2><p>大模型的训练是一个<strong>海量数据、超强算力和复杂算法</strong>紧密结合的迭代过程。可以简化为以下几个核心步骤：</p><ol><li><strong>初始化</strong>：模型一开始是一个“新生儿”，它的参数（可以理解为脑细胞之间的连接强度）是随机设置的，没有任何知识。</li><li><strong>前向传播</strong>：从训练数据中取出一小批（Batch）文本（例如，“法国的首都是”），输入到模型中。模型根据当前参数，计算并输出一个预测结果（例如，它可能预测下一个词是“伦敦”）。</li><li><strong>计算损失</strong>：将模型的预测结果（“伦敦”）与正确答案（“巴黎”）进行对比，通过一个叫“损失函数”的指标来计算预测的误差有多大。误差越大，说明模型当前的表现越差。</li><li><strong>反向传播</strong>：这是学习的核心步骤。算法会从输出层开始，反向逐层计算每个参数对最终误差应负多少“责任”。</li><li><strong>参数更新</strong>：使用“优化器”算法（如Adam），根据每个参数的“责任”大小，对它们进行微调，以减少下一次预测的误差。这就好比老师批改作业，告诉你错在哪里，如何改正。</li><li><strong>循环迭代</strong>：重复步骤2-5<strong>数十万甚至数百万次</strong>，使用海量的数据批次。每一次迭代，模型都在进行微小的调整和学习，直到最终预测变得非常准确，损失降到很低。</li></ol><p>这个过程需要在一个拥有成千上万张顶级GPU&#x2F;TPU的超级计算机集群上运行数周甚至数月。</p><h2 id="5-所谓的“喂”数据是什么？"><a href="#5-所谓的“喂”数据是什么？" class="headerlink" title="5. 所谓的“喂”数据是什么？"></a>5. 所谓的“喂”数据是什么？</h2><p>“喂”数据绝非人工选择单个文件，而是一个<strong>高度自动化、工业化</strong>的流程。数据来源是混合的，主要包括：</p><ul><li><strong>网络爬虫（Web Crawling）</strong>： 这是数据的主要来源（可占70-80%）。团队会使用强大的爬虫系统（如Common Crawl）抓取整个互联网上的公开网页、文章、论坛帖子等，形成一个原始数据湖（Data Lake）。这提供了知识的广度和语言的多样性。</li><li><strong>授权或开源数据集</strong>：使用已有的高质量文本库，如：<ul><li>书籍数据库（如Project Gutenberg）：提供高质量、长逻辑的文本。</li><li>学术论文库（arXiv）: 提供专业、严谨的知识。</li><li>代码仓库（如Github）：用于训练模型的代码能力。</li></ul></li><li><strong>合成数据</strong>：模型自己生成的数据，用于特定目的的微调。</li></ul><h2 id="6-如何决定“喂”哪些数据？——-数据清洗与过滤"><a href="#6-如何决定“喂”哪些数据？——-数据清洗与过滤" class="headerlink" title="6. 如何决定“喂”哪些数据？—— 数据清洗与过滤"></a>6. 如何决定“喂”哪些数据？—— 数据清洗与过滤</h2><p>原始网络数据充满了垃圾信息、重复内容、偏见和有害内容，绝不能直接使用。决定过程是一个多步骤的“数据清洗”管道（Data Pipeline）：</p><ol><li><strong>去重</strong>：删除完全重复或高度相似的内容，防止模型对某些内容产生过拟合（记忆而非理解）。</li><li><strong>基于质量的过滤</strong>：<ul><li><strong>启发式规则</strong>：过滤掉垃圾内容、弹出内容文本、攻击性语言、大量乱码等。</li><li><strong>模型打分</strong>：训练一个小的分类器模型，来预测“一段文本的质量高低”。高质量的文本通常来自维基百科、知名新闻媒体、高质量博客等。低分内容会被丢弃。</li></ul></li><li><strong>基于内容的过滤</strong>：坚决删除涉及极端暴力、色情、仇恨言论、非法活动等明显有害的内容。</li><li><strong>多样性保障</strong>：<ul><li>需要确保数据覆盖不同的主题、语言风格、文化背景，防止模型产生偏见（Bias），让它能成为一个通用的助手，而不是只代表某一类群体的观点。</li></ul></li><li><strong>隐私保护</strong>：过滤掉包含个人身份信息（PII）的数据，如电话号码、邮箱、地址等。</li></ol><h2 id="7-如何保证“喂”的数据的正确性？"><a href="#7-如何保证“喂”的数据的正确性？" class="headerlink" title="7. 如何保证“喂”的数据的正确性？"></a>7. 如何保证“喂”的数据的正确性？</h2><p>首先，我们需要明白：**绝对100%的正确性是无法保证的。**但市面的模型厂商都是竭尽所能逼近这个目标。</p><ol><li><p><strong>依赖高质量信源</strong>：清洗管道会倾向于保留来自权威、可信来源（如百科全书、教科书、经审核的新闻机构、同行评议的科学文章）的数据。这些数据正确的概率远高于随机网友的评论。</p></li><li><p><strong>“多数共识”原则</strong>：互联网上的知识虽然有不一致，但对于事实性知识（如“水的化学式是H₂O”），正确信息会以极高的频率出现，而错误的信息则相对零星。模型通过海量学习，会倾向学到那个最普通的、共识性的答案。</p></li><li><p><strong>承认局限性并设置护栏</strong>：</p><ul><li><strong>知识截至日期</strong>：模型的知识基于其训练数据的最新日期。在此之后的事件，模型是不知道的。负责任的AI会明确告知用户它的知识截止时间（例如，我的知识截至日期是2024年7月）。</li><li><strong>“幻觉”的应对</strong>：模型有时会“自信地编造”错误答案，这被称为“幻觉”。应对方法包括：在输出时让模型引用来源（如果可能）；在系统层面提示模型“如果不知道，就承认不知道”；以及通过后续的RLHF微调来让模型更倾向于诚实。</li></ul></li><li><p><strong>持续的后训练优化</strong>：模型发布后，开发者可以通过用户的反馈、新的高质量数据集，对模型进行微调，以修正其中已知的错误知识或补充新知识。</p></li></ol><h2 id="8-大模型的Transformer架构"><a href="#8-大模型的Transformer架构" class="headerlink" title="8. 大模型的Transformer架构"></a>8. 大模型的Transformer架构</h2><p>Transformer 架构的核心思想是：<strong>让模型在处理一段信息（比如一句话）时，能够瞬间直接“关注”到这段信息中所有其他部分的重要性，而无需像过去那样一步一步按顺序处理。</strong></p><p>这种”关注“的能力，就是所谓的**“注意力机制”<strong>（<strong>Attention Mechanism</strong>），尤其是</strong>“自注意力”**（<strong>Self-Attention</strong>）。这是Transformer的灵魂。</p><p>在Transformer出现之前，主流模型是RNN（循环神经网络）和它的变体LSTM。</p><ul><li>RNN的问题：它像一个人一样逐字阅读。要理解一句话的第10个词，它必须先把前9个词过一遍。这个过程：<ol><li><strong>慢</strong>：无法并行计算。</li><li><strong>易遗忘</strong>：对于长句子，读到后面可能就忘了开头讲了什么（”长程依赖“问题）。</li></ol></li></ul><p>Transformer 的提出，完全颠覆了这种顺序处理的方式，实现了并行处理，并完美解决了长程依赖问题。</p><ul><li><strong>旧模型（RNN）</strong>：像是一个人在做汇报，必须按顺序说完A再说B，台下的人容易走神忘记开头。</li><li><strong>Transformer 模型</strong>：像是所有专家（每个单词）同时坐在一个房间里。当讨论到某个具体问题（如“it”指代谁）时，<strong>每个专家都可以自由地、同时地和任何其他专家交流</strong>，快速形成共识。这个过程高效、全面，且没有信息损失。</li></ul><h2 id="9-MOE（Mixture-of-Experts，混合专家模型）"><a href="#9-MOE（Mixture-of-Experts，混合专家模型）" class="headerlink" title="9. MOE（Mixture of Experts，混合专家模型）"></a>9. MOE（Mixture of Experts，混合专家模型）</h2><p>如果说Transformer是模型的“强大大脑”，那么MOE就是在这个大脑里组建了一个“<strong>专家委员会</strong>”。</p><p>MOE的核心思想是：<strong>与其用一个巨大的神经网络处理所有问题，不如设计一堆各有所长的“专家”（Expert）网络，并有一个“调度员”（Router）来决定每个问题该由哪位或几位专家来处理</strong>。这样做的目的非常明确：<strong>在极大地增加模型参数总量（从而提升能力）的同时，尽可能地控制计算成本</strong>。</p><p>随着模型越来越大（从百亿到万亿参数），出现了两个致命问题：</p><ol><li><strong>计算成本爆炸</strong>：每次处理一个输入（无论简单还是复杂），都需要激活整个万亿参数模型，训练和推理的代价高昂到无法承受。</li><li><strong>激活浪费</strong>：模型中存在“稀疏性”。比如，回答“法国的首都是什么？”这种事实性问题，可能只需要动用模型里关于地理知识的部分参数，而不需要动用写诗、编程的参数。但传统模型每次都会“全员上岗”，造成巨大的计算浪费。</li></ol><p>MOE的发明就是为了解决这些痛点而生的。</p><h4 id="MOE的核心优势与挑战"><a href="#MOE的核心优势与挑战" class="headerlink" title="MOE的核心优势与挑战"></a>MOE的核心优势与挑战</h4><p>优势：</p><ol><li><strong>更低的计算成本，更大的模型规模</strong>：这是最核心的优势。可以用相对较少的计算资源来训练和运行一个“名义上”参数巨大的模型。例如，Google的Switch Transformer有<strong>1.6万亿</strong>参数，但推理成本仅相当于一个约1000亿参数的稠密模型。</li><li><strong>潜在的专家专业化</strong>：模型可以自发地让不同专家专注于不同领域，这可能会提升模型在各项任务上的整体表现。</li></ol><p>挑战：</p><ol><li><strong>训练不稳定</strong>：需要精心设计才能让调度员公平地分配任务，避免“强者恒强”（总是将任务分配给几个受欢迎的专家，而其他专家得不到训练）。</li><li><strong>通信开销</strong>：在多个GPU或多台服务器上训练时，需要频繁地在不同设备之间传输数据和调度专家，这对网络带宽的要求极高。</li><li><strong>微调难度</strong>：对MOE模型进行下游任务的微调比传统模型更复杂。</li></ol><h3 id="MOE与Transformer的关系"><a href="#MOE与Transformer的关系" class="headerlink" title="MOE与Transformer的关系"></a>MOE与Transformer的关系</h3><p><strong>MOE不是来取代Transformer的，而是来增强它的。</strong></p><p>您可以这样理解：</p><ul><li><strong>Transformer</strong> 是构建现代AI模型的<strong>基础积木</strong>。</li><li><strong>MOE</strong> 是一种<strong>设计架构</strong>，它告诉我们应该如何<strong>组织和运用</strong>这些积木。<ul><li>传统方式：用很多积木搭一个<strong>巨大且密实的城堡</strong>（稠密模型）。</li><li>MOE方式：用同样的积木搭<strong>很多个小房子（专家）</strong>，并修一条聪明的路（调度员）把它们连接起来。</li></ul></li></ul><p>目前几乎所有顶尖的大模型都在采用或探索MOE技术，例如：</p><ul><li><strong>Google</strong> 的 <code>Switch Transformer</code>、<code>GLaM</code></li><li><strong>OpenAI</strong> 的传闻指出，GPT-4 很可能是一个MOE架构模型。</li><li><strong>Mistral AI</strong> 的 <code>Mixtral 8x7B</code> 就是一个非常著名的开源MOE模型，它由8个“专家”组成，每次推理激活2个。它的性能远超其参数规模所预示的水平。</li><li><strong>DeepSeek</strong> 的 <code>DeepSeek-V2</code> 也采用了创新的MOE架构。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>大模型时代给我的最终感受是：<strong>我们正站在一个历史性的分岔路口</strong>。</p><p>它不像一次普通的升级，而更像<strong>一次地基的重新铺设</strong>。它带来的不仅是工具的革新，更是对个体能力、社会结构、知识乃至人性本身的一次深度拷问。</p><p>作为个人，最好的应对方式或许是：保持开放，积极学习，善用其力，同时更深地耕耘那些使自己之所以为人的独特价值。<strong>大模型或许不会取代你，但一个会使用大模型的人，取代你的概率非常大</strong>。这场变革的关键不在于技术本身，而在于我们如何适应和采用它。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微服务架构理论与原则</title>
      <link href="/2025/08/04/wei-fu-wu-jia-gou-li-lun-yu-yuan-ze/"/>
      <url>/2025/08/04/wei-fu-wu-jia-gou-li-lun-yu-yuan-ze/</url>
      
        <content type="html"><![CDATA[<h1 id="微服务架构理论与原则"><a href="#微服务架构理论与原则" class="headerlink" title="微服务架构理论与原则"></a>微服务架构理论与原则</h1><h2 id="何为微服务架构？"><a href="#何为微服务架构？" class="headerlink" title="何为微服务架构？"></a>何为微服务架构？</h2><p>它是一种<strong>架构风格</strong>，旨在将一个大型、复杂的单体应用拆解成一组小型的、独立的服务。每个服务都围绕着特定的<strong>业务能力</strong>来构建，并且可以被<strong>独立地开发、测试、部署和扩展</strong>。</p><p>它的核心思想可以总结为以下几点：</p><ol><li><strong>服务即组件 (Services as Components)</strong>: 与传统的代码库（Library）组件不同，微服务是通过网络调用（如 REST API、gRPC）来交互的组件，这使得它可以跨语言、跨技术栈。</li><li><strong>围绕业务能力组织 (Organized around Business Capabilities)</strong>: 每个服务都应该是一个高内聚的业务单元，比如订单服务、用户服务、库存服务。这与传统按技术分层（如UI层、业务逻辑层、数据层）的团队组织方式截然不同。</li><li><strong>去中心化 (Decentralized)</strong>:<ul><li><strong>技术去中心化</strong>: 每个服务可以选择最适合自身业务场景的技术栈（语言、数据库等），而不是被统一的技术栈锁定。</li><li><strong>数据去中心化</strong>: 每个服务拥有自己独立的数据库，避免了单体应用中“一个数据库走天下”的模式，保证了服务的自治性。</li><li><strong>治理去中心化</strong>: 团队可以自治地决定开发节奏、部署周期和技术选型，实现了真正的“You build it, you run it”。</li></ul></li><li><strong>基础设施自动化 (Infrastructure Automation)</strong>: 由于服务数量众多，必须依赖高度自动化的CI&#x2F;CD流水线、自动化测试和云原生基础设施（如容器化、服务编排）来高效地管理和部署。</li></ol><h2 id="它解决了什么问题，又带来了哪些新的挑战？"><a href="#它解决了什么问题，又带来了哪些新的挑战？" class="headerlink" title="它解决了什么问题，又带来了哪些新的挑战？"></a>它解决了什么问题，又带来了哪些新的挑战？</h2><p>微服务架构主要是为了解决<strong>单体架构 (Monolithic Architecture)</strong> 在大规模、快速迭代的复杂应用场景下所暴露出的痛点：</p><ol><li><strong>降低了系统复杂性 (Reduced Complexity)</strong>: 单体应用随着业务增长会变得异常臃肿，代码耦合严重，新员工难以理解，修改一处代码可能引发全局性的问题。微服务将复杂性分散到各个独立的服务中，每个服务的逻辑都相对简单、清晰。</li><li><strong>提高了开发和部署效率 (Improved Development and Deployment Speed)</strong>:<ul><li>单体架构中，任何微小的改动都需要重新编译、测试和部署整个应用，流程漫长且风险高。</li><li>在微服务架构中，团队可以独立修改、测试和部署自己的服务，无需等待其他团队。这使得CI&#x2F;CD能够真正落地，大大加快了功能的上线速度。</li></ul></li><li><strong>增强了技术异构性和灵活性 (Enabled Technological Heterogeneity)</strong>: 单体应用一旦选定技术栈，后期很难更换。微服务允许不同的服务根据自身特点选择最合适的技术，比如计算密集型服务可以用Go或C++，而业务逻辑密集的可以用Java或Python。这使得团队可以拥抱新技术，而不是被历史技术债所束缚。</li><li><strong>提升了系统的可扩展性 (Enhanced Scalability)</strong>: 单体应用只能作为一个整体进行扩展，如果只有某个功能模块（如“秒杀”）成为瓶颈，我们也不得不复制整个应用，造成资源浪费。微服务允许我们只针对那些真正需要扩展的服务进行独立的、水平的扩展。</li><li><strong>提高了容错性 (Increased Fault Isolation)</strong>: 在单体应用中，一个模块的内存泄漏或bug可能导致整个应用程序崩溃。在微服务中，如果一个服务出现故障，只要有适当的熔断、降级机制，就不会影响到其他服务的核心功能，系统的“爆炸半径”被有效控制。</li></ol><hr><p>微服务架构并非银弹，它在解决老问题的同时，也引入了一系列源于<strong>分布式系统</strong>的复杂性和新挑战：</p><ol><li><strong>服务拆分原则的挑战 (Challenge of Service Decomposition)</strong>:<ul><li><strong>如何拆？</strong>: 这是实施微服务的第一个也是最难的挑战。如果拆分粒度过大，就退化成了“分布式单体”，没有享受到微服务的优势；如果粒度过小，服务间交互的开销会急剧增加，运维成本极高。</li><li><strong>拆分原则</strong>: 业界常用的拆分原则是基于<strong>领域驱动设计 (Domain-Driven Design, DDD)</strong> 中的<strong>限界上下文 (Bounded Context)</strong>。目标是做到“高内聚、低耦合”，即把关联紧密的业务逻辑和数据放在一个服务内，减少服务间的依赖。</li></ul></li><li><strong>服务治理的复杂性 (Complexity of Service Governance)</strong>:<ul><li><strong>服务发现 (Service Discovery)</strong>: 服务A如何知道服务B的网络地址？当服务B动态扩缩容时，地址会变化。需要一个注册中心（如 Nacos, Consul, Eureka）来统一管理服务实例的地址。</li><li><strong>负载均衡 (Load Balancing)</strong>: 当一个服务有多个实例时，如何将请求均匀地分发到这些实例上？这需要客户端或服务端的负载均衡策略。</li><li><strong>熔断、限流、降级 (Resilience Patterns)</strong>: 网络是不可靠的，下游服务可能会变慢或宕机。我们需要引入像 Hystrix 或 Sentinel 这样的库来实现：<ul><li><strong>熔断 (Circuit Breaking)</strong>: 暂时切断对故障服务的调用，避免资源耗尽和连锁反应。</li><li><strong>限流 (Rate Limiting)</strong>: 防止突发流量冲垮服务。</li><li><strong>降级 (Degradation)</strong>: 在非核心服务不可用时，返回一个默认值或执行备用逻辑，保证核心业务不受影响。</li></ul></li></ul></li><li><strong>分布式事务的难题 (Dilemma of Distributed Transactions)</strong>:<ul><li>由于每个服务都有自己的数据库，一次跨多个服务的业务操作（如电商下单：创建订单、扣减库存、增加积分）无法通过传统的本地ACID事务来保证一致性。</li><li><strong>解决方案</strong>: 通常放弃强一致性，转而追求<strong>最终一致性 (Eventual Consistency)</strong>。常见的模式包括：<ul><li><strong>SAGA模式</strong>: 将长事务拆分为多个本地事务，每个本地事务都有一个对应的补偿操作。如果某个步骤失败，就依次调用前面已成功步骤的补偿操作来回滚。</li><li><strong>TCC (Try-Confirm-Cancel)</strong>: 对每个服务都实现 Try、Confirm、Cancel 三个接口，进行两阶段提交。</li><li><strong>基于消息队列的最终一致性</strong>: 这是最常见的模式。下单服务完成自己的本地事务后，向消息队列（如 Kafka, RocketMQ）发送一个“订单已创建”的事件，库存服务和积分服务订阅该事件并执行各自的逻辑。这种方式实现了服务间的解耦，但需要处理消息丢失、重复消费等问题。</li></ul></li></ul></li><li><strong>监控和运维的挑战 (Challenges in Monitoring and Operations)</strong>:<ul><li><strong>日志分散</strong>: 一个用户请求可能会流经多个服务，每个服务都有自己的日志文件。排查问题时，需要将这些分散的日志聚合起来。通常使用 <strong>ELK (Elasticsearch, Logstash, Kibana)</strong> 或 EFK 技术栈来做<strong>集中式日志管理</strong>。</li><li><strong>调用链追踪 (Distributed Tracing)</strong>: 为了清晰地看到一个请求在分布式系统中的完整路径、耗时和依赖关系，我们需要引入像 <strong>Jaeger</strong>, <strong>Zipkin</strong> 或 <strong>SkyWalking</strong> 这样的分布式追踪系统。</li><li><strong>度量聚合 (Metrics Aggregation)</strong>: 需要一个统一的平台（如 <strong>Prometheus + Grafana</strong>）来聚合所有服务的健康状况、性能指标（CPU、内存、QPS、延迟等），并进行实时监控和告警。</li><li><strong>部署复杂</strong>: 管理几十上百个服务的部署、配置和版本控制，是一项巨大的挑战，必须依赖强大的<strong>自动化部署工具（CI&#x2F;CD）和容器编排平台（如 Kubernetes）</strong>。</li></ul></li></ol><h2 id="服务拆分原则、服务治理、分布式事务、监控等"><a href="#服务拆分原则、服务治理、分布式事务、监控等" class="headerlink" title="服务拆分原则、服务治理、分布式事务、监控等"></a>服务拆分原则、服务治理、分布式事务、监控等</h2><h3 id="服务拆分原则-Service-Decomposition-Principles"><a href="#服务拆分原则-Service-Decomposition-Principles" class="headerlink" title="服务拆分原则 (Service Decomposition Principles)"></a>服务拆分原则 (Service Decomposition Principles)</h3><p>我们遵循的核心原则是**“高内聚，低耦合”<strong>，具体落地的指导思想主要来源于</strong>领域驱动设计 (Domain-Driven Design, DDD)**。</p><ol><li><strong>基于限界上下文 (Bounded Context) 进行拆分</strong>：<ul><li><strong>什么是限界上下文？</strong>：DDD中的一个核心概念，它是一个业务边界。在这个边界内，领域模型（比如一个“商品”对象）有其唯一的、无歧义的含义。跨越这个边界，同样的词汇可能有完全不同的含义。</li><li><strong>举例</strong>：在“电商”这个大领域里，“商品(Product)”这个词在不同上下文有不同含义：<ul><li>在<strong>商品中心</strong>（或叫后台管理上下文）里，它关心的是商品的SPU、SKU、规格、描述、图片等静态属性。</li><li>在<strong>交易中心</strong>（或叫订单上下文）里，它关心的是下单那一刻的商品快照价格、名称、购买数量。</li><li>在<strong>仓储中心</strong>（或叫库存上下文）里，它只关心商品的SKU ID和库存数量。</li></ul></li><li><strong>如何拆分</strong>：我们就应该将这三个上下文分别拆分为三个独立的微服务：<strong>商品服务</strong>、<strong>订单服务</strong>和<strong>库存服务</strong>。每个服务维护自己上下文内的模型和数据，边界非常清晰。</li></ul></li><li><strong>其他辅助拆分原则</strong>：<ul><li><strong>按业务能力拆分 (Decompose by Business Capability)</strong>：这是DDD思想的一种简化应用。识别出公司或产品有哪些核心的业务能力，比如用户管理、支付能力、通知能力等，然后将每个能力封装成一个服务。</li><li><strong>关注变更频率</strong>：将那些经常一起变更的业务逻辑放在同一个服务里，而将变更频率不同的逻辑分离开。</li><li><strong>关注团队结构（康威定律）</strong>：系统的架构往往会反映出开发这个系统的组织的沟通结构。我们可以反向利用这个定律，将一个独立的、小型的开发团队（比如一个Squad）对应到一个或几个微服务上，让他们可以独立负责、快速迭代。</li></ul></li></ol><p><strong>关于拆分粒度</strong>：</p><ul><li><strong>粒度过大</strong>：服务内部逻辑复杂，耦合严重，开发效率低，接近单体。</li><li><strong>粒度过小</strong>：服务数量爆炸式增长，服务间交互的网络开销和运维成本剧增，分布式事务问题变得更棘手。</li><li><strong>经验法则</strong>：一个好的起点是，一个服务应该可以在<strong>两周内</strong>被一个小型团队完全重写。如果一个服务大到没人敢动，那它可能太大了。</li></ul><hr><h3 id="服务治理-Service-Governance"><a href="#服务治理-Service-Governance" class="headerlink" title="服务治理 (Service Governance)"></a>服务治理 (Service Governance)</h3><p>服务治理就是解决分布式环境下服务生命周期管理的一系列问题。</p><p>它主要包含以下几个方面：</p><ol><li><strong>服务发现 (Service Discovery)</strong>：<ul><li><strong>问题</strong>：在云原生和弹性伸缩的环境下，服务的IP地址和端口是动态变化的。服务A不能把服务B的地址写死在配置文件里。</li><li><strong>解决方案</strong>：引入<strong>注册中心 (Registry Center)</strong>。<ul><li><strong>服务注册</strong>：服务B启动时，会把自己的IP、端口等信息“注册”到注册中心，并定期发送心跳来表明自己还“活着”。</li><li><strong>服务发现</strong>：服务A想调用服务B时，它会去注册中心“查询”服务B的地址列表，然后通过负载均衡策略选择一个实例进行调用。</li></ul></li><li><strong>主流工具</strong>：Nacos, Consul, Eureka, Zookeeper。</li></ul></li><li><strong>服务容错 (Resilience)</strong>：<ul><li><strong>问题</strong>：分布式系统中，网络延迟、服务宕机是常态。一个核心原则是：<strong>任何一次网络调用都可能失败</strong>。我们必须防止单个服务的故障导致整个系统的连锁崩溃（即“雪崩效应”）。</li><li><strong>解决方案（三板斧）</strong>：<ul><li><strong>熔断 (Circuit Breaking)</strong>：当服务A发现调用服务B的失败率超过一定阈值时，会自动“跳闸”，在接下来的一段时间内，所有对服务B的调用都会立即失败并返回一个降级结果，而不会再发起网络请求。这给了服务B恢复的时间，也保护了服务A的资源不被耗尽在无用的等待上。</li><li><strong>降级 (Degradation)</strong>：当服务B不可用时，服务A不应该卡死或崩溃，而是执行一个“备用逻辑”。比如，电商网站的商品推荐服务挂了，我们可以暂时不显示推荐栏，或者显示一个静态的、预先准备好的推荐列表。核心是保证主流程可用。</li><li><strong>限流 (Rate Limiting)</strong>：为了防止突发流量（如秒杀、恶意攻击）冲垮某个服务，我们需要在服务的入口处限制其在单位时间内能够处理的请求数量。常见的限流算法有令牌桶、漏桶算法。</li></ul></li><li><strong>主流工具</strong>：Sentinel, Hystrix, Resilience4j。</li></ul></li><li><strong>负载均衡 (Load Balancing)</strong>：<ul><li><strong>问题</strong>：一个服务通常有多个实例，客户端如何决定调用哪一个？</li><li><strong>解决方案</strong>：<ul><li><strong>服务端负载均衡</strong>：如使用Nginx、F5等硬件或软件，请求先到达负载均衡器，再由它分发给后端的服务实例。</li><li><strong>客户端负载均衡</strong>：客户端（服务A）从注册中心获取到服务B的所有实例列表后，在客户端内部根据某种策略（如轮询、随机、加权响应时间）选择一个实例来调用。Ribbon就是典型的客户端负载均衡器。</li></ul></li></ul></li></ol><hr><h3 id="分布式事务-Distributed-Transactions"><a href="#分布式事务-Distributed-Transactions" class="headerlink" title="分布式事务 (Distributed Transactions)"></a>分布式事务 (Distributed Transactions)</h3><p>示例问题：一个操作需要同时修改订单和库存两个数据库，你怎么保证数据的一致性？</p><p>由于每个服务都有自己独立的数据库，我们无法使用传统关系型数据库的ACID事务来保证跨多个服务操作的原子性。我们必须接受一个事实：在分布式系统中，<strong>强一致性非常昂贵且难以实现</strong>，我们通常追求的是<strong>最终一致性 (Eventual Consistency)</strong>。</p><p>这意味着系统允许在短时间内存在数据不一致的状态，但最终会通过某种机制自我修复，达到一致。常见的实现模式有：</p><ol><li><strong>SAGA模式</strong>：<ul><li><strong>核心思想</strong>：将一个长的分布式事务，拆分成多个由各个服务执行的<strong>本地事务</strong>。每个本地事务都有一个对应的<strong>补偿事务 (Compensating Transaction)</strong>。</li><li><strong>执行流程</strong>：顺序执行每个本地事务。如果某个事务失败，SAGA协调器会依次调用前面所有已成功事务的“补偿事务”，以回滚整个过程。</li><li><strong>举例（下单）</strong>：<ol><li><strong>Try</strong>: 创建订单 (T1) → 扣减库存 (T2) → 扣减积分 (T3)</li><li><strong>Compensation</strong>: 取消订单 (C1) ← 增加库存 (C2) ← 增加积分 (C3)</li><li>如果T3失败，则依次执行C2和C1。</li></ol></li><li><strong>优点</strong>：业务流程清晰，易于理解。</li><li><strong>缺点</strong>：补偿逻辑开发复杂，且不保证隔离性（在回滚前，其他请求可能看到中间状态）。</li></ul></li><li><strong>基于消息队列 (MQ) 的最终一致性</strong>：<ul><li><strong>核心思想</strong>：这是业界最流行的方式。服务之间不直接RPC调用，而是通过异步消息来解耦。</li><li><strong>执行流程</strong>：<ol><li>订单服务执行本地事务，成功创建订单。</li><li>在<strong>同一个本地事务</strong>中，向一个“本地消息表”插入一条消息（如“订单已创建”）。</li><li>一个独立的任务会定时扫描这个本地消息表，将消息投递到消息队列（如Kafka, RocketMQ）。（这一步是为了保证“业务操作”和“发消息”的原子性）</li><li>库存服务和积分服务订阅该消息，收到后执行各自的本地事务。</li></ol></li><li><strong>优点</strong>：服务间高度解耦，吞吐量高。</li><li><strong>缺点</strong>：依赖消息队列的可靠性，需要处理消息重复消费、消息乱序等问题，整个业务流程被异步化，调试和追踪更复杂。</li></ul></li><li><strong>TCC (Try-Confirm-Cancel) 模式</strong>：<ul><li><strong>核心思想</strong>：一种两阶段提交的变种。需要业务方为每个操作都实现三个接口：<ul><li><strong>Try</strong>：预留资源阶段。比如冻结库存、冻结积分。</li><li><strong>Confirm</strong>：确认执行阶段。如果所有服务的Try都成功了，则依次调用所有服务的Confirm，真正完成业务操作（扣减库存、扣减积分）。</li><li><strong>Cancel</strong>：取消执行阶段。如果任何一个服务的Try失败了，则依次调用所有服务的Cancel，释放预留的资源。</li></ul></li><li><strong>优点</strong>：一致性比SAGA强，接近ACID。</li><li><strong>缺点</strong>：对业务的侵入性非常强，开发成本极高，很少在实际中大规模使用。</li></ul></li></ol><hr><h3 id="监控-Monitoring"><a href="#监控-Monitoring" class="headerlink" title="监控 (Monitoring)"></a>监控 (Monitoring)</h3><p>当微服务系统出了问题，怎么快速定位是哪个服务、哪行代码的错？</p><p>微服务的监控比单体复杂得多，因为问题可能出在任何一个服务或它们之间的网络调用上。我们需要建立一个立体的、全方位的<strong>可观测性 (Observability)</strong> 平台，它主要由三驾马车组成：</p><ol><li><strong>集中式日志 (Logging)</strong>：<ul><li><strong>做什么</strong>：<strong>ELK&#x2F;EFK</strong>技术栈，将所有微服务的日志（应用日志、系统日志、访问日志等）都采集、聚合到一个统一的平台。</li><li><strong>关键点</strong>：必须在请求的入口处生成一个全局唯一的<strong>Trace ID</strong>，并让它在整个调用链中透传下去。这样，我们才能在Kibana中通过一个Trace ID串联起一个请求在所有服务中的日志，形成完整的“故事线”。</li></ul></li><li><strong>分布式追踪 (Tracing)</strong>：<ul><li><strong>做什么</strong>：它专注于记录一个请求的<strong>端到端旅程</strong>。它能清晰地展示出：请求流经了哪些服务？每个服务处理了多长时间？服务间的依赖关系是怎样的？</li><li><strong>如何实现</strong>：通过在服务调用时注入和传递上下文信息（如Trace ID, Span ID），并将这些信息上报给追踪系统。</li><li><strong>可视化效果</strong>：通常会生成一个<strong>火焰图 (Flame Graph)</strong>，让我们可以非常直观地看到整个调用的瓶颈在哪里。</li><li><strong>主流工具</strong>：Jaeger, SkyWalking, Zipkin。它们都遵循了OpenTracing规范。</li></ul></li><li><strong>度量聚合 (Metrics)</strong>：<ul><li><strong>做什么</strong>：它关注的是<strong>可聚合的数值型数据</strong>。比如：<ul><li><strong>系统层面</strong>：CPU使用率、内存、磁盘IO、网络流量。</li><li><strong>应用层面</strong>：QPS（每秒请求数）、RT（响应时间）、错误率、JVM状态（GC次数、堆内存）等。</li></ul></li><li><strong>如何实现</strong>：服务通过一个标准接口（Endpoint）暴露自己的度量数据，监控系统定期来抓取（Pull）这些数据，并存入时间序列数据库。</li><li><strong>可视化与告警</strong>：使用图表（Dashboard）来展示这些指标的趋势，并设置告警规则（如“P99响应时间连续5分钟超过500ms”），当规则被触发时，通过短信、电话、钉钉等方式通知开发人员。</li><li><strong>主流工具</strong>：<strong>Prometheus (采集与存储) + Grafana (可视化与告警)</strong> 是目前的事实标准。</li></ul></li></ol><h2 id="CAP理论和BASE理论"><a href="#CAP理论和BASE理论" class="headerlink" title="CAP理论和BASE理论"></a><strong>CAP理论</strong>和<strong>BASE理论</strong></h2><h3 id="CAP-理论-CAP-Theorem"><a href="#CAP-理论-CAP-Theorem" class="headerlink" title="CAP 理论 (CAP Theorem)"></a>CAP 理论 (CAP Theorem)</h3><p>CAP 理论由 Eric Brewer 教授提出，它指出，在一个分布式计算系统中，你<strong>不可能同时</strong>完美地满足以下三个特性，<strong>最多只能满足其中两个</strong>。</p><ol><li><strong>C - 一致性 (Consistency)</strong><ul><li><strong>定义</strong>：指<strong>强一致性</strong>。当一次写操作成功后，任何后续的读请求都必须能读到这个最新的数据。无论客户端连接到哪个节点，看到的数据都应该是一致的。</li><li><strong>通俗理解</strong>：所有节点在同一时间拥有相同的数据副本。就像一个银行账户，你在ATM机存入100元后，你的手机银行App应该立刻显示更新后的余额，而不是旧的。</li></ul></li><li><strong>A - 可用性 (Availability)</strong><ul><li><strong>定义</strong>：系统中的任何一个<strong>健康的</strong>节点，都必须能够在有限的时间内响应客户端的每一个请求，不能出现超时或错误。</li><li><strong>通俗理解</strong>：系统7x24小时服务，永不宕机。只要我向系统发出请求，它就必须给我一个响应（但不保证这个响应里的数据是最新的）。</li></ul></li><li><strong>P - 分区容错性 (Partition Tolerance)</strong><ul><li><strong>定义</strong>：分布式系统中的节点被部署在不同的网络区域（机房、地理位置），节点间的网络通信可能会中断，形成“网络分区”。分区容错性指，即使网络分区发生了，导致部分节点之间无法通信，整个系统仍然能够继续对外提供服务。</li><li><strong>核心要点</strong>：在今天的互联网分布式系统中，网络故障是常态，而不是例外。因此，<strong>P 是一个必须满足的前提条件，而不是一个可选项</strong>。架构师是无法选择放弃P的。</li></ul></li></ol><p><strong>真正的取舍：当网络分区 (P) 发生时，你必须在 C 和 A 之间做出选择。</strong></p><ul><li><strong>选择 CP (放弃 A)</strong>：为了保证数据一致性，当网络分区发生时，主节点（拥有最新数据）无法将数据同步给其他节点。为了避免其他节点返回旧的、不一致的数据，系统可能会选择<strong>拒绝</strong>来自这些节点的读写请求。这时，系统就失去了部分可用性。</li><li><strong>选择 AP (放弃 C)</strong>：为了保证可用性，当网络分区发生时，即使节点之间无法通信，每个节点仍然可以独立地响应客户端请求。但这可能导致一个节点接受了新的写请求，而其他节点毫不知情，从而造成了节点间的数据不一致。</li></ul><hr><h3 id="BASE-理论-BASE-Theory"><a href="#BASE-理论-BASE-Theory" class="headerlink" title="BASE 理论 (BASE Theory)"></a>BASE 理论 (BASE Theory)</h3><p>BASE 理论是 CAP 理论中 <strong>AP</strong> 方案的延伸和工程实践产物。它不是像数据库ACID那样追求强一致性，而是提出了一套适应大规模分布式系统的、以可用性为中心的指导思想。</p><ol><li><strong>BA - 基本可用 (Basically Available)</strong><ul><li><strong>定义</strong>：系统允许损失部分可用性。这体现在两方面：<ul><li><strong>响应时间损失</strong>：正常情况下0.5秒返回，现在因为系统故障，可能需要2-3秒。</li><li><strong>功能损失</strong>：在电商大促时，为了保证核心的交易流程，可以暂时关闭一些非核心功能，比如商品评论、积分系统等（即服务降级）。</li></ul></li></ul></li><li><strong>S - 软状态 (Soft State)</strong><ul><li><strong>定义</strong>：允许系统中的数据存在中间状态，并且这个状态不影响系统的整体可用性。这个中间状态是相对于“硬状态”（数据必须时刻保持一致）而言的。</li></ul></li><li><strong>E - 最终一致性 (Eventually Consistent)</strong><ul><li><strong>定义</strong>：这是BASE理论的核心。系统不要求数据时刻保持强一致，但承诺在经过“一段时间”后，数据最终会达到一致的状态。这个“一段时间”被称为“不一致性窗口”。</li></ul></li></ol><p><strong>总结：BASE 理论的核心思想是，我们为了获得更高的系统可用性和扩展性，愿意牺牲强一致性，转而接受数据在一段时间内的不一致。</strong></p><hr><h2 id="幂等性是什么？"><a href="#幂等性是什么？" class="headerlink" title="幂等性是什么？"></a>幂等性是什么？</h2><p><strong>定义</strong>：在数学和计算机科学中，幂等性（Idempotency）指一个操作<strong>执行一次</strong>和<strong>执行N次</strong>所产生的**结果（或对资源造成的影响）**是完全相同的。</p><p>在RESTful API（即HTTP协议）的上下文中，幂等性是设计健壮、可预测的API的关键。它的主要作用是<strong>支持在网络不稳定的情况下进行安全的“重试”</strong>。</p><p>当客户端发送一个请求后，可能会因为网络超时等原因没有收到服务器的响应。这时客户端就陷入了困境：请求是丢失了，还是服务器已经处理了只是响应丢失了？</p><ul><li>如果操作是<strong>幂等</strong>的，客户端可以<strong>毫无顾忌地重新发送</strong>同一个请求，而不用担心会产生意料之外的副作用（比如重复创建数据）。</li><li>如果操作是<strong>非幂等</strong>的，客户端就不能轻易重试，否则可能导致严重问题（比如重复下单、重复扣款）。</li></ul><h3 id="HTTP方法与幂等性"><a href="#HTTP方法与幂等性" class="headerlink" title="HTTP方法与幂等性"></a>HTTP方法与幂等性</h3><p>不同的HTTP动词（Verb）天生就具有不同的幂等性特征：</p><table><thead><tr><th>HTTP 方法</th><th>动作</th><th>是否幂等？</th><th>解释</th></tr></thead><tbody><tr><td><strong>GET</strong></td><td>查询资源</td><td>✅ <strong>是</strong></td><td><code>GET /users/123</code> 执行多少次，都只是获取该用户的信息，不会改变服务器状态。</td></tr><tr><td><strong>HEAD</strong></td><td>获取资源元信息</td><td>✅ <strong>是</strong></td><td>和GET一样，是只读操作。</td></tr><tr><td><strong>OPTIONS</strong></td><td>获取支持的方法</td><td>✅ <strong>是</strong></td><td>同样是只读操作。</td></tr><tr><td><strong>PUT</strong></td><td><strong>完整</strong>更新&#x2F;替换资源</td><td>✅ <strong>是</strong></td><td><code>PUT /users/123</code> 并附带完整的用户信息，执行一次是将用户123更新为新数据，执行N次结果还是一样。</td></tr><tr><td><strong>DELETE</strong></td><td>删除资源</td><td>✅ <strong>是</strong></td><td><code>DELETE /users/123</code> 执行一次是删除该用户，执行N次，结果仍然是“该用户不存在”（虽然第二次以后可能返回404，但服务器的最终状态是一致的）。</td></tr><tr><td><strong>POST</strong></td><td>创建子资源</td><td>❌ <strong>否</strong></td><td><code>POST /users</code> 执行一次是创建一个新用户（如ID为124），再执行一次会创建另一个新用户（如ID为125）。</td></tr><tr><td><strong>PATCH</strong></td><td><strong>部分</strong>更新资源</td><td>❌ <strong>不一定</strong></td><td><code>PATCH</code> 的幂等性取决于操作本身。<code>PATCH /users/123</code> 更新用户名为 “Tom” 是幂等的。但 <code>PATCH /posts/1/likes</code> 执行“点赞数+1”的操作就不是幂等的。</td></tr></tbody></table><h2 id="幂等性的错误设计"><a href="#幂等性的错误设计" class="headerlink" title="幂等性的错误设计"></a>幂等性的错误设计</h2><h3 id="一、非幂等的-PUT-接口是严重的设计问题"><a href="#一、非幂等的-PUT-接口是严重的设计问题" class="headerlink" title="一、非幂等的 PUT 接口是严重的设计问题"></a>一、非幂等的 PUT 接口是严重的设计问题</h3><p>如果一个 <code>PUT</code> 接口，执行一次和多次的状态不一样，那<strong>绝对是这个接口设计有严重的问题</strong>。</p><p>这不仅仅是一个“不好的实践”，而是直接<strong>违反了 HTTP 协议的规范 (RFC 7231)</strong>。</p><p>规范中对 <code>PUT</code> 方法的定义是：用请求中的负载（payload）<strong>完整替换</strong>目标资源的状态。</p><p><strong>正确的设计：</strong><br>客户端发送 <code>PUT /users/123</code> 请求，请求体为 <code>&#123;&quot;name&quot;: &quot;Alice&quot;, &quot;email&quot;: &quot;alice@example.com&quot;&#125;</code>。</p><ul><li><strong>第一次执行</strong>: 服务器在数据库中找到 ID 为 123 的用户，将其所有字段更新为请求体中的数据。如果用户不存在，则创建该用户。</li><li><strong>第二次执行</strong>: 服务器再次找到 ID 为 123 的用户，并再次将其所有字段更新为请求体中的数据。</li></ul><p>无论执行多少次，ID 为 123 的用户的最终状态都是 <code>&#123;name: &quot;Alice&quot;, email: &quot;alice@example.com&quot;&#125;</code>。这就是幂等性。</p><hr><p><strong>错误的设计 (Non-idempotent PUT)：</strong><br>假设有一个设计师错误地用 <code>PUT</code> 来实现“增加用户积分”的操作。<br>客户端发送 <code>PUT /users/123/points</code> 请求，请求体为 <code>&#123;&quot;points_to_add&quot;: 10&#125;</code>。</p><ul><li><strong>第一次执行</strong>: 服务器为用户123增加了10个积分。</li><li><strong>第二次执行</strong>: 服务器<strong>再次</strong>为用户123增加了10个积分。</li></ul><p>这就破坏了幂等性，并且会带来灾难性后果。如果客户端因为网络超时而重试了这个请求，就会导致用户的积分被错误地增加了两次。</p><hr><h3 id="二、在-GET-请求中修改数据"><a href="#二、在-GET-请求中修改数据" class="headerlink" title="二、在 GET 请求中修改数据"></a>二、在 GET 请求中修改数据</h3><p>场景：有一个获取文章阅读量的GET接口，那么每次请求这个接口时，接口方法内部就会修改数据库的阅读量，使其加一。</p><p>这种做法违反了<code>GET</code>方法最核心的两个原则：<strong>安全性 (Safety)</strong> 和 <strong>幂等性 (Idempotency)</strong>。</p><h3 id="1-违反了安全性原则-❌"><a href="#1-违反了安全性原则-❌" class="headerlink" title="1. 违反了安全性原则 ❌"></a>1. 违反了安全性原则 ❌</h3><p>在HTTP协议中，一个方法如果是**“安全”<strong>的，意味着执行它</strong>不会对服务器上的资源状态产生任何改变（或称“副作用”）**。<code>GET</code>和<code>HEAD</code>方法被规定为必须是安全的。</p><ul><li><strong>你的场景</strong>：<code>GET</code>接口修改了数据库中的阅读量，这显然是一个写操作，改变了资源的状态。</li><li><strong>后果</strong>：这破坏了HTTP规范和所有Web基础设施（浏览器、代理、爬虫等）对<code>GET</code>请求的基本假设。就像你去图书馆的目录电脑上查一本书的位置，结果你每查一次，电脑就把这本书换到一个新书架上一样，这是混乱且不可预测的。</li></ul><h3 id="2-违反了幂等性原则-❌"><a href="#2-违反了幂等性原则-❌" class="headerlink" title="2. 违反了幂等性原则 ❌"></a>2. 违反了幂等性原则 ❌</h3><p>我们已经知道，幂等性指执行N次和执行1次的效果相同。安全的<code>GET</code>方法天然是幂等的。但你这个<code>GET</code>接口因为每次都会+1，所以执行N次会导致阅读量增加N，这显然不是幂等的。</p><h3 id="3-导致了实际的技术灾难：缓存失效与数据污染-💣"><a href="#3-导致了实际的技术灾难：缓存失效与数据污染-💣" class="headerlink" title="3. 导致了实际的技术灾难：缓存失效与数据污染 💣"></a>3. 导致了实际的技术灾难：缓存失效与数据污染 💣</h3><p>这是在工程实践中最致命的问题。</p><ul><li><strong>缓存机制</strong>：浏览器、CDN、反向代理（如Nginx）等都会默认<code>GET</code>请求是安全的，并积极地缓存其响应结果以提高性能。</li><li><strong>失效场景</strong>：<ol><li>第一个用户访问 <code>GET /articles/123</code>。服务器返回文章内容和<code>&quot;views&quot;: 101</code>，同时数据库中的阅读量变为101。</li><li>这个响应被CDN缓存了（比如缓存5分钟）。</li><li>在接下来的5分钟内，<strong>又有100个用户</strong>访问了同一个URL。</li><li>这100次请求全部被CDN直接响应，返回的都是缓存的<code>&#123;&quot;views&quot;: 101&#125;</code>。它们根本<strong>不会到达你的服务器</strong>。</li><li><strong>结果</strong>：真实阅读量应该是201，但数据库里仍然是101，显示给用户的也一直是101。你的计数功能完全失效了。</li></ol></li></ul><h3 id="4-引发不可控的调用"><a href="#4-引发不可控的调用" class="headerlink" title="4. 引发不可控的调用"></a>4. 引发不可控的调用</h3><ul><li><strong>搜索引擎爬虫 (Crawlers)</strong>：像Googlebot这样的网络爬虫会通过<code>GET</code>请求来抓取和索引网页内容。它们会把你的文章阅读量刷得虚高。</li><li><strong>浏览器预加载 (Prefetching)</strong>：一些现代浏览器为了提升用户体验，可能会在后台预先加载用户可能会点击的链接。这些预加载的<code>GET</code>请求也会意外地增加阅读量。</li></ul><h2 id="如何保证接口的幂等性？有哪些实现方案？"><a href="#如何保证接口的幂等性？有哪些实现方案？" class="headerlink" title="如何保证接口的幂等性？有哪些实现方案？"></a>如何保证接口的<strong>幂等性</strong>？有哪些实现方案？</h2><h3 id="1-Token-机制（令牌机制）"><a href="#1-Token-机制（令牌机制）" class="headerlink" title="1. Token 机制（令牌机制）"></a>1. Token 机制（令牌机制）</h3><p>这是一种非常通用的方案，尤其适用于防止表单重复提交等场景。它将一次完整的业务请求分为了两个步骤：</p><ol><li><strong>第一步：客户端获取Token</strong><ul><li>在进入业务操作页面（比如确认订单页面）时，客户端需要先向服务器发起一个“前置请求”，以获取一个本次操作专用的、唯一的<code>token</code>。</li><li>服务器生成这个<code>token</code>后，会将其存储在后端的存储系统里（通常是Redis，因为它速度快且支持设置过期时间）。</li></ul></li><li><strong>第二步：客户端携带Token发起业务请求</strong><ul><li>当用户点击“提交订单”按钮时，客户端会将上一步获取的<code>token</code>随同业务数据一起发送给服务器。</li><li>服务器收到请求后，会去Redis中查找这个<code>token</code>。<ul><li><strong>如果找到了</strong>：说明这是第一次请求。服务器会立即<strong>删除</strong>这个<code>token</code>，然后继续执行业务逻辑。</li><li><strong>如果找不到了</strong>：说明这个<code>token</code>已经被用过（或者已过期），服务器就直接拒绝本次请求，返回一个“请勿重复提交”的提示。</li></ul></li></ul></li></ol><p><strong>流程图:</strong></p><ul><li><strong>优点</strong>:<ul><li>逻辑通用，可以封装成一个通用服务，不与具体业务逻辑耦合。</li></ul></li><li><strong>缺点&#x2F;挑战</strong>:<ul><li>需要额外的一次请求来获取<code>token</code>，增加了网络交互。</li><li><strong>原子性问题</strong>：在高并发下，“检查<code>token</code>”和“删除<code>token</code>”这两个操作必须是<strong>原子性</strong>的。否则，两个请求可能同时检查到<code>token</code>存在，然后都去执行业务。通常使用Redis的<code>SET key value NX EX seconds</code>命令或Lua脚本来保证原子性。</li></ul></li></ul><hr><h3 id="2-唯一ID机制（客户端生成）"><a href="#2-唯一ID机制（客户端生成）" class="headerlink" title="2. 唯一ID机制（客户端生成）"></a>2. 唯一ID机制（客户端生成）</h3><p>这种方案将生成唯一标识的责任交给了调用方（客户端），简化了服务端的逻辑。</p><ul><li><strong>实现流程</strong>:<ol><li>调用方在发起请求前，先自行生成一个全局唯一的ID（<code>request_id</code>或<code>idempotency_key</code>），通常使用UUID算法生成。</li><li>将这个<code>request_id</code>放在请求的Header或Body中，发送给服务器。</li><li>服务器收到请求后，首先根据这个<code>request_id</code>去查询一个存储系统（如Redis或数据库）。<ul><li><strong>如果记录不存在</strong>：说明是新请求。服务器将这个<code>request_id</code>作为key存入存储系统（可以设置一个过期时间），然后执行业务逻辑。</li><li><strong>如果记录已存在</strong>：说明是重复请求，直接丢弃，并返回上一次的处理结果（如果需要的话）。</li></ul></li></ol></li><li><strong>优点</strong>:<ul><li>实现简单，将唯一性生成的压力分散到了客户端。</li><li>服务端逻辑相对轻量。</li></ul></li><li><strong>缺点&#x2F;挑战</strong>:<ul><li>依赖调用方遵循协议正确地生成和传递唯一ID。</li><li>同样需要保证“检查ID”和“写入ID”这两个操作的原子性。</li></ul></li></ul><hr><h3 id="3-数据库唯一键约束"><a href="#3-数据库唯一键约束" class="headerlink" title="3. 数据库唯一键约束"></a>3. 数据库唯一键约束</h3><p>这是最简单、最可靠的防重方式之一，它利用了数据库底层<code>UNIQUE KEY</code>的特性来保证幂等性。</p><ul><li><strong>实现流程</strong>:<ol><li>在你的业务表中，找到一个或多个能够<strong>唯一标识一笔业务</strong>的字段。例如，在订单表中，<code>order_no</code>（订单号）就是天然的唯一标识。</li><li>为这个字段（或字段组合）在数据库层面添加一个<strong>唯一键约束 (UNIQUE KEY)</strong>。</li><li>当业务请求过来需要插入数据时，直接执行<code>INSERT</code>操作。<ul><li><strong>第一次请求</strong>: 插入成功。</li><li><strong>后续的重复请求</strong>: 再次尝试插入同样<code>order_no</code>的数据时，数据库会因为违反了唯一键约束而直接抛出异常。</li></ul></li><li>你的代码在捕获到这个特定的数据库异常后，就知道这是一个重复请求，然后可以进行相应的处理（比如返回成功或提示重复）。</li></ol></li><li><strong>优点</strong>:<ul><li>实现成本极低，无需引入额外的组件（如Redis）。</li><li>可靠性极高，由数据库保证了最终的一致性。</li></ul></li><li><strong>缺点&#x2F;挑战</strong>:<ul><li>只适用于那些有明显业务唯一标识的“插入”场景。对于“更新”操作，它就无能为力了。</li><li>与业务逻辑强耦合，不具备通用性。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
            <tag> 架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown代码样式测试文件</title>
      <link href="/2025/04/26/demo/"/>
      <url>/2025/04/26/demo/</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown代码样式测试文件"><a href="#Markdown代码样式测试文件" class="headerlink" title="Markdown代码样式测试文件"></a>Markdown代码样式测试文件</h1><p>这是一个用于测试Markdown代码格式和样式的文件，包含了多种代码块和格式化内容。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#%E4%BB%A3%E7%A0%81%E5%9D%97%E6%B5%8B%E8%AF%95">代码块测试</a></li><li><a href="#%E8%A1%A8%E6%A0%BC%E6%B5%8B%E8%AF%95">表格测试</a></li><li><a href="#%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95">数学公式测试</a></li><li><a href="#%E5%88%97%E8%A1%A8%E6%B5%8B%E8%AF%95">列表测试</a></li><li><a href="#%E5%BC%95%E7%94%A8%E6%B5%8B%E8%AF%95">引用测试</a></li></ul><h2 id="代码块测试"><a href="#代码块测试" class="headerlink" title="代码块测试"></a>代码块测试</h2><h3 id="普通代码块"><a href="#普通代码块" class="headerlink" title="普通代码块"></a>普通代码块</h3><p>这是一个没有指定语言的代码块：</p><pre><code>function normalCode() &#123;  console.log(&quot;这是一个普通代码块&quot;);&#125;</code></pre><h3 id="Python代码块"><a href="#Python代码块" class="headerlink" title="Python代码块"></a>Python代码块</h3><pre class="line-numbers language-language-python"><code class="language-language-python">def hello_world():    print("Hello, World!")    # 一个简单的类定义class Person:    def __init__(self, name, age):        self.name = name        self.age = age            def greet(self):        return f"你好，我是&#123;self.name&#125;，今年&#123;self.age&#125;岁"        # 使用类p = Person("张三", 25)print(p.greet())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="JavaScript代码块"><a href="#JavaScript代码块" class="headerlink" title="JavaScript代码块"></a>JavaScript代码块</h3><pre class="line-numbers language-language-javascript"><code class="language-language-javascript">// 箭头函数const add = (a, b) => a + b;// 使用模板字符串function greet(name) &#123;  return `Hello, $&#123;name&#125;!`;&#125;// Promise示例function fetchData() &#123;  return new Promise((resolve, reject) => &#123;    setTimeout(() => &#123;      resolve(&#123; data: "这是一些数据" &#125;);    &#125;, 1000);  &#125;);&#125;// 异步函数async function getData() &#123;  try &#123;    const result = await fetchData();    console.log(result.data);  &#125; catch (error) &#123;    console.error("发生错误:", error);  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Bash-Shell代码块"><a href="#Bash-Shell代码块" class="headerlink" title="Bash&#x2F;Shell代码块"></a>Bash&#x2F;Shell代码块</h3><pre class="line-numbers language-language-bash"><code class="language-language-bash">#!/bin/bash# 这是一个简单的shell脚本echo "当前目录是:"pwd# 循环示例for i in &#123;1..5&#125;; do  echo "第 $i 次迭代"done# 条件判断if [ -f "config.yml" ]; then  echo "配置文件存在"else  echo "配置文件不存在"fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Java代码块"><a href="#Java代码块" class="headerlink" title="Java代码块"></a>Java代码块</h3><pre class="line-numbers language-language-java"><code class="language-language-java">public class HelloWorld &#123;    public static void main(String[] args) &#123;        System.out.println("Hello, World!");                // 创建一个对象        Person person = new Person("李四", 30);        System.out.println(person.greet());    &#125;&#125;class Person &#123;    private String name;    private int age;        public Person(String name, int age) &#123;        this.name = name;        this.age = age;    &#125;        public String greet() &#123;        return "你好，我是" + name + "，今年" + age + "岁";    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="包含管道符的代码块（测试特殊字符）"><a href="#包含管道符的代码块（测试特殊字符）" class="headerlink" title="包含管道符的代码块（测试特殊字符）"></a>包含管道符的代码块（测试特殊字符）</h3><p>下面的代码块包含了管道符 <code>|</code>，这可能会被误解为表格分隔符：</p><pre class="line-numbers language-language-text"><code class="language-language-text"># 这行包含管道符echo "A" | grep "A"# 数据示例名称 | 年龄 | 职业小明 | 25 | 工程师小红 | 24 | 设计师<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="表格测试"><a href="#表格测试" class="headerlink" title="表格测试"></a>表格测试</h2><p>这是一个标准的 Markdown 表格：</p><table><thead><tr><th>名称</th><th>年龄</th><th>职业</th></tr></thead><tbody><tr><td>小明</td><td>25</td><td>工程师</td></tr><tr><td>小红</td><td>24</td><td>设计师</td></tr><tr><td>小李</td><td>26</td><td>医生</td></tr></tbody></table><p>左对齐、居中和右对齐的混合表格：</p><table><thead><tr><th align="left">名称 (左对齐)</th><th align="center">年龄 (居中)</th><th align="right">薪资 (右对齐)</th></tr></thead><tbody><tr><td align="left">张三</td><td align="center">25</td><td align="right">¥10,000</td></tr><tr><td align="left">李四</td><td align="center">26</td><td align="right">¥12,000</td></tr><tr><td align="left">王五</td><td align="center">24</td><td align="right">¥9,500</td></tr></tbody></table><h2 id="数学公式测试"><a href="#数学公式测试" class="headerlink" title="数学公式测试"></a>数学公式测试</h2><p>这里是一些数学公式测试（使用 LaTeX 语法）：</p><p>行内公式: $E &#x3D; mc^2$</p><p>独立公式:</p><p>$$<br>\frac{d}{dx}e^x &#x3D; e^x<br>$$</p><p>复杂公式:</p><p>$$<br>\begin{align}<br>\nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; &#x3D; \frac{4\pi}{c}\vec{\mathbf{j}} \<br>\nabla \cdot \vec{\mathbf{E}} &amp; &#x3D; 4 \pi \rho \<br>\nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; &#x3D; \vec{\mathbf{0}} \<br>\nabla \cdot \vec{\mathbf{B}} &amp; &#x3D; 0<br>\end{align}<br>$$</p><p>余弦相似度计算公式:</p><p>$$<br>similarity(A,B) &#x3D; \frac{A \cdot B}{||A|| \cdot ||B||}<br>$$</p><p>欧氏距离计算公式:</p><p>$$<br>distance(A,B) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n}(A_i-B_i)^2}<br>$$</p><h2 id="列表测试"><a href="#列表测试" class="headerlink" title="列表测试"></a>列表测试</h2><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><ul><li>第一项</li><li>第二项</li><li>第三项<ul><li>子项 1</li><li>子项 2<ul><li>子子项 1</li><li>子子项 2</li></ul></li></ul></li></ul><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><ol><li>第一步</li><li>第二步</li><li>第三步<ol><li>子步骤 1</li><li>子步骤 2<ol><li>详细步骤 a</li><li>详细步骤 b</li></ol></li></ol></li></ol><h3 id="任务列表"><a href="#任务列表" class="headerlink" title="任务列表"></a>任务列表</h3><ul><li><input checked disabled type="checkbox"> 已完成任务</li><li><input disabled type="checkbox"> 未完成任务</li><li><input disabled type="checkbox"> 另一个未完成任务<ul><li><input checked disabled type="checkbox"> 已完成子任务</li><li><input disabled type="checkbox"> 未完成子任务</li></ul></li></ul><h2 id="引用测试"><a href="#引用测试" class="headerlink" title="引用测试"></a>引用测试</h2><blockquote><p>这是一个简单的引用</p></blockquote><p>多层嵌套引用:</p><blockquote><p>第一层引用</p><blockquote><p>第二层引用</p><blockquote><p>第三层引用</p><p>继续第三层</p></blockquote></blockquote></blockquote><p>带有其他元素的引用:</p><blockquote><h4 id="引用中的标题"><a href="#引用中的标题" class="headerlink" title="引用中的标题"></a>引用中的标题</h4><ul><li>引用中的列表项</li><li>另一个列表项</li></ul><pre class="line-numbers language-language-python"><code class="language-language-python"># 引用中的代码块print("Hello from a quote!")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote><h2 id="图片和链接测试"><a href="#图片和链接测试" class="headerlink" title="图片和链接测试"></a>图片和链接测试</h2><h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><p><a href="https://www.markdownguide.org/">Markdown语法指南</a></p><p><a href="https://hexo.io/">Hexo官方网站</a></p><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p><img src="https://via.placeholder.com/600x400" alt="示例图片" title="这是一个占位图片"></p><h2 id="高级格式测试"><a href="#高级格式测试" class="headerlink" title="高级格式测试"></a>高级格式测试</h2><h3 id="高亮文本"><a href="#高亮文本" class="headerlink" title="高亮文本"></a>高亮文本</h3><p>使用 <code>&lt;mark&gt;</code> 标签可以 <mark>高亮显示文本</mark>。</p><h3 id="删除线和下划线"><a href="#删除线和下划线" class="headerlink" title="删除线和下划线"></a>删除线和下划线</h3><p><del>这是删除线文本</del> 和 <u>这是下划线文本</u>。</p><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><p>这是一个带有脚注的文本<a href="%E8%BF%99%E6%98%AF%E8%84%9A%E6%B3%A8%E7%9A%84%E5%86%85%E5%AE%B9%E3%80%82">^1</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个Markdown文档包含了多种常见的代码块和格式化元素，可以用来测试您的Hexo主题是否正确渲染这些元素。如果所有内容都能正确显示，那么您的配置应该是正确的。</p><p>如果出现问题，特别注意:</p><ol><li>代码块中的语言标记是否正确</li><li>包含特殊字符的代码块是否正确渲染</li><li>数学公式是否正确显示</li><li>表格是否对齐且正确显示</li></ol>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConcurrentHashMap 整理</title>
      <link href="/2025/02/28/concurrenthashmap/"/>
      <url>/2025/02/28/concurrenthashmap/</url>
      
        <content type="html"><![CDATA[<p>ConcurrentHashMap 提供了线程安全的映射操作，可以在多线程环境下被安全地访问和修改，内部使用 数组+链表+红黑树 结构来存储元素。适用于高并发，特别是高读少写的场景。</p><h2 id="JDK-8-之前数据结构（数组-链表）"><a href="#JDK-8-之前数据结构（数组-链表）" class="headerlink" title="JDK 8 之前数据结构（数组+链表）"></a>JDK 8 之前数据结构（数组+链表）</h2><ul><li>内部结构分为多个 Segment（分段）。每个 Segment 是一个静态内部类，继承了 ReentrantLock，因此 Segment 就是可重入锁。</li><li>每个 Segment 包含多个 HashEntry，每个 HashEntry 是一个链表节点，包含键值对和指向下一个节点的引用。</li></ul><h2 id="并发操作机制"><a href="#并发操作机制" class="headerlink" title="并发操作机制"></a>并发操作机制</h2><ul><li><strong>并发读取</strong>：允许多个线程同时读取不同的 Segment 的数据，因为读取操作不需要锁定整个 Segment。</li><li><strong>并发写入</strong>：写入操作（如 put 和 remove）会在对应的 Segment 上加锁。这意味着不同 Segment 的写入操作可以同时进行，而相同 Segment 的写入操作则需要等待锁。</li></ul><p><strong>示例场景</strong></p><p>假设有 ConcurrentHashMap 内部有四个Segment，线程A和线程B分别向不同的键写入值，他们通过哈希算法定位到不同的Segment。因此，线程A和B可以同时进行操作，不会互相堵塞，从而实现高效的并发。</p><p>那假如说线程A和B通过哈希算法定位到同一个Segment呢？</p><p><strong>当多个线程定位到同一个 Segment 时，它们必须获得该 Segment 的锁才能进行任何操作。这意味着在任何时候，每个Segment只允许一个线程进行操作</strong>。</p><h2 id="扩容机制"><a href="#扩容机制" class="headerlink" title="扩容机制"></a>扩容机制</h2><h3 id="触发扩容"><a href="#触发扩容" class="headerlink" title="触发扩容"></a><strong>触发扩容</strong></h3><ul><li>当任何一个 Segment 的填充度达到阈值时（基于负载因子和 Segment 的容量），就会触发该 Segment 的扩容。</li><li>扩容是针对于单个 Segment 的，而不是整个 ConcurrentHashMap。</li><li>如果一个线程在进行插入操作时触发了扩容，该线程会锁定对应的 Segment 并开始扩容过程，其他试图访问同一个 Segment 的线程将会被堵塞（其他的Segment不会受影响），直到扩容完成。</li></ul><h3 id="扩容过程"><a href="#扩容过程" class="headerlink" title="扩容过程"></a><strong>扩容过程</strong></h3><p>当某个 Segment 的填充度达到设定阈值时，该 Segment 将独立触发扩容，这个过程包括：</p><ol><li>创建一个新的哈系统数组，大小通常是原数组的两倍。</li><li>重新计算旧数据的哈希值并将数据迁移到新数组。</li><li>迁移完成后，将旧 Segment 的引用更新为新 Segment。</li></ol><p><strong>利弊点</strong>：</p><ul><li>这种扩容机制减少了整个 ConcurrentHashMap 范围内的锁竞争，因为只有被扩容的 Segment 会被锁定，扩容和数据迁移通常由触发扩容的单个线程完成的。</li><li>然而会存在局限性，在单个 Segment 达到较大时，扩容过程仍可能达到性能瓶颈，因为其他线程需要等待扩容线程完成操作。</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>提高数据访问效率<ul><li>在不同线程访问不同 Segment 的场景中，由于锁是分段的，线程间几乎不会相互堵塞，这大大提高了数据访问的效率。</li></ul></li><li>减少锁的范围<ul><li>锁分段技术通过仅锁定部分数据结构，来减少锁的范围，这意味着更少的线程会因为等待锁被堵塞。</li></ul></li><li>降低锁竞争<ul><li>锁分段机制有效降低了竞争，特别是有大量读操作和一定量写操作的情况下。</li></ul></li></ol><h2 id="JDK-8及以后"><a href="#JDK-8及以后" class="headerlink" title="JDK 8及以后"></a>JDK 8及以后</h2><p><strong>ConcurrentHashMap 的进化</strong></p><p>在JDK8及之后的版本中，ConcurrentHashMap的内部结构和锁机制有了重大改进，主要是引入了<code>红黑树</code> 来优化长链表带来的性能问题，并且取消了 Segment 的使用，改为直接对节点进行分散加锁。</p><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a><strong>数据结构</strong></h3><ul><li><strong>Node数组</strong>：使用统一的 Node 数组替代了旧版本的多个 Segment。每个 Node 包含了一个 key, value 及指向下一个 Node 的引用。</li><li><strong>链表和红黑树</strong>：当链表长度超过设定阈值时（默认为8），自动转化为红黑树，提升查找效率。</li></ul><h3 id="底线实现"><a href="#底线实现" class="headerlink" title="底线实现"></a><strong>底线实现</strong></h3><ul><li><strong>CAS操作</strong>：用于无锁的读取和高效的更新，实现原子性操作。</li><li><strong>Synchronized 同步块</strong>：控制复杂的写操作，如转换链表为红黑树。</li></ul><h3 id="特性和并发控制"><a href="#特性和并发控制" class="headerlink" title="特性和并发控制"></a>特性和并发控制</h3><ul><li><strong>并发读取</strong>：无锁操作允许多线程可以同时进行读取操作，读取操作是完全无锁的。</li><li><strong>并发写入</strong>：写入操作使用了 CAS 操作和 synchronized 块。CAS 用于简单的更新，而 synchronized 块用于删除或（链表转红黑树）操作。</li></ul><p><strong>示例场景</strong></p><p>假设有两个线程：线程A和线程B，两个线程分别向ConcurrentHashMap中添加值：</p><ul><li>两个键经过哈希运算之后，分别发生在不同的key中，那么更新操作主要依赖于CAS操作来安全地完成更新，而不需要锁定这个段。</li><li>如果两个键恰好映射到同一个桶，且其中一个操作需要更复杂的同步（如转换链表为红黑树或删除操作），则会使用synchronized来锁定这个特定的桶或节点，另外一个线程需要被堵塞。</li></ul><p>这种新的机制减少了锁的使用，提高了读取效率和写操作的并发控制能力。允许更多线程无锁地读取数据，同时仍然保持一定程度的写入性能。</p><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><h3 id="触发条件"><a href="#触发条件" class="headerlink" title="触发条件"></a><strong>触发条件</strong></h3><ul><li>扩容通常在两种情况下被触发：当桶的数量不足以容纳当前元素时，或者单个链表的长度过长，影响了查询效率。</li></ul><h3 id="扩容过程-1"><a href="#扩容过程-1" class="headerlink" title="扩容过程"></a><strong>扩容过程</strong></h3><ol><li>新建数组：<ul><li>当扩容开始时，创建一个新的<code>节点数组</code>，其容量是原数组的两倍。</li></ul></li><li>数据迁移：<ul><li>ConcurrentHashMap 会将旧数组中的每个桶（bucket）中的节点迁移到新数组中。整个过程是并发进行的，多个线程可以同时参与到不同桶的迁移过程中。</li></ul></li><li>重哈希：<ul><li>每个节点在迁移时需要重新计算其在新数组中的位置。这是因为数组大小改变后，原哈希值映射到新数组的位置也可能发生变化。</li></ul></li><li>转移节点：<ul><li>对于链表中的每个节点，根据其哈希值确定它在新数组中的位置，并将其移动到新位置。如果原链表已经转换为红黑树，那么红黑树也会重新组织并迁移到新数组中。</li></ul></li></ol><p><strong>并发控制：</strong></p><p>在扩容过程中，ConcurrentHashMap 使用了精细的并发控制机制来确保数据一致性和线程安全。它允许多个线程同时参与扩容过程，每个线程可以处理一部分数据的迁移，从而加快整体扩容过程。</p><h3 id="锁定特定桶（bucket）"><a href="#锁定特定桶（bucket）" class="headerlink" title="锁定特定桶（bucket）"></a>锁定特定桶（bucket）</h3><ol><li><strong>粒度更细的锁</strong>：锁的粒度更细。它不再锁定整个Segment，而是锁定特定的桶。</li><li><strong>使用synchronized关键字</strong>：Synchronized关键字来锁定特定的桶。当一个线程想要访问或修改一个桶时，它必须首先获取该桶的锁。</li><li><strong>每个桶作为独立的锁</strong>：每个桶都作为一个独立的锁，可以被单独锁定或解锁。这减少了线程的竞争，因为不同的线程可以锁定不同的桶。</li></ol><p><strong>工作流程</strong></p><ol><li>**计算桶的索引：**当一个线程想要添加、删除或访问一个元素时，它首先会计算元素的hash值。</li><li><strong>获取桶的锁</strong>：线程在操作桶之前必须首先获取该桶的锁。如果锁不可用（已经被其他线程持有），线程将被堵塞，直到锁变为可用。</li><li><strong>操作桶</strong>：一旦线程获取了桶的锁，它就可以安全地修改该桶。</li><li><strong>释放锁</strong>：操作完成后，线程必须释放桶的锁，以便其他线程可以锁定该桶。</li></ol><p>通过这种方式，Java 8的ConcurrentHashMap允许多个线程同时访问和修改不同的桶，从而实现了更高的并发性和性能。同时，通过锁定特定的桶，它确保了线程安全性，防止了多个线程同时修改同一个桶的情况。</p><h2 id="CAS和Synchronized在ConcurrentHashMap中的应用"><a href="#CAS和Synchronized在ConcurrentHashMap中的应用" class="headerlink" title="CAS和Synchronized在ConcurrentHashMap中的应用"></a>CAS和Synchronized在ConcurrentHashMap中的应用</h2><h3 id="CAS"><a href="#CAS" class="headerlink" title="CAS"></a><strong>CAS</strong></h3><p><code>CAS</code> 操作是一种无锁的同步机制。它包括三个操作数：<code>内存位置（要操作的数据）</code>、<code>预期原值</code>和<code>新值</code>。CAS首先检查<code>内存位置的当前值</code>是否与<code>预期原值</code>相同，如果相同，就将此位置更新为<code>新值</code>。整个过程是<code>原子的</code>。</p><ol><li>用于插入操作：<ul><li>当插入一个新的键值对时，ConcurrentHashMap 使用CAS操作来确保线程安全。</li><li>它首先计算键的哈希值，然后使用这个哈希值来找到对应的桶。</li><li>使用CAS操作来插入新的节点到桶中，如果CAS操作失败（说明有其他线程已经修改了这个桶），它会重试操作，直到成功。</li></ul></li><li>用于删除操作：<ul><li>当删除一个键值对时，ConcurrentHashMap 同样使用CAS操作。</li><li>它找到要删除的节点，然后使用CAS操作来安全地从桶中删除节点。</li></ul></li><li>用于扩容：<ul><li>当ConcurrentHashMap的大小超过一定的阈值，它会进行扩容。</li><li>扩容操作也使用CAS来确保只有一个线程可以扩容数组，并且在扩容过程中，其他线程还可以继续安全地访问和修改 ConcurrentHashMap。</li></ul></li></ol><h3 id="Synchronized"><a href="#Synchronized" class="headerlink" title="Synchronized"></a><strong>Synchronized</strong></h3><ol><li>复杂结构的修改：<ul><li>当链表长度超过阈值，将链表转换为红黑树时，需要锁定整个链表。这种转换是复杂的结构修改，涉及多个节点，因此使用 synchronized 来确保在修改期间，没有其他线程可以修改这些节点。</li></ul></li><li>删除操作：<ul><li>在删除过程中，synchronized 被用来锁定特定节点或整个桶，保证操作的原子性和一致性。</li></ul></li></ol><h2 id="为什么使用CAS"><a href="#为什么使用CAS" class="headerlink" title="为什么使用CAS"></a>为什么使用CAS</h2><ol><li>无锁优势：<ul><li>CAS 提供了一种<code>无锁</code>的同步方式。相比传统的锁机制，CAS在多线程环境下可以减少堵塞和上下文切换的开销，减少线程之间的竞争，提高系统整体性能。</li></ul></li><li>原子性：<ul><li>CAS 保证了原子性，无需使用复杂的锁定机制。</li></ul></li><li>避免死锁：<ul><li>CAS 可以避免死锁的发生，因为不会像传统锁那样持有资源。</li></ul></li></ol><p><strong>替代方案的局限性</strong></p><ul><li><strong>悲观锁</strong>：使用传统的悲观锁（synchronized 或 ReentrantLock）可能会导致在高并发环境下性能下降。</li><li><strong>自旋锁</strong>：自旋锁可以作为一种替代方案，但它在等待锁释放时会占用处理器时间，这在长时间等待的情况下可能不高效。</li></ul><h2 id="Java-8-ConcurrentHashMap-引入了什么新的技术？"><a href="#Java-8-ConcurrentHashMap-引入了什么新的技术？" class="headerlink" title="Java 8 ConcurrentHashMap 引入了什么新的技术？"></a>Java 8 ConcurrentHashMap 引入了什么新的技术？</h2><ol><li>统一的 Node 数组：<ul><li>使用单个统一的 Node 数组替代了分段的 Segment 数组。每个 Node 包含了一个键值对。</li></ul></li><li>CAS 操作：<ul><li>CAS用于在无锁的情况下实现安全的更新。在更新节点时不需要锁定整个结构。</li></ul></li><li>synchronized 同步块：<ul><li>对于需要同步的操作（如节点的删除或整个桶的更新），使用了内部的 synchronized 同步块来控制。</li></ul></li><li>引入链表和红黑树：<ul><li>在链表长度超过一定阈值时，链表转换为红黑树，这提高了查找效率。</li></ul></li></ol><h2 id="为什么要移除掉JDK7中的分段锁Segment？"><a href="#为什么要移除掉JDK7中的分段锁Segment？" class="headerlink" title="为什么要移除掉JDK7中的分段锁Segment？"></a>为什么要移除掉JDK7中的分段锁Segment？</h2><ol><li>提高并发性和性能：<ul><li>分段锁虽然允许多个线程同时访问不同的Segment，但如果多个线程修改同一个Segment，它们仍然会堵塞。堵塞意味着线程会被挂起，操作系统需要进行上下文切换，阻塞和上下文切换可能导致 CPU 资源的不充分利用，因为等待锁的线程不能执行任何操作。</li><li>在 Java 8中，通过锁定特定的桶而不是整个Segment，引入CAS和 Synchronized 减少锁的竞争，可以进一步提高并发读。CAS 自旋是在用户态完成的，不涉及操作系统层面的线程挂起或上下文切换。</li></ul></li><li>减少内存开销：<ul><li>每个Segment对象都是一个额外的内存开销，不需要再为每个 Segment 维护独立的哈希表和锁结构，降低了内存占用。</li></ul></li><li>优化扩容过程：<ul><li>新的扩容机制允许多个线程协作进行，减少了扩容期间的堵塞和性能损失。</li></ul></li></ol><h2 id="ConcurrentHashMap-一定是线程安全的吗？"><a href="#ConcurrentHashMap-一定是线程安全的吗？" class="headerlink" title="ConcurrentHashMap 一定是线程安全的吗？"></a>ConcurrentHashMap 一定是线程安全的吗？</h2><p>高并发场景下，当涉及到符合操作的情况下，ConcurrentHashMap 还是会出现数据不一致的情况。</p><p>ConcurrentHashMap 本身确保了单个操作，如<code>get</code>、<code>put</code>、<code>remove</code> 等的线程安全性，这意味着每个这样的操作都是原子的。然而，当这些操作组合在一起形成复合逻辑时，例如：</p><ol><li><p>检查后执行（Check-then-act）：如先通过</p><pre><code>get</code></pre><p>检查键是否存在，然后根据检查结果决定是否执行</p><pre><code>put</code></pre><p>。在多线程环境中，即使这两个操作各是安全的，组合在一起就可能不再是线程安全的。</p><ul><li><strong>问题示例</strong>：如果两个线程同时发现某个键不存在，它们可能各自创建一个新的值，并尝试将其插入到<code>ConcurrentHashMap</code> 中。这可能会导致后一个线程的操作覆盖前一个线程的操作。</li></ul></li></ol><p><strong>避免措施</strong></p><ol><li>使用原子操作：<code>ConcurrentHashMap</code> 提供了一些复合操作，如<code>putIfAbsent</code>、<code>compute</code> 、<code>computeIfAbsent</code> 和<code>computeIfPresent</code> ，这些方法可以安全地执行复合逻辑，确保整个过程的原子性。</li></ol><h3 id="computeIfAbsent"><a href="#computeIfAbsent" class="headerlink" title="computeIfAbsent"></a>computeIfAbsent</h3><ul><li>这个方法用于在<code>键不存在</code>时计算其值，并将其添加到 HashMap 中。</li><li>方法签名：computeIfAbsent(K key, Function mappingFunction)</li><li>工作原理：如果给定的键在 HashMap 中不存在或其值为 null，computeIfAbsent 会使用提供的映射函数计算其值，并将键值对添加到 HashMap 中。如果键已经存在，则不执行任何操作。</li><li>应用场景：常用于缓存实现，或者你希望键不存在时自动计算并存储值时。</li></ul><pre class="line-numbers language-language-java"><code class="language-language-java">Map<String, Integer> map = new HashMap<>();map.put("apple", 3);// 使用 computeIfAbsent 更新不存在的键 "banana" 的值map.computeIfAbsent("banana", k -> 6);System.out.println(map); // 输出: &#123;apple=3, banana=6&#125;// 使用 computeIfAbsent 尝试更新存在的键 "apple" 的值map.computeIfAbsent("apple", k -> 10);System.out.println(map); // 输出: &#123;apple=3, banana=6&#125;，"apple" 的值没有改变<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="computeIfPresent"><a href="#computeIfPresent" class="headerlink" title="computeIfPresent"></a>computeIfPresent</h3><ul><li>这个方法用于在<code>键存在</code>且值非null时，重新计算并更新其值。</li><li>方法签名：computeIfPresent(K key, BiFunction remappingFunction)</li><li>工作原理：如果给定的键在 HashMap 中存在且其值非 null，computeIfPresent 会使用提供的重新映射函数根据旧值计算新值，并替换旧值。如果计算出的新值为 null，则删除该建。</li><li>应用场景：常用于需要基于旧值更新键值对的场合。</li></ul><pre class="line-numbers language-language-java"><code class="language-language-java">Map<String, Integer> map = new HashMap<>();map.put("apple", 3);// 使用 computeIfPresent 更新存在的键 "apple" 的值map.computeIfPresent("apple", (k, v) -> v + 2);System.out.println(map); // 输出: &#123;apple=5&#125;// 使用 computeIfPresent 尝试更新不存在的键 "banana" 的值map.computeIfPresent("banana", (k, v) -> v + 2);System.out.println(map); // 输出: &#123;apple=5&#125;，没有 "banana" 键<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 并发集合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ConcurrentHash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ 整理</title>
      <link href="/2025/02/26/rabbitmq/"/>
      <url>/2025/02/26/rabbitmq/</url>
      
        <content type="html"><![CDATA[<h2 id="消息队列-RabbitMQ"><a href="#消息队列-RabbitMQ" class="headerlink" title="消息队列-RabbitMQ"></a>消息队列-RabbitMQ</h2><p>RabbitMQ 基于 AMQP 协议，Advanced Message Queuing Protocol（高级消息队列协议），是一个网络协议，是应用层协议的一个开放标准，为面向消息中间设计的。基于这个协议的客户端和消息中间件可以传递消息，并且不会因为客户端或中间件产品的不同所受限，也不会受到不同开发语言的限制。</p><h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><ol><li>灵活的路由能力<ul><li>RabbitMQ 提供了多种交换机类型（如 direct、fanout、topic、headers），这些交换器支持灵活的消息路由机制。</li><li>它允许精确控制消息应该被发送到哪个队列，支持复杂的路由方案。</li></ul></li><li>高可靠性和持久性<ul><li>RabbitMQ 支持消息和队列的持久化，确保在系统故障时消息不会丢失。</li><li>它支持镜像队列（mirrored queues）来提高可用性，这通过在多个节点上复制队列来实现。</li></ul></li><li>多种协议和语言支持<ul><li>RabbitMQ 支持多种消息协议，包括 AMQP、STOMP、MQTT 等。</li><li>它提供了广泛的客户端库支持，适用于各种编程语言，如 Java、Python、Ruby、.NET等。</li></ul></li><li>集群和负载均衡<ul><li>RabbitMQ 可以在多个服务器上形成集群，提高吞吐量和可靠性。</li><li>它支持负载均衡和故障转移，帮助保持应用的高可用性。</li></ul></li><li>管理和监控<ul><li>RabbitMQ 附带管理界面，允许用户轻松管理和监控他们的消息系统。</li><li>提供了丰富的监控和管理 API，方便集成到现有的监控系统。</li></ul></li><li>易于扩展和集成<ul><li>它的插件系统使得扩展 RabbitMQ 功能变得简单，可以添加新的功能或集成到其他系统中。</li><li>社区提供了许多插件，例如用于消息追踪、Shovel 插件用于在多个 RabbitMQ 实例之间迁移消息等。</li></ul></li><li>事务支持<ul><li>RabbitMQ 提供事务功能，允许将消息的发送、接受和确认操作组合为单个原子操作。</li><li>虽然事务会增加一定的开销，但它们对于确保消息处理的完整性非常有用。</li></ul></li><li>灵活的消息处理模式<ul><li>支持多种消息处理模式，包括简单的点对点通信、工作队列模式、发布&#x2F;订阅模式等。</li><li>通过不同的交换器和队列配置，可以灵活地实现各种消息处理需求。</li></ul></li></ol><h3 id="主要结构"><a href="#主要结构" class="headerlink" title="主要结构"></a>主要结构</h3><p><img src="http://www.hyxiaoblog.com/images/21/rabbitmq_model.png" alt="img"></p><ol><li>交换器（Exchanges）<ul><li>交换器是 RabbitMQ 消息路由的核心组件，它负责接收生产者发送的消息并根据路由规则将它们转发到一个或多个队列。</li><li>RabbitMQ 提供了几种类型的交换器：<code>direct</code>、<code>topic</code>、<code>fanout</code> 和 <code>headers</code>，每种类型的交换器都有其特定的路由逻辑。</li></ul></li><li>队列（Queues）<ul><li>队列是 RabbitMQ 中的基本存储结构，用于存储待消费的消息。</li><li>队列与交换器之间通过绑定关系（Binding）建立连接，决定了哪些消息应该被路由到特定的队列。</li></ul></li><li>绑定（Bindings）<ul><li>绑定是交换器和队列之间的关系，它基于路由键（routing key）和（对于某些交换器类型）绑定键（binding key）来路由消息。</li><li>绑定可以包含路由键匹配规则，尤其是在适用 topic 交换器时。</li></ul></li><li>生产者（Producers）<ul><li>生产者是发送消息到交换器的应用程序或服务。</li><li>生产者决定将消息发送到哪个交换器，并指定消息的路由键。</li></ul></li><li>消费者（Consumers）<ul><li>消费者从队列中提取消息并处理它们。</li><li>消费者可以监听一个或多个队列，以接收和处理存储在队列中的消息。</li></ul></li><li>消息（Messages）<ul><li>消息是 RabbitMQ 传递的数据单元，由生产者创建并发送到交换器。</li><li>每个消息都包含有效负载（payload，即实际数据）和标签（header），其中可能包括路由键、消息优先级、过期时间等信息。</li></ul></li><li>虚拟主机（Virtual Hosts）<ul><li>虚拟主机提供了一种隔离方式，可以在同一个 RabbitMQ 服务器上运行多个独立的消息系统。</li><li>每个虚拟主机都有自己的队列、交换器和绑定。</li></ul></li><li>用户和权限<ul><li>RabbitMQ 允许你定义多个用户，并且可以给每个用户分配不同的角色和权限。</li><li>这种权限控制可以用来限制对特定队列、交换器和其他资源的访问。</li></ul></li></ol><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li><strong>生产者</strong>创建一条消息，这条消息包含有效负载（即实际要传递的数据）和一些元数据（如路由键）。</li><li>生产者将这条消息发送到一个<strong>交换器</strong>。生产者在发送消息时指定<strong>交换器</strong>和<strong>路由键</strong>。</li><li>交换器接收到消息后，根据<strong>交换类型</strong>（如 direct、fanout、topic）和消息的路由键来决定如何路由消息。</li><li>在<strong>binding key</strong>的帮助下，交换器将消息路由到一个或多个<strong>队列</strong>。绑定定义了交换器和队列之间的关系，并可能包含与路由键匹配的模式。</li><li>一旦消息被路由到某个队列，它将在那里等待，直到出现一个<strong>消费者</strong>准备好接收它。</li><li>如果消息配置了消息持久化，消息将在磁盘上存储，以防止在系统崩溃时丢失。</li><li>消费者从队列中接收消息，并进行处理。消费者可以是任何能够连接到队列并读取消息的应用程序或服务。</li><li>消费者可以手动或自动确认消息。确认（acknowledge）是否告诉 RabbitMQ 消息已经被接收并处理。</li><li>一旦消息被正确处理，消费者会发送一个确认给 RabbitMQ。</li><li>如果消费者因为某种原因无法处理消息，它可以拒绝（reject）或将消息重回队列（nack）。</li><li>当 RabbitMQ 收到确认后，它将从队列中移除该消息。</li><li>如果在处理消息时发生故障，系统可以配置为重试机制或将消息移动到死信队列（dead-letter queue）。</li></ol><p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/e21bc9b8-8a43-4e48-b3f9-1dee429ddef2/Screenshot_2024-07-10-13-19-25-957_com.android.chrome-edit.jpg" alt="Screenshot_2024-07-10-13-19-25-957_com.android.chrome-edit.jpg"></p><h3 id="Exchanges-交换器"><a href="#Exchanges-交换器" class="headerlink" title="Exchanges 交换器"></a>Exchanges 交换器</h3><p>在RabbitMQ中，交换器（Exchanges）是决定如何路由消息的关键组件。它们接收来自生产者的消息，并根据特定规则将它们路由到一个或多个队列。</p><p>RabbitMQ 提供了四种主要的交换器类型，每种类型根据不同的规则路由消息。</p><ol><li><p><strong>Direct Exchange</strong></p><ul><li><strong>工作原理</strong>：根据消息的路由键（routing key）将消息路由到具有相同绑定值（binding key）的队列。</li><li><strong>用途</strong>：当你想要将消息发送到指定的队列时，可以使用 direct exchanges。它适用于一对一的消息路由场景。</li><li><strong>示例场景</strong>：订单服务向指定的处理队列发送订单更新消息。</li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">// 创建 Direct Exchange@Beanpublic DirectExchange directExchange() &#123;    return new DirectExchange("direct_exchange");&#125;// 创建队列@Beanpublic Queue orderQueue() &#123;    return new Queue("order_queue");&#125;// 绑定队列到交换器@Beanpublic Binding bindingDirect(Queue orderQueue, DirectExchange directExchange) &#123;    return BindingBuilder.bind(orderQueue).to(directExchange).with("order_key");&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>Fanout Exchange</strong></p><ul><li><strong>工作原理</strong>：忽略路由键，将消息广播到所有绑定到该交换器的队列。</li><li><strong>用途</strong>：当你需要将相同的消息发送到多个队列时，fanout exchange 非常有用。它实现发布&#x2F;订阅模式。</li><li><strong>示例场景</strong>：广播系统通知或实时更新，如股票价格变动。</li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">// 创建 Fanout Exchange@Beanpublic FanoutExchange fanoutExchange() &#123;    return new FanoutExchange("fanout_exchange");&#125;// 创建队列@Beanpublic Queue notificationQueue() &#123;    return new Queue("notification_queue");&#125;// 绑定队列到交换器@Beanpublic Binding bindingFanout(Queue notificationQueue, FanoutExchange fanoutExchange) &#123;    return BindingBuilder.bind(notificationQueue).to(fanoutExchange);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>Topic Exchange</strong></p><ul><li><strong>工作原理</strong>：根据消息的路由键和队列的绑定键（可以包含通配符）进行模糊匹配，来决定消息路由到哪些队列。</li><li><strong>用途</strong>：当你需要根据特定模式或多个标准路由消息时，topic exchange 是最佳选择。它提供了很高的灵活性。</li><li><strong>示例场景</strong>：向不同的日志记录系统发送不同级别（如 error，info，debug）的日志消息。</li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">// 创建 Topic Exchange@Beanpublic TopicExchange topicExchange() &#123;    return new TopicExchange("topic_exchange");&#125;// 创建队列@Beanpublic Queue logQueue() &#123;    return new Queue("log_queue");&#125;// 绑定队列到交换器@Beanpublic Binding bindingTopic(Queue logQueue, TopicExchange topicExchange) &#123;    return BindingBuilder.bind(logQueue).to(topicExchange).with("log.*");&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-language-java"><code class="language-language-java">@Componentpublic class LogMessagePublisher &#123;    @Autowired    private RabbitTemplate rabbitTemplate;    @Autowired    private TopicExchange topicExchange;    public void sendInfoLog(String message) &#123;        rabbitTemplate.convertAndSend(topicExchange.getName(), "log.info", message);    &#125;    public void sendErrorLog(String message) &#123;        rabbitTemplate.convertAndSend(topicExchange.getName(), "log.error", message);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>Headers Exchange</strong></p><ul><li><strong>工作原理</strong>：基于消息头（headers）中的一个或多个属性进行匹配，而不是路由键。匹配规则可以是“全部匹配”（all）或“任意匹配”（any）。</li><li><strong>用途</strong>：当你需要基于多个属性进行复杂的路由规则时，可以使用 headers exchanges。</li><li><strong>示例场景</strong>：基于消息内容的多个属性（如源、目的地、优先级）路由消息到不同的处理队列。</li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">// 创建 Headers Exchange@Beanpublic HeadersExchange headersExchange() &#123;    return new HeadersExchange("headers_exchange");&#125;// 创建队列@Beanpublic Queue processQueue() &#123;    return new Queue("process_queue");&#125;// 绑定队列到交换器@Beanpublic Binding bindingHeaders(Queue processQueue, HeadersExchange headersExchange) &#123;    Map<String, Object> map = new HashMap<>();    map.put("x-match", "all");    map.put("header1", "value1");    map.put("header2", "value2");    return BindingBuilder.bind(processQueue).to(headersExchange).whereAll(map).match();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h3 id="Queue-队列"><a href="#Queue-队列" class="headerlink" title="Queue 队列"></a>Queue 队列</h3><h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a><strong>持久化</strong></h3><ul><li><strong>持久化队列</strong>：在 RabbitMQ 中，队列可以被标记为持久化（durable）。持久化队列会在代理重启后自动重建，但这本身并不保证队列中的消息也是持久的。</li><li><strong>消息持久化</strong>：要确保消息不丢失，除了声明队列为持久化之外，发送到队列的每条消息也需要被标记为持久化。这意味着消息会被写入磁盘，尽管这可能会引入额外的性能开销。</li><li><strong>写入磁盘的时机</strong>：RabbitMQ 不会为每条消息写入磁盘，而是可能仅在必要时进行，这取决于消息代理的配置和当前的负载。这种机制提供了性能和持久性之间的平衡。</li></ul><p><strong>在代码中使用队列持久化</strong></p><p>声明一个持久化队列</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Beanpublic Queue myDurableQueue() &#123;    return new Queue("myDurableQueue", true); // true 表示队列是持久化的&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在这个示例中，<code>Queue</code> 的构造函数接受一个布尔值作为第二个参数，用来指定队列是否持久化。<code>true</code> 表示队列是持久化的。</p><p>对于消息的持久化，可以在发送消息时设置它的持久性。使用 <code>MessageProperties</code> 设置消息为持久化：</p><pre class="line-numbers language-language-java"><code class="language-language-java">MessageProperties messageProperties = new MessageProperties();messageProperties.setDeliveryMode(MessageDeliveryMode.PERSISTENT);Message message = new Message("Your message content".getBytes(), messageProperties);rabbitTemplate.send("myDurableQueue", message);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个示例中，<code>MessageDeliveryMode.PERSISTENT</code> 指定了消息应该被持久化。</p><h3 id="事务支持"><a href="#事务支持" class="headerlink" title="事务支持"></a><strong>事务支持</strong></h3><ul><li>RabbitMQ 中的事务允许你将一系列动作（如发送消息、确认消息）包装在一个事务中。事务保证了这些操作要么全部成功，要么全部失败，从而提供了一定程度的消息处理保证。</li><li>操作步骤：<ul><li>开启事务：通过 <code>channel.txSelect()</code> 开启事务。</li><li>执行操作：在事务中发送消息或执行其他操作。</li><li>提交或回滚：使用 <code>channel.txCommit()</code> 提交事务或使用 <code>channel.txRollback()</code> 回滚事务。</li></ul></li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">Channel channel = connection.createChannel();try &#123;    channel.txSelect();    channel.basicPublish("", "queue_name", null, message.getBytes());    // 可能的其他操作...    channel.txCommit();&#125; catch (Exception e) &#123;    channel.txRollback();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是这种事务机制在 RabbitMQ 中会显得影响消息吞吐量。由于每个事务都涉及到磁盘 I&#x2F;O 操作来确保状态的持久化，这会导致性能瓶颈。对于需要高吞吐量的系统，事务可能不是一个理想的选择。</p><p><strong>替代方案</strong></p><p><img src="http://www.hyxiaoblog.com/images/21/message_reliable.png" alt="img"></p><ul><li><p><strong>confirm 确认模式</strong></p><ul><li>消息一旦从生产者发送给 MQ Server，也就是被交换器接收到，这个时候会有一个回调 confirmCallback，代表 MQ Server 接收到了。不管消息是否消费成功，这个回调函数一定会被执行。</li></ul><p><strong>示例代码</strong>：在代码设置 confirm 确认模式</p><pre class="line-numbers language-language-java"><code class="language-language-java">/*** correlationData: 相关性数据* ack: 交换机是否成功接收到消息，true：成功* cause: 失败的原因（成功则为null）*/rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -> &#123;    if (ack) &#123;        log.info("交换机接收消息成功~~ &#123;&#125;", cause);        // 确认成功逻辑    &#125; else &#123;        log.info("交换机接收消息失败~~ &#123;&#125;", cause);        // 确认失败逻辑    &#125;&#125;);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>return 回退模式</strong></p><ul><li>如果交换器无法根据自身的类型和路由键或绑定规则将消息路由到一个队列，它会调用生产者的回退回调函数（<code>returnCallback</code>）。这通常意味着消息无法送达任何队列。</li></ul><p><strong>示例代码</strong>：在代码设置 return 回退模式</p><pre class="line-numbers language-language-java"><code class="language-language-java">rabbitTemplate.setReturnsCallback(returnedMessage -> &#123;    log.info("return");    log.info("returnedMessage:&#123;&#125;", returnedMessage);&#125;);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p><strong>结合使用</strong></p><p>通常情况下，<code>confirmCallback</code> 和 <code>returnCallback</code> 被一起使用，以确保消息不仅被交换器接收，而且能被正确路由到队列。</p><pre class="line-numbers language-language-java"><code class="language-language-java">rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -> &#123;    if (ack) &#123;        // 确认成功逻辑    &#125; else &#123;        // 确认失败逻辑，例如记录日志、重试等    &#125;&#125;);rabbitTemplate.setReturnCallback((message, replyCode, replyText, exchange, routingKey) -> &#123;    // 处理无法路由的消息，例如记录日志、通知生产者等&#125;);rabbitTemplate.setMandatory(true); // 设置为 true，以确保触发 returnCallback<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>设置了 <code>setMandatory(true)</code> 之后，如果消息无法被正确路由到队列，<code>returnCallback</code> 会被触发。如果没有设置 <code>mandatory</code> 参数，无法路由的消息会被 RabbitMQ 丢弃，生产者不会收到通知。</p><h3 id="死信队列"><a href="#死信队列" class="headerlink" title="死信队列"></a><strong>死信队列</strong></h3><p>死信队列（Dead Letter Queue, DLQ）是用来收集死信的队列。在消息队列服务中，死信通常是指<strong>那些无法被正常消费的消息</strong>。有几种情况会导致消息称为死信：</p><ol><li><strong>消息被拒绝（Rejected）</strong>：消费者收到消息后，可以选择拒绝它，这是可以设置是否重新入队。如果消费者拒绝消息且不重新入队，该消息就会成为死信。</li><li><strong>消息过期</strong>：如果消息在队列中存在时间超过了设置的TTL（Time-To-Live），它就会变成信息。</li><li><strong>队列达到最大长度</strong>：如果队列满了，新消息就会变成死信。</li></ol><p>在 RabbitMQ 中，你可以通过配置队列的参数来指定死信交换器（DLX）和死信路由键（DLK）。当消息成为死信后，它会被发送到 DLX 指定的交换器，并根据 DLK 进行路由。</p><p><strong>示例代码</strong>：配置死信队列</p><pre class="line-numbers language-language-java"><code class="language-language-java">Map<String, Object> args = new HashMap<>();args.put("x-dead-letter-exchange", "dlx_exchange");args.put("x-dead-letter-routing-key", "dlx_routing_key");@Beanpublic Queue myQueue() &#123;return new Queue("myQueue", true, false, false, args);&#125;@Beanpublic DirectExchange deadLetterExchange() &#123;return new DirectExchange("dlx_exchange");&#125;@Beanpublic Queue dlQueue() &#123;return new Queue("deadLetterQueue");&#125;@Beanpublic Binding dlBinding() &#123;return BindingBuilder.bind(dlQueue()).to(deadLetterExchange()).with("dlx_routing_key");&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个配置中，如果 <code>myQueue</code> 中的消息变成了死信，它会被路由到 <code>deadLetterQueue</code> 队列。</p><h3 id="延迟队列"><a href="#延迟队列" class="headerlink" title="延迟队列"></a><strong>延迟队列</strong></h3><p>延迟队列（Delayed Queue）是一种特殊类型的队列，消息被发送到这个队列之后不会立即被消费，而是会在延迟一定时间后才能被消费。</p><p>需要注意的是，RabbitMQ 本身默认不支持延迟队列，但是可以通过<strong>安装插件</strong>或者<strong>使用消息TTL和死信队列的组合</strong>来实现相似的功能。</p><ol><li><strong>安装插件</strong></li></ol><ul><li><p>先通过命令行 <code>rabbitmqctl veriosn</code> 得到mq的版本号，根据版本号去下载插件</p></li><li><p>下载延迟插件 <a href="https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases">https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases</a></p></li><li><p>下载好了之后，传到 Linux 服务器</p></li><li><p>然后再从虚拟机拷贝到 docker 的 rabbitmq 插件中：<code>docker cp rabbitmq_delayed_message_exchange-3.9.0.ez rabbitmq:/plugins</code></p></li><li><p>运行命令开启延迟插件，<code>rabbitmq-plugins enable rabbitmq_delayed_message_exchange</code></p></li><li><p>查看插件列表是否存在延迟插件，<code>rabbitmq-plugins list</code></p></li><li><p><code>ctrl+d</code>退出控制台并且重启rabbitmq，<code>docker restart rabbitmq</code></p><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Configurationpublic class DelayConfig_Article &#123;    // 定义交换机的名称    public static final String EXCHANGE_DELAY_ARTICLE = "exchange_delay_article";    // 定义队列的名称    public static final String QUEUE_DELAY_ARTICLE = "queue_delay_article";    // 统一定义路由key    public static final String DELAY_DISPLAY_ARTICLE = "delay.display.article";    // 创建交换机    @Bean(EXCHANGE_DELAY_ARTICLE)    public Exchange exchange() &#123;        return ExchangeBuilder                    .topicExchange(EXCHANGE_DELAY_ARTICLE)                    .durable(true)                    .delayed()              // 设置延迟特性                    .build();    &#125;    // 创建队列    @Bean(QUEUE_DELAY_ARTICLE)    public Queue queue() &#123;        return QueueBuilder                .durable(QUEUE_DELAY_ARTICLE)                .build();    &#125;    // 创建绑定关系    @Bean    public Binding delayBindingArticle(@Qualifier(EXCHANGE_DELAY_ARTICLE) Exchange exchange,                                        @Qualifier(QUEUE_DELAY_ARTICLE) Queue queue) &#123;        return BindingBuilder                .bind(queue)                .to(exchange)                .with("delay.display.*")                .noargs();    &#125;    /**     * 设置消息属性处理器，目的是设置延迟的时间     * @param times     * @return     */    public static MessagePostProcessor setDelayTimes(Integer times) &#123;        return message -> &#123;            message.getMessageProperties().setDeliveryMode(MessageDeliveryMode.PERSISTENT);            message.getMessageProperties().setDelay(times);            return message;        &#125;;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-language-java"><code class="language-language-java">...Integer delayTimes = 10 * 1000;MessagePostProcessor processor = DelayConfig_Article.setDelayTimes(delayTimes);rabbitTemplate.convertAndSend(DelayConfig_Article.EXCHANGE_DELAY_ARTICLE,                                DelayConfig_Article.DELAY_DISPLAY_ARTICLE,                                message,                                processor);...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>这个插件扩展了 RabbitMQ 的功能，允许创建一个自定义的交换器类型 <code>x-delayed-message</code>，这种类型的交换器可以接收带有延迟信息的消息，并在指定的延迟时间后将消息路由到绑定的队列。</p><ul><li>使用 <code>.delayed()</code>方法标记交换器以支持延迟消息。</li><li>在发送消息时，通过<code>MessagePostProcessor</code>设置消息的延迟时间，这是通过设置消息属性中的 <code>x-delay</code>来实现的。</li><li>当消息发送到这个交换器时，它不会立即被路由到一个队列，而是会在经过指定的延迟时间之后被路由。</li></ul><ol><li><p><strong>使用TTL和DLX实现延迟队列：</strong></p><pre class="line-numbers language-language-java"><code class="language-language-java">@Beanpublic Queue delayProcessQueue() &#123;    Map<String, Object> args = new HashMap<>();    args.put("x-dead-letter-exchange", "dlx_exchange");    args.put("x-dead-letter-routing-key", "process_routing_key");    args.put("x-message-ttl", 60000);    // 消息TTL为60000ms(1分钟)    return new Queue("delayQueue", true, false, false, args);&#125;@Beanpublic DirectExchange processExchange() &#123;    return new DirectExchange("processExchange");&#125;@Beanpublic Queue processQueue() &#123;    return new Queue("processQueue");&#125;#Beanpublic Binding processBinding() &#123;    return BindingBuilder.bind(processQueue()).to(processExchange()).with("process_routing_key");&#125;@Beanpublic Binding delayBinding() &#123;    return BindingBuilder.bind(delayProcessQueue()).to(processExchange()).with("delay_routing_key");&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>在这个示例中，<code>delayQueue</code> 是一个带有 TTL 和 DLX 配置的队列。消息会在这个队列中延迟一定时间（这里是1分钟），然后通过 DLX 被发送到 <code>processQueue</code> 队列进行处理。</p><p>使用 TTL 和 DLX 实现的延迟队列的<strong>工作流程</strong>大致如下：</p><ol><li><p>生产者（<code>producer</code>）将消息发送到一个普通的交换器（<code>processExchange</code>）。</p></li><li><p>该交换器将消息路由到配置有 TTL 和 DLX 的队列（<code>delayQueue</code>）。</p></li><li><p>消息在<code>delayQueue</code>中停留，直到其 TTL 过期。如果消息在 TTL 时间内没有被消费，它就会变成死信。</p></li><li><p>一旦消息成为死信，它会被发送到配置在<code>delayQueue</code>上的DLX（<code>dlx_exchange</code>）。</p></li><li><p>DLX 交换器（<code>dlx_exchange</code>）将这个死信路由到最终的处理队列（<code>processQueue</code>）。</p></li><li><p>消费者（<code>comsumer</code>）监听<code>processQueue</code>，准备处理消息。</p><pre class="line-numbers language-language-rust"><code class="language-language-rust">producer -> processExchange -> delayQueue --(after TTL)--> DLX(dlx_exchange) -> processQueue -> consumer<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>这里的<code>DLX</code>是<code>delayQueue</code>配置的死信交换器，它负责接收<code>delayQueue</code>中过期的消息，并将它们路由到<code>processQueue</code>。需要确保<code>delayQueue</code>的DLX指向了正确的交换器，这个交换器会将消息路由到<code>processQueue</code>。</p><h3 id="重回队列"><a href="#重回队列" class="headerlink" title="重回队列"></a><strong>重回队列</strong></h3><p>在 RabbitMQ 中，“重回队列”（Requeue）是一个过程，当消费者因为某些原因不能处理接收到的消息时，它可以选择将消息发送回队列中以便稍后重新消费。</p><p><strong>重回队列适合场景</strong></p><ol><li><p>手动拒绝消息：</p><ul><li><p>当消费者接收到消息但不想或不能立即处理时，它可以使用 <code>basicReject</code> 或 <code>basicNack</code> 方法拒绝消息，并将 <code>requeue</code> 标志设置为 <code>true</code>，这会将消息重新放回队列。</p><pre class="line-numbers language-language-java"><code class="language-language-java">channel.basicNack(message.getMessageProperties().getDeliveryTag(), true, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li><p>消费者断开连接：</p><ul><li>如果消费者在处理消息时断开连接（比如由于崩溃或网络问题），没有发送 ack（消息确认），那么 RabbitMQ 会将消息重新入队。</li></ul></li></ol><p><strong>与死信队列和延迟队列的关系</strong></p><pre><code>**死信队列**：</code></pre><ul><li><p>如果消息被消费者多次拒绝且不重回队列（<code>requeue</code>标志设置为<code>false</code>），或者消息超过了最大重试次数（如果设置了），那么这条消息可以被发送到死信队列。</p></li><li><p>死信队列用来收集那些无法被成功消费的消息，以便后续可以对这些消息进行监控和处理。</p><p><strong>延迟队列</strong>：</p></li><li><p>与重回队列不同，延迟队列通常是用来延迟消息的投递。当你希望消息在一段时间后被处理，而不是立即被消费时，可以使用延迟队列。</p></li><li><p>如果一个消息在延迟队列中到期后不能被成功消费，并且不再被重回队列，它同样可以变成死信并被发送到死信队列。</p></li></ul><p><strong>注意事项</strong></p><ul><li>无限重试的风险：<ul><li>如果消息被不断地重回队列而不进行任何修改，这可能导致“死循环”，其中消费者不断尝试处理同一条消息。</li><li>为了避免这种情况，通常需要实现一些逻辑来跟踪消息的重试次数，并在超过某个阈值后将其发送到死信队列或其他的处理队列。</li></ul></li><li>消息顺序：<ul><li>如果队列中的消息顺序很重要，那么重回队列可能会打乱原有的顺序，因为重新排队的消息将会放置在队列的末尾。</li></ul></li></ul><h3 id="手动确认和自动确认"><a href="#手动确认和自动确认" class="headerlink" title="手动确认和自动确认"></a>手动确认和自动确认</h3><p>在 RabbitMQ 中，消息确认（ACK）是保证消息被正确处理的一种机制。生产者将消息发送到队列中，消费者从队列中获取消息并进行处理。一旦消息被成功处理，消费者应该向 RabbitMQ 发送一个确认信号，即 ACK。这个确认过程可以是自动的或手动的，具体取决于消费者的配置。</p><p><strong>手动确认（Manual-acknowledge）</strong></p><ul><li><p>在手动确认模式下，消费者必须明确地告诉 RabbitMQ 它已经完成了消息的处理，并且消息可以被安全地删除。</p></li><li><p>如果消费者在处理消息之前发生故障，消息不会丢失，因为它还没有被确认。</p></li><li><p>手动确认给了消费者更多的控制，但也增加了复杂性，因为需要在代码中明确地处理ACK。</p></li><li><p>使用手动确认可以避免消息的意外丢失，适用于需要保证<strong>消息处理可靠性</strong>的场景。</p><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">// 消费者接收消息的方法public void onMessage(Message message, Channel channel) throws IOException &#123;    try &#123;        // 处理消息的逻辑...        // 手动发送ACK        channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);    &#125; catch (Exception e) &#123;        // 如果处理消息时发生异常，拒绝消息，并根据情况决定是否重新入队        channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个示例中，如果消息成功处理，消费者会调用 <code>basicAck</code> 方法来确认消息。如果处理失败，它会调用 <code>basicNack</code> 方法来拒绝消息，并可能选择将其重新入队。</p></li></ul><p><strong>自动确认（Auto-acknowledge）</strong></p><ul><li>在自动确认模式下，一旦消息被投递给消费者，RabbitMQ 就会立即将其标记为已确认。</li><li>这种模式下简化了处理流程，因为消费者不需要显示地发送 ACK 信号。</li><li>但是，如果消费者在处理消息后发生故障（例如，应用崩溃或断电），那么消息可能会丢失，因为 RabbitMQ 认为它已经被处理了。</li><li>自动确认适用于那些对消息丢失不敏感或可以容忍丢失的场景。</li></ul><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="RabbitMQ-如何保证消息的可靠性"><a href="#RabbitMQ-如何保证消息的可靠性" class="headerlink" title="RabbitMQ 如何保证消息的可靠性"></a>RabbitMQ 如何保证消息的可靠性</h3><p><strong>可靠性</strong>主要关注消息在传输过程中不丢失，确保消息从生产者到MQ Server 再到消费者的过程中被正确处理。</p><p>RabbitMQ 提供了多种机制来确保消息的可靠性和安全性，以下是几个主要的方法：</p><ol><li>消息持久化<ul><li>将消息标记为持久化，并且确保队列也是持久的，这样即使在 RabbitMQ 重启后，消息也不会丢失。</li></ul></li><li>事务<ul><li>虽然事务会降低性能，但它们可以确保一系列操作（如消息发布）要么全部成功，要么全部不执行。</li></ul></li><li>confirm 确认模式和 return 回退模式<ul><li>前者是一个轻量级的、异步的机制，允许生产者知道其消息是否已成功到达交换器。</li><li>后者是当消息无法路由到任何一个队列时，它会调用生产者的回退回调函数。</li></ul></li><li>消费者确认（Consumer Acknowledgements）<ul><li>通过手动确认模式，消费者可以控制什么时候确认消息。这确保了只有在消息被正确处理后，它才会从队列中移除。</li></ul></li><li>死信队列<ul><li>设置死信队列可以处理无法正常消费的消息，例如因为消息被拒绝、过期或队列达到最大长度。</li></ul></li></ol><h3 id="RabbitMQ-如何做到削峰限流"><a href="#RabbitMQ-如何做到削峰限流" class="headerlink" title="RabbitMQ 如何做到削峰限流"></a>RabbitMQ 如何做到削峰限流</h3><ol><li><p><strong>预取计数（Prefetch Count）</strong></p><p>通过设置预取计数，可以限制 RabbitMQ 向每个消费者发送的未确认消息的数量。一旦达到这个数量，RabbitMQ 将停止向该消费者发送更多消息，直到它开始确认消息。</p><pre class="line-numbers language-language-java"><code class="language-language-java">channel.basicQos(prefetchCount);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在这里，<code>prefetchCount</code> 是你想要设置的预取数量。这个设置可以帮助避免给单个消费者分配过多消息，从而平衡负载并限制处理速率。</p></li><li><p><strong>消息延迟</strong></p><p>可以将消息发送到延迟队列&#x2F;死信队列，并设定消息的延迟时间。这样，消息会在指定的延迟之后才被投递到实际的工作队列。</p></li><li><p><strong>手动消息确认</strong></p><p>通过手动确认消息，可以更细粒度地控制消息的处理速度。消费者在完成消息处理后手动发送确认，这样可以根据处理能力来调整接收消息的速度。</p></li></ol><h3 id="RabbitMQ-如何保证最终一致性"><a href="#RabbitMQ-如何保证最终一致性" class="headerlink" title="RabbitMQ 如何保证最终一致性"></a>RabbitMQ 如何保证最终一致性</h3><p>使用数据库（DB）来辅助 RabbitMQ 实现消息的最终一致性是一个常见的模式，尤其是在分布式系统和微服务架构中。这种方法通常涉及到在消息处理和数据库操作之间保持一致性。以下是如何借助数据库实现 RabbitMQ 消息的最终一致性的步骤：</p><p>&#x2F;&#x2F;TODO 消息的延迟投递，做二次确认，回调检查</p><ol><li>事务日志记录<ul><li>在发送消息前，首先在数据库中记录事务日志。这个日志记录了即将发送的消息及其状态。这样做的目的是在发送消息之前确保有一个持久的记录存在。</li></ul></li><li>发送消息<ul><li>发送消息到 RabbitMQ，此时消息可能仅被标记为“待发送”或类似状态。</li></ul></li><li>发布者确认<ul><li>利用 RabbitMQ 的发布者确认机制来确保消息已被交换器接收。一旦收到确认，更新数据库中的事务日志，表明消息已成功发送。</li></ul></li><li>消费者处理<ul><li>消费者从 RabbitMQ 接收消息并进行处理。处理过程可能涉及到更改数据库的数据。</li></ul></li><li>处理确认<ul><li>处理完成后，消费者手动确认消息。这个确认告诉 RabbitMQ 消息已被成功处理，可以将数据的日志记录删除或者更改消息状态为完成。</li></ul></li><li>处理失败和重试<ul><li>如果消费者处理失败，可以利用 RabbitMQ 的重试机制或将消息放入死信队列。同时，可以在数据库中记录失败事件，以便进行后续的错误处理或人工干预。</li></ul></li><li>幂等性处理<ul><li>在消费者端实现幂等性，确保即使同一消息被重复消费，也不会对系统状态产生不良影响。</li></ul></li><li>同步或补偿机制<ul><li>在某些情况下，如果处理过程中发生了故障或不一致性，可能需要在数据库中执行补偿操作来恢复一致性。</li></ul></li><li>监控和告警<ul><li>通过对 RabbitMQ 队列和数据库的监控，可以及时发现问题并进行干预，比如处理积压的消息或解决数据不一致的问题。</li></ul></li></ol><h3 id="RabbitMQ-如何避免消息被重复消费"><a href="#RabbitMQ-如何避免消息被重复消费" class="headerlink" title="RabbitMQ 如何避免消息被重复消费"></a>RabbitMQ 如何避免消息被重复消费</h3><ol><li>使用唯一标识符<ul><li>为每条消息分配一个唯一标识符（如 UUID 或 消息 ID）。在消息者处理消息之前，先检查数据库或缓存中是否存在该标识符。如果存在，表明该消息已被处理，可以跳过；如果不存在，进行处理并记录该标识符。</li></ul></li><li>消息指纹<ul><li>为消息内容生成一个指纹（如哈希值），并检查这个指纹是否已被处理。这种方法适用于消息内容不变的情况。</li></ul></li><li>利用Redis的原子性<ul><li>需要考虑两个问题<ul><li>问题1：是否要进行数据落库，如果落库的话，关键解决的问题是数据库和缓存如何做到原子性？（假如redis保存数据成功，数据库保存失败的情况）</li><li>问题2：如果不进行落库，那么都存储到缓存中，如何设置定时同步的策略？（并且有可能会出现redis保存数据失败的情况）</li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RabbitMQ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型 RAG 应用开发基础及入门</title>
      <link href="/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/"/>
      <url>/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是大语言模型的幻觉？"><a href="#什么是大语言模型的幻觉？" class="headerlink" title="什么是大语言模型的幻觉？"></a>什么是大语言模型的幻觉？</h2><p>大语言模型在处理自然语言时，有时候会出现”幻觉“现象。所谓幻觉，就是模型生成的内容与事实或上下文不一致的问题。这些问题会严重影响AI应用的可靠性和实用性。</p><h2 id="幻觉的两大类型"><a href="#幻觉的两大类型" class="headerlink" title="幻觉的两大类型"></a>幻觉的两大类型</h2><h3 id="事实性幻觉"><a href="#事实性幻觉" class="headerlink" title="事实性幻觉"></a>事实性幻觉</h3><p>指模型生成的内容与实际事实不匹配。比如在回答”第一个登上月球的人是谁?”这个问题时:</p><ul><li>错误回答: “Charles Lindbergh在1951年月球任务中第一个登上月球”</li><li>正确事实: Neil Armstrong才是第一个登上月球的人(1969年阿波罗11号任务)</li></ul><p>这种幻觉之所以危险，是因为模型生成的内容看起来很可信，但实际上完全错误。</p><h3 id="忠实性幻觉"><a href="#忠实性幻觉" class="headerlink" title="忠实性幻觉"></a>忠实性幻觉</h3><p>指模型生成的内容与提供的上下文不一致。这种幻觉可以分为三类：</p><ul><li>输出与原文不一致（编出原文中没有的信息）</li><li>上下文之间不一致（前后矛盾）</li><li>逻辑链不一致（推理过程存在漏洞）</li></ul><p>比如在总结新闻时，模型可能会添加原文中不存在的细节，或者前后描述矛盾。</p><h2 id="为什么会产生幻觉？"><a href="#为什么会产生幻觉？" class="headerlink" title="为什么会产生幻觉？"></a>为什么会产生幻觉？</h2><p>大语言模型产生幻觉的原因主要来自三个方面：</p><ol><li>数据源导致的幻觉<ul><li>训练数据中的质量问题</li><li>数据中存在的错误信息</li><li>数据覆盖范围有限</li></ul></li><li>训练过程导致的幻觉<ul><li>架构限制：无法准确理解长文本的上下文关联</li><li>累积错误：生成过程中的错误会逐步传递和放大</li></ul></li><li>推理相关的幻觉<ul><li>回答过于简略</li><li>生成过程中的不完整推理</li></ul></li></ol><h2 id="如何评估幻觉问题"><a href="#如何评估幻觉问题" class="headerlink" title="如何评估幻觉问题"></a>如何评估幻觉问题</h2><p>为了客观评估模型的幻觉问题，我们可以使用多种方法：</p><ol><li>事实一致性评估：将生成内容与权威来源进行比对</li><li>分类器评估：使用专门训练的模型来检测是否存在幻觉</li><li>问答测量：通过问答来验证生成内容的一致性</li><li>不确定度分析：评估模型对自身输出的确信程度</li><li>提示测量：让模型自我评估，通过特定提示策略来评估生成内容</li></ol><h2 id="RAG解决方案"><a href="#RAG解决方案" class="headerlink" title="RAG解决方案"></a>RAG解决方案</h2><h3 id="RAG是什么？"><a href="#RAG是什么？" class="headerlink" title="RAG是什么？"></a>RAG是什么？</h3><p><strong>RAG</strong>（Retrieval-Augmented Generation）也叫<strong>检索增强生成</strong>，是指对大语言模型输出进行优化，使其能够参考并利用数据源之外的权威知识。简单来说，RAG就是从外部检索对应的知识内容，和用户的提问一起构成Prompt发给大模型，再让大模型生成内容。</p><p>它的核心思想是：</p><ol><li><strong>从外部知识库检索相关信息</strong></li><li><strong>将检索到的信息作为上下文提供给模型</strong></li><li><strong>让模型基于这些上下文生成回答</strong></li></ol><p>简单来说：RAG &#x3D; 外部知识检索 + Prompt构建 + LLM 生成</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image1.png" alt="image"></p><h3 id="为什么需要RAG？"><a href="#为什么需要RAG？" class="headerlink" title="为什么需要RAG？"></a>为什么需要RAG？</h3><p>LLM虽然是一个强大的工具，但它本身拒绝了解任何时事，且它给出的答案总是非常流畅，内容却不一定靠谱。这存在几个主要的问题:</p><ol><li>LLM的训练数据量有限且无法更新到最新知识。</li><li>当用户需要专业或领域特定的数据时，LLM往往缺乏相应的知识</li><li>对于答案的问答内容很难从源创进行溯源</li><li>由于技术限制，不同的训练源使用相同的大语言技术，可能会产生不确信的响应</li></ol><p>而RAG为解决这些问题带来了以下优势：</p><ul><li><strong>经济高效</strong>：预训练和微调模型的成本很高，而RAG是一种经济高效的新方法</li><li><strong>信息时效</strong>：使用RAG可以为LLM提供最新的研究、统计数据或新闻</li><li><strong>增强用户信任度</strong>：RAG允许LLM通过来源归属来呈现具体的信息，输出可以包括对来源的引文或参考，这可以增加对对话的生成式人工智能解决方案的任何信心</li></ul><h3 id="RAG是如何工作的？"><a href="#RAG是如何工作的？" class="headerlink" title="RAG是如何工作的？"></a>RAG是如何工作的？</h3><p>RAG采用三种主要的检索方式：</p><ol><li><strong>一次性检索</strong>：<ul><li>从单次检索中获取相关知识</li><li>直接预置到大模型的提示词中</li><li>不会收集反馈信息</li></ul></li><li><strong>迭代检索</strong>：<ul><li>允许在对话过程中多次检索</li><li>每一轮都可能有新的检索</li><li>支持多轮对话优化</li></ul></li><li><strong>事后检索</strong>：<ul><li>先生成答案</li><li>然后检索验证</li><li>对答案进行修正</li></ul></li></ol><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image2.png" alt="image"></p><h3 id="RAG实战示例"><a href="#RAG实战示例" class="headerlink" title="RAG实战示例"></a>RAG实战示例</h3><p>以一个简单的问答场景为例，展示RAG的实际应用流程:</p><ol><li>用户提问:”公司有销售什么产品？”</li><li>系统处理流程:<ul><li>使用检索器获取产品相关文档</li><li>将文档内容与问题组合成提示词</li><li>通过LLM生成回答</li><li>确保回答基于检索到的事实信息</li></ul></li><li>最终输出:包含准确的产品信息，并且所有信息都可以溯源。</li></ol><h2 id="AI应用开发利器：向量数据库详解"><a href="#AI应用开发利器：向量数据库详解" class="headerlink" title="AI应用开发利器：向量数据库详解"></a>AI应用开发利器：向量数据库详解</h2><h3 id="什么是向量数据库？"><a href="#什么是向量数据库？" class="headerlink" title="什么是向量数据库？"></a>什么是向量数据库？</h3><p>向量数据库（Vector Database）是一种专门用于存储和处理向量数据的数据库系统。它不同于传统的关系型数据库，因为它需要将所有数据映射为特定的向量格式，并采用相似性搜索作为主要的检索方式。</p><h3 id="一个生动的例子：识别猫咪"><a href="#一个生动的例子：识别猫咪" class="headerlink" title="一个生动的例子：识别猫咪"></a>一个生动的例子：识别猫咪</h3><p>让我们通过一个识别猫咪的例子来理解向量数据库。假设我们有一组不同品种的猫咪图片：</p><ul><li>波斯猫</li><li>英国短毛猫</li><li>暹罗猫</li><li>布偶猫</li><li>无毛猫</li></ul><p>每张猫咪图片都可以用一组数字向量来表示其特征，如:</p><pre class="line-numbers language-language-json"><code class="language-language-json">波斯猫: [0.4, 0.3, 0.4, 0.5, 0.3, 0.4, 0.5, ...]英国短毛猫: [0.7, 0.2, 0.5, 0.5, 0.5, 0.5, 0.5, ...]暹罗猫: [0.5, 0.3, 0.4, 0.5, 0.3, 0.4, 0.5, ...]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这些数字代表了猫咪的各种特征，比如:</p><ul><li>毛发长度</li><li>体型大小</li><li>面部特征</li><li>耳朵形状等等</li></ul><h3 id="向量数据库的优势"><a href="#向量数据库的优势" class="headerlink" title="向量数据库的优势"></a>向量数据库的优势</h3><p>与传统的数据库相比，向量数据库有以下特点：</p><ol><li><strong>数据类型</strong>：<ul><li>传统数据库：数值、字符串、时间等结构化数据</li><li>向量数据库：向量数据(不存储原始数据，有的也支持)</li></ul></li><li><strong>数据规模</strong>：<ul><li>传统数据库：小，1亿条数据对关系型数据库来说规模很大</li><li>向量数据库：大，最少千亿数据是基线</li></ul></li><li><strong>数据组织方式</strong>：<ul><li>传统数据库：基于表格、按照行和列组织</li><li>向量数据库：基于向量、按向量维度组织</li></ul></li><li><strong>查找方式</strong>：<ul><li>传统数据库：精确查找&#x2F;范围查找</li><li>向量数据库：近似查找，查询结果是与输入向量最相似的向量</li></ul></li></ol><h3 id="相似性搜索算法"><a href="#相似性搜索算法" class="headerlink" title="相似性搜索算法"></a>相似性搜索算法</h3><p>在向量数据库中，支持通过多种方式来计算两个向量的相似度：</p><p><strong>余弦相似度</strong>：主要是用于衡量向量在方向上的相似性，特别适用于文本、图像和高维空间中的向量。它不受向量长度的影响，只考虑方向的相似程度，计算公式如下（计算两个向量间的夹角的余弦值，取值范围为[-1, 1]）：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">similarity(A,B) = (A·B)/(||A||·||B||)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>欧式距离</strong>：主要是用于衡量向量之间的直线距离，得到的值可能很大，最小为0，通常用于低维空间或需要考虑向量各个维度之间差异的情况。欧式距离较小的向量被认为更相似，计算公式如下：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">distance(A,B) = √∑(Ai-Bi)²<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>例如下图：左侧就是<code>欧式距离</code>，右侧就是<code>余弦相似度</code>。</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image3.png" alt="image"></p><h3 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h3><p>向量数据库的主要应用场景包括：</p><ol><li>人脸识别</li><li>图像搜索</li><li>音频识别</li><li>智能推荐系统</li></ol><p>这些场景的共同特点是：需要对非结构化数据（如图片、文本、音频）进行相似度搜索。</p><p>在RAG中，我们会将文档的知识按特定规则分成小块，转换成向量存储到向量数据库中。当人类提问时，我们将问题转换为向量，在数据库中找到最相似的文本块，这些文本块可以成为Prompt的补充内容。</p><h2 id="深入理解Embedding嵌入技术"><a href="#深入理解Embedding嵌入技术" class="headerlink" title="深入理解Embedding嵌入技术"></a>深入理解Embedding嵌入技术</h2><h3 id="Embedding-是什么？"><a href="#Embedding-是什么？" class="headerlink" title="Embedding 是什么？"></a>Embedding 是什么？</h3><p>Embedding(嵌入)是一种在机器学习中广泛使用的技术，它能将文本、图片、视频等非结构化数据映射到向量空间中。一个Embedding向量通常是一个包含N个浮点数的数组，这个向量不仅表示了数据的特征，更重要的是通过学习可以表达它们的内在语义。简而言之，Embedding就是一个模型生成方法，可以将非结构化的数据，例如文本&#x2F;图片&#x2F;视频等数据映射成有意义的向量数据。比如一段文本、一张图片、一段视频，警告Embedding模型处理后都会变成类似这样的向量：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">[0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image4.png" alt="image"></p><h3 id="主流的Embedding模型"><a href="#主流的Embedding模型" class="headerlink" title="主流的Embedding模型"></a>主流的Embedding模型</h3><p>目前主要有这几类Embedding模型：</p><ol><li><strong>Word2Vec（词嵌入模型）</strong><ul><li>通过学习词语转化为连续的向量表示</li><li>基于两种主要算法：<code>CBOW</code> 和 <code>Skip-gram</code></li><li>能够捕捉词语之间的语义关系</li></ul></li><li><strong>1GloVe</strong><ul><li>类似Word2Vec但采用不同的训练方式</li><li>同时考虑全局共现信息</li><li>能较好地保存词语间的语义关系</li><li>适用于多种自然语言处理任务</li></ul></li><li><strong>FastText</strong><ul><li>考虑了单词的子词信息</li><li>能处理训练集中未出现的生词</li><li>支持多语言处理</li></ul></li><li><strong>大模型Embeddings</strong><ul><li>如OpenAI的text-embedding-ada-002</li><li>输入维度8191个tokens</li><li>输出维度1536维向量</li></ul></li></ol><h3 id="Embedding的神奇之处"><a href="#Embedding的神奇之处" class="headerlink" title="Embedding的神奇之处"></a>Embedding的神奇之处</h3><p>Embedding最有趣的特性是它能够捕捉语义关系。让我们看一个著名的例子</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">King - Man + Woman ≈ Queen(国王 - 男人 + 女人 ≈ 女王)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这个公式展示了Embedding不仅仅是把词语转换成数字，它还能：</p><ol><li>保留词语之间的关系</li><li>支持向量运算</li><li>产生有意义的结果</li></ol><p>我们可以通过可视化的方式看到这些词语在向量空间中的分布：</p><ul><li>woman和girl的向量位置接近</li><li>man和boy的向量位置接近</li><li>king和queen虽然性别不同，但都位于表示”统治者”的维度上</li></ul><h3 id="Embedding的重要价值"><a href="#Embedding的重要价值" class="headerlink" title="Embedding的重要价值"></a>Embedding的重要价值</h3><ol><li><strong>降维</strong>：将高维数据映射到低维空间，大大降低了计算复杂度</li><li><strong>捕捉语义信息</strong>：不仅能记录表面的词频信息，还能捕捉深层的语义关联</li><li><strong>泛化性</strong>：Embedding学习到的是通用的语言表达方式，可以应用到新的场景</li><li><strong>泛化能力</strong>：对于未见过的数据，也能基于已学习的语义特征给出合理的向量表示</li><li><strong>可视化支持</strong>：虽然Embedding本身很复杂，但我们可以使用t-SNE等工具将其可视化，帮助理解数据的内在结构。</li></ol><h3 id="在RAG中的应用"><a href="#在RAG中的应用" class="headerlink" title="在RAG中的应用"></a>在RAG中的应用</h3><p>在RAG系统中，Embedding主要用于两个场景：</p><ol><li><strong>文档向量化</strong>：将知识库中的文档转换为向量</li><li><strong>查询向量化</strong>：将用户的问题转换为向量</li></ol><p>通过比较这些向量的相似度，我们可以找到与用户问题最相关的文档片段，从而提供更准确的答案。</p><h2 id="RAG应用实战-OpenAI-Embedding与LangChain的结合"><a href="#RAG应用实战-OpenAI-Embedding与LangChain的结合" class="headerlink" title="RAG应用实战:OpenAI Embedding与LangChain的结合"></a>RAG应用实战:OpenAI Embedding与LangChain的结合</h2><h3 id="OpenAI-Embedding接口简介"><a href="#OpenAI-Embedding接口简介" class="headerlink" title="OpenAI Embedding接口简介"></a>OpenAI Embedding接口简介</h3><p>OpenAI提供了多个Embedding模型选择，以下是几个主要模型的对比:</p><table><thead><tr><th>模型</th><th>Token数(每个文档800个)</th><th>性能评估</th><th>最大输入</th><th>向量维度</th></tr></thead><tbody><tr><td>text-embedding-3-small</td><td>62,500</td><td>62.3%</td><td>8191</td><td>1536</td></tr><tr><td>text-embedding-3-large</td><td>9,615</td><td>64.6%</td><td>8191</td><td>3072</td></tr><tr><td>text-embedding-ada-002</td><td>12,500</td><td>61.0%</td><td>8191</td><td>1536</td></tr></tbody></table><h3 id="LangChain中的Embedding组件使用"><a href="#LangChain中的Embedding组件使用" class="headerlink" title="LangChain中的Embedding组件使用"></a>LangChain中的Embedding组件使用</h3><p>在LangChain中，Embedding类提供了统一的接口来使用各种嵌入模型:</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">class Embeddings(ABC):    """Interface for embedding models."""        @abstractmethod    def embed_documents(self, texts: List[str]) -> List[List[float]]:        """Embed search docs."""            @abstractmethod    def embed_query(self, text: str) -> List[float]:        """Embed query text."""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用示例:</p><pre class="line-numbers language-language-python"><code class="language-language-python">import dotenvimport numpy as npfrom langchain_openai import OpenAIEmbeddingsfrom numpy.linalg import norm# 初始化Embedding模型embeddings = OpenAIEmbeddings()# 进行文本嵌入query_vector = embeddings.embed_query("你好, 我是小潘")documents_vector = embeddings.embed_documents([    "你好, 我是小潘",    "这个自然语言处理的人叫小潘",    "来知若惘, 既心若旷"])# 计算相似度def cosine_similarity(vector1, vector2):    dot_product = np.dot(vector1, vector2)    norm1 = norm(vector1)    norm2 = norm(vector2)    return dot_product / (norm1 * norm2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="CacheBackEmbedding的使用"><a href="#CacheBackEmbedding的使用" class="headerlink" title="CacheBackEmbedding的使用"></a>CacheBackEmbedding的使用</h3><p>为了提高性能，LangChain提供了缓存功能：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.embeddings import CacheBackedEmbeddingsfrom langchain.storage import LocalFileStoreembeddings = OpenAIEmbeddings(model="text-embedding-3-small")embeddings_with_cache = CacheBackedEmbeddings.from_bytes_store(    embeddings,    LocalFileStore("./cache/"),    namespace=embeddings.model,    query_embedding_cache=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用缓存时需要注意：</p><ol><li>underlying_embedder: 使用的基础嵌入模型</li><li>document_embedding_cache: 用于缓存文档的存储结构</li><li>batch_size: 可选参数，默认None</li><li>namespace: 用于文档缓存的命名空间</li><li>query_embedding_cache: 是否缓存查询向量</li></ol><h3 id="运行流程分析"><a href="#运行流程分析" class="headerlink" title="运行流程分析"></a>运行流程分析</h3><p>一个完整的RAG应用运行流程如下：</p><ol><li><strong>文档预处理</strong><ul><li>分割文档</li><li>生成向量</li><li>存入向量数据库</li></ul></li><li><strong>查询处理</strong><ul><li>将用户问题转为向量</li><li>在向量数据库中检索</li><li>组合上下文生成回答</li></ul></li><li><strong>缓存优化</strong><ul><li>缓存常见文档的向量</li><li>缓存常见查询的向量</li><li>提供响应速度</li></ul></li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>向量维度的选择<ul><li>需要平衡精度和效率</li><li>维度越高，表达能力越强，但计算成本也越高</li></ul></li><li>缓存策略<ul><li>合理设置缓存大小</li><li>选择适当的缓存淘汰策略</li><li>定期更新缓存</li></ul></li><li>性能优化<ul><li>使用批处理提高效率</li><li>合理使用多线程</li><li>监控资源使用情况</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发优化策略详解</title>
      <link href="/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/"/>
      <url>/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="引言：理解RAG及其重要性"><a href="#引言：理解RAG及其重要性" class="headerlink" title="引言：理解RAG及其重要性"></a>引言：理解RAG及其重要性</h2><p>在大语言模型（LLM）应用开发中，检索增强生成（Retrival-Augmented Generation, RAG）已经成为提升模型输出质量的关键技术。本文将深入探讨在LangChain框架中如何优化RAG应用，帮助开发者构建更智能、更准确的AI应用。</p><h2 id="RAG的基本概念"><a href="#RAG的基本概念" class="headerlink" title="RAG的基本概念"></a>RAG的基本概念</h2><blockquote><p>📌 什么是RAG?<br>RAG是一种将外部知识检索与语言模型生成相结合的技术架构。它通过检索相关信息来增强LLM的知识储备，从而产生更准确、更可靠的输出。</p></blockquote><h3 id="为什么需要优化RAG？"><a href="#为什么需要优化RAG？" class="headerlink" title="为什么需要优化RAG？"></a>为什么需要优化RAG？</h3><p>在实际应用中，基础的RAG实现往往会遇到以下挑战：</p><ol><li>检索准确性不足</li><li>复杂问题处理能力有限</li><li>知识关联不够紧密</li><li>响应质量不够稳定</li></ol><p>这些问题促使我们需要采用多种优化策略来提升RAG的性能。</p><h3 id="第一部分：多查询检索优化策略"><a href="#第一部分：多查询检索优化策略" class="headerlink" title="第一部分：多查询检索优化策略"></a>第一部分：多查询检索优化策略</h3><h4 id="理解多查询检索的必要性"><a href="#理解多查询检索的必要性" class="headerlink" title="理解多查询检索的必要性"></a>理解多查询检索的必要性</h4><p>在RAG应用中，单一查询往往无法完整捕捉用户问题的所有方面。例如，当用户问”Python如何实现多线程并发控制？“时，我们可能需要同时检索：</p><ul><li>Python线程基础知识</li><li>并发控制机制</li><li>线程安全实现方法</li></ul><h4 id="多查询检索的工作原理"><a href="#多查询检索的工作原理" class="headerlink" title="多查询检索的工作原理"></a>多查询检索的工作原理</h4><blockquote><p>🔍 核心思路：利用LLM的理解能力，将一个复杂查询拆分或重写为多个相关查询，然后通过融合算法整合检索结果。</p></blockquote><p><strong>工作流程</strong>：</p><ol><li><strong>查询重写</strong>：LLM将原始查询转换为多个相关查询</li><li><strong>并行检索</strong>：对每个查询进行独立检索</li><li><strong>结果融合</strong>：使用RRF（Reciprocal Rank Fusion）算法融合检索结果</li><li><strong>内容生成</strong>：将融合后的结果输入LLM生成最终答案</li></ol><h3 id="代码实现示例"><a href="#代码实现示例" class="headerlink" title="代码实现示例"></a>代码实现示例</h3><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.retrievers import MultiQueryRetrieverfrom langchain.chains import LLMChainfrom langchain.prompts import PromptTemplate# 1. 创建多查询检索器retriever = MultiQueryRetriever(    retriever=base_retriever,    llm=ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0),    prompt_template="""基于用户的问题，生成3个不同的相关查询：    原始问题: &#123;question&#125;    生成的查询应该探索问题的不同方面。    """)# 2. 使用RRF算法融合结果def rrf_fusion(results, k=60):    fused_scores = &#123;&#125;    for rank, doc in enumerate(results):        doc_str = doc.page_content        if doc_str not in fused_scores:            fused_scores[doc_str] = 1.0 / (k + rank + 1)        else:            fused_scores[doc_str] += 1.0 / (k + rank + 1)        # 排序并返回结果    sorted_results = sorted(fused_scores.items(),                           key=lambda x: x[1],                           reverse=True)    return sorted_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RRF 算法原理如下</p><pre class="line-numbers language-language-python"><code class="language-language-python">"""RRF (Reciprocal Rank Fusion) 算法的核心公式：RRFscore(d ∈ D) = ∑ 1/(k + r(d))其中：- d 是文档- D 是所有文档集合- k 是一个常数(通常取60)- r(d)是文档d在排序中的位置这个公式的特点：1. 对排名靠前的文档给予更高的权重2. k参数可以调节排名的影响程度3. 适合融合不同来源的排序结果"""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="优化效果分析"><a href="#优化效果分析" class="headerlink" title="优化效果分析"></a>优化效果分析</h4><p>多查询检索策略带来的主要优势：</p><ol><li><strong>提升召回率</strong><ul><li>通过多角度查询提高相关文档的覆盖率</li><li>减少因单一查询表达不当导致的漏检</li></ul></li><li><strong>提高准确性</strong><ul><li>RRF融合算法可以突出高质量的共同结果</li><li>降低单个查询的噪声影响</li></ul></li><li><strong>增强鲁棒性</strong><ul><li>对查询表达的变化更不敏感</li><li>能更好地处理复杂或模糊的问题</li></ul></li></ol><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，需要注意以下几点：</p><ul><li><strong>查询数量选择</strong>：通常生成3-5个查询即可，过多查询可能引入噪声</li><li><strong>相似度阈值设置</strong>：建议在RRF融合时设置合适的相似度阈值，过滤低相关性结果</li><li><strong>资源消耗考虑</strong>：多查询会增加API调用和计算资源，需要在效果和成本间权衡</li></ul><blockquote><p>💡 实践小贴士：可以通过监控检索结果的diversity和relevance指标，来调整多查询策略的参数。</p></blockquote><h3 id="第二部分：问题分解策略优化"><a href="#第二部分：问题分解策略优化" class="headerlink" title="第二部分：问题分解策略优化"></a>第二部分：问题分解策略优化</h3><h4 id="复杂问题的分解处理"><a href="#复杂问题的分解处理" class="headerlink" title="复杂问题的分解处理"></a>复杂问题的分解处理</h4><p>在实际应用中，我们经常遇到复杂的多层次问题。例如：”请分析特斯拉近五年的财务状况，并评估其在电动汽车市场的竞争优势。”这类问题需要：</p><ul><li>处理大量相关信息</li><li>分析多个维度</li><li>综合多方面结论</li></ul><p><strong>并行分解模式</strong>:</p><blockquote><p>🔄 并行模式：将问题同时分解为多个独立子问题，分别获取答案后合并。</p></blockquote><pre class="line-numbers language-language-python"><code class="language-language-python"># 并行分解示例decomposition_chain = &#123;    "question": RunnablePassthrough(),    | decomposition_prompt    # 分解问题    | ChatOpenAI(temperature=0)    | StrOutputParser()&#125;# 并行处理子问题sub_questions = decomposition_chain.invoke(question)answers = await asyncio.gather(*[    process_subquestion(q) for q in sub_questions])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>串行分解模式</strong>:</p><blockquote><p>⛓️ 串行模式：按照逻辑顺序依次处理子问题，后面的问题依赖前面的答案。</p></blockquote><pre class="line-numbers language-language-python"><code class="language-language-python"># 串行分解示例class StepBackRetriever(BaseRetriever):    def _get_relevant_documents(        self, query: str, *, run_manager: CallbackManagerForRetrieverRun    ) -> List[Document]:        # 1. 生成中间查询        intermediate_query = self.llm.predict(            f"为了回答'&#123;query&#125;'，我们需要先了解什么？"        )                # 2. 检索中间知识        intermediate_docs = self.retriever.get_relevant_documents(            intermediate_query        )                # 3. 基于中间知识检索最终答案        final_docs = self.retriever.get_relevant_documents(query)                return intermediate_docs + final_docs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Step-Back-策略实现"><a href="#Step-Back-策略实现" class="headerlink" title="Step-Back 策略实现"></a>Step-Back 策略实现</h4><p>Step-Back策略是一种特殊的串行分解方法，它通过“后退一步”来获取更基础的知识背景。</p><pre class="line-numbers language-language-python"><code class="language-language-python">"""示例：用户问题"量子计算机如何影响现代密码学？"Step-Back分解：1. 基础知识查询：   - 什么是量子计算机的基本原理？   - 现代密码学的核心技术有哪些？2. 关联分析：   - 量子计算对RSA等算法的影响   - 后量子密码学的发展3. 最终综合：   基于以上知识形成完整答案"""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>工作流程</strong>：</p><ol><li>分析原始问题</li><li>生成更基础的前置问题</li><li>获取基础知识</li><li>结合基础知识回答原问题</li></ol><h4 id="Step-Back-代码实现"><a href="#Step-Back-代码实现" class="headerlink" title="Step-Back 代码实现"></a>Step-Back 代码实现</h4><pre class="line-numbers language-language-python"><code class="language-language-python">system_prompt = """你是一位专业的助手，需要：1. 理解用户的具体问题2. 思考需要哪些基础知识3. 生成相关的基础问题4. 基于基础知识回答原问题"""few_shot_prompt = FewShotChatMessagePromptTemplate(    example_prompt=example_prompt,    examples=examples,    suffix="现在，请帮我回答：&#123;question&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="优化效果对比"><a href="#优化效果对比" class="headerlink" title="优化效果对比"></a>优化效果对比</h4><table><thead><tr><th>分解策略</th><th>适用场景</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>并行分解</td><td>独立子问题</td><td>处理速度快，资源利用高</td><td>结果整合可能不够连贯</td></tr><tr><td>串行分解</td><td>逻辑依赖性强</td><td>答案更连贯，逻辑性强</td><td>处理时间较长</td></tr><tr><td>Step-Back</td><td>需要深入理解</td><td>回答更全面，准确度高</td><td>资源消耗较大</td></tr></tbody></table><h4 id="优化建议"><a href="#优化建议" class="headerlink" title="优化建议"></a>优化建议</h4><ol><li>选择策略时考虑因素:<ul><li>问题的复杂度</li><li>子问题间的依赖关系</li><li>响应时间要求</li><li>资源限制</li></ul></li><li>优化建议：<ul><li>对于并行模式，注意结果融合的质量</li><li>串行模式要控制分解的层级深度</li><li>Step-Back策略要平衡基础知识的范围</li></ul></li></ol><blockquote><p>🌟 最佳实践：可以根据问题类型动态选择分解策略，甚至组合使用多种策略。</p></blockquote><h3 id="第三部分：混合检索策略实现"><a href="#第三部分：混合检索策略实现" class="headerlink" title="第三部分：混合检索策略实现"></a>第三部分：混合检索策略实现</h3><h4 id="理解混合检索的价值"><a href="#理解混合检索的价值" class="headerlink" title="理解混合检索的价值"></a>理解混合检索的价值</h4><p>在实际应用中，单一的检索方法往往难以应对所有场景。例如：</p><ul><li>语义检索擅长理解上下文，但可能错过关键词</li><li>关键词检索准确度高，但缺乏语义理解</li><li>密集检索和稀疏检索各有优势</li></ul><p>因此，将多种检索方法结合起来，可以取长补短，提升整体检索效果。</p><h4 id="混合检索器的架构设计"><a href="#混合检索器的架构设计" class="headerlink" title="混合检索器的架构设计"></a>混合检索器的架构设计</h4><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.retrievers import EnsembleRetrieverfrom langchain_community.retrievers import BM25Retrieverfrom langchain_community.vectorstores import FAISS# 1. 创建不同类型的检索器# BM25检索器（基于关键词）bm25_retriever = BM25Retriever.from_documents(    documents, k=4)# FAISS检索器（基于向量）faiss_retriever = FAISS.from_documents(    documents,    embedding=OpenAIEmbeddings(model="text-embedding-3-small")).as_retriever(search_kwargs=&#123;"k": 4&#125;)# 2. 创建集成检索器ensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, faiss_retriever],    weights=[0.5, 0.5])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="主要检索方法的特点"><a href="#主要检索方法的特点" class="headerlink" title="主要检索方法的特点"></a>主要检索方法的特点</h4><p>下面是几种常用检索方法的对比：</p><table><thead><tr><th>检索方法</th><th>优势</th><th>适用场景</th><th>注意事项</th></tr></thead><tbody><tr><td>BM25</td><td>精确匹配，速度快</td><td>关键词搜索</td><td>不理解语义变化</td></tr><tr><td>向量检索</td><td>理解语义相似</td><td>概念搜索</td><td>计算资源消耗大</td></tr><tr><td>混合检索</td><td>综合优势</td><td>复杂查询</td><td>需要调整权重</td></tr></tbody></table><h4 id="实现细节和优化"><a href="#实现细节和优化" class="headerlink" title="实现细节和优化"></a>实现细节和优化</h4><p><strong>检索器配置</strong>:</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 配置检索参数faiss_retriever = faiss_db.as_retriever(    search_kwargs=&#123;"k": 4&#125;).configurable_fields(    search_kwargs=ConfigurableField(        id="search_kwargs_faiss",        name="检索参数",        description="设置检索的参数"    ))# 设置运行时配置config = &#123;"configurable": &#123;"search_kwargs_faiss": &#123;"k": 4&#125;&#125;&#125;docs = ensemble_retriever.invoke("查询", config=config)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>权重调整策略</strong>:</p><ol><li><strong>初始设置</strong>：开始时可以给各检索器相同权重</li><li><strong>动态调整</strong>：根据查询类型动态调整权重</li><li><strong>性能监控</strong>：跟踪各检索器的表现，定期优化权重</li><li><strong>场景适配</strong>：针对不同领域调整最优权重组合</li></ol><h4 id="应用效果优化"><a href="#应用效果优化" class="headerlink" title="应用效果优化"></a>应用效果优化</h4><p>为了获得最佳检索效果，建议：</p><ol><li>检索器选择<ul><li>根据数据特点选择合适的检索器组合</li><li>考虑计算资源和响应时间的平衡</li><li>评估检索器的互补性</li></ul></li><li>参数优化<ul><li>使用验证集调整检索参数</li><li>监控检索质量指标</li><li>定期更新检索模型</li></ul></li><li>结果融合<ul><li>采用多样化的融合策略</li><li>考虑结果的去重和排序</li><li>平衡相关性和多样性</li></ul></li></ol><h4 id="性能监控与改进"><a href="#性能监控与改进" class="headerlink" title="性能监控与改进"></a>性能监控与改进</h4><pre class="line-numbers language-language-python"><code class="language-language-python"># 性能监控示例def evaluate_retrieval(retriever, test_queries, ground_truth):    metrics = &#123;        'precision': [],        'recall': [],        'latency': []    &#125;        for query, truth in zip(test_queries, ground_truth):        start_time = time.time()        results = retriever.get_relevant_documents(query)        latency = time.time() - start_time                # 计算评估指标        metrics['latency'].append(latency)        # ... 计算precision和recall            return metrics    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结：RAG优化策略的实践指南"><a href="#总结：RAG优化策略的实践指南" class="headerlink" title="总结：RAG优化策略的实践指南"></a>总结：RAG优化策略的实践指南</h3><h4 id="优化策略的综合比较"><a href="#优化策略的综合比较" class="headerlink" title="优化策略的综合比较"></a>优化策略的综合比较</h4><p>以下是我们讨论过的主要优化策略的特点对比：</p><table><thead><tr><th>优化策略</th><th>主要优势</th><th>实现复杂度</th><th>资源消耗</th><th>适用场景</th></tr></thead><tbody><tr><td>多查询检索</td><td>提高召回率</td><td>中等</td><td>中等</td><td>复杂查询、模糊问题</td></tr><tr><td>问题分解</td><td>提升理解深度</td><td>较高</td><td>较高</td><td>多维度分析问题</td></tr><tr><td>Step-Back</td><td>增强理解准确性</td><td>高</td><td>高</td><td>需要深入理解的问题</td></tr><tr><td>混合检索</td><td>综合性能提升</td><td>中等</td><td>较高</td><td>通用场景</td></tr></tbody></table><h4 id="优化路径建议"><a href="#优化路径建议" class="headerlink" title="优化路径建议"></a>优化路径建议</h4><ol><li><strong>基础阶段</strong><ul><li>实现基本的RAG流程</li><li>优化向量检索参数</li><li>改进提示词设计</li></ul></li><li><strong>进阶阶段</strong><ul><li>引入多查询策略</li><li>实现基本的问题分解</li><li>尝试混合检索方法</li></ul></li><li><strong>高级阶段</strong><ul><li>实现完整的Step-Back策略</li><li>优化多检索器集成</li><li>构建自适应检索系统</li></ul></li></ol><p><strong>场景选择指南</strong>:</p><p>根据不同的应用场景，推荐以下优化组合：</p><ol><li><strong>知识问答系统</strong><ul><li>多查询检索 + 混合检索</li><li>重点优化检索准确性</li></ul></li><li><strong>文档分析系统</strong><ul><li>问题分解 + Step-Back</li><li>强化深度理解能力</li></ul></li><li><strong>通用对话系统</strong><ul><li>混合检索 + 多查询</li><li>平衡效率和准确性</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain初入门</title>
      <link href="/2025/02/09/langchain-chu-ru-men/"/>
      <url>/2025/02/09/langchain-chu-ru-men/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么选择LangChain"><a href="#为什么选择LangChain" class="headerlink" title="为什么选择LangChain"></a>为什么选择LangChain</h2><p>LangChain作为一个强大的框架，具有以下优势：</p><ul><li><p><strong>组件化和标准化</strong>：提供了标准化的接口来处理各种LLM，使开发更加灵活和可维护。</p></li><li><p><strong>丰富的工具集成</strong>：内置了大量工具和集成，可以轻松连接数据库、搜索引擎等外部服务。</p></li><li><p><strong>链式处理能力</strong>：可以将多个组件组合成链，实现复杂的处理流程。</p></li><li><p><strong>内存管理</strong>：提供了多种记忆组件，使应用能够保持上下文连贯性。</p></li></ul><h2 id="LangChain简介"><a href="#LangChain简介" class="headerlink" title="LangChain简介"></a>LangChain简介</h2><p>LangChain是一个用于开发由语言模型驱动的应用程序的框架。</p><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><ul><li><p><strong>Models (模型)</strong>：提供与大语言模型的统一交互接口，支持各类LLM、聊天模型和文本嵌入模型的调用</p></li><li><p><strong>Prompts (提示)</strong>：专门用于管理和优化提示模板，提供标准化的提示工程工具</p></li><li><p><strong>Indexes (索引)</strong>：提供高效的文档加载、分割和向量存储系统，支持大规模文本处理和检索</p></li><li><p><strong>Memory (记忆)</strong>：用于在交互过程中管理和存储状态信息，确保对话的连贯性和上下文理解</p></li><li><p><strong>Chains (链)</strong>：能将多个组件组合成端到端应用的核心机制，实现复杂的处理流程</p></li><li><p><strong>Agents (代理)</strong>：赋予LLM使用工具的能力，支持自主推理和行动决策</p></li></ul><p><img src="/medias/featureimages/blog/langchain-chu-ru-men/image1.png" alt="image"></p><h4 id="Prompts组件"><a href="#Prompts组件" class="headerlink" title="Prompts组件"></a>Prompts组件</h4><p><strong>概念与作用</strong>:</p><p>在LLM应用开发中,我们通常不会直接将用户输入传递给大模型,而是会将用户输入添加到一个更大的文本片段中,这个文本片段被称为Prompt。Prompt为大模型提供了任务相关的上下文和指令,帮助模型更好地理解和执行任务。</p><p>LangChain中的Prompts组件提供了一系列工具来管理和优化这些提示模板。主要包含两大类:</p><ul><li><p>PromptTemplate: 将Prompt按照template进行格式化,处理变量和组合</p></li><li><p>Selectors: 根据不同条件选择不同的提示词</p></li></ul><p><strong>基本构成</strong>:</p><p>在LangChain中,Prompts组件包含多个子组件:</p><p>角色提示模板:</p><ul><li><p>SystemMessagePromptTemplate: 系统角色消息模板</p></li><li><p>HumanMessagePromptTemplate: 人类角色消息模板</p></li><li><p>AIMessagePromptTemplate: AI角色消息模板</p></li></ul><p>提示模板类型:</p><ul><li><p>PromptTemplate: 文本提示模板</p></li><li><p>ChatPromptTemplate: 聊天消息提示模板</p></li><li><p>MessagePlaceholder: 消息占位符</p></li></ul><p><strong>关键操作</strong>:</p><p>格式化LangChain支持两种格式化方式</p><pre class="line-numbers language-language-python"><code class="language-language-python"># f-string方式prompt = PromptTemplate.from_template("请将一个关于&#123;subject&#125;的笑话")# jinja2方式prompt = PromptTemplate.from_template(    "请将一个关于&#123;&#123;subject&#125;&#125;的笑话",    template_format="jinja2")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提示模板拼接</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 字符串提示拼接prompt = (    PromptTemplate.from_template("请将一个关于&#123;subject&#125;的冷笑话")    + "，让我开心下"    + "\n使用&#123;language&#125;语言。")# 聊天提示拼接system_prompt = ChatPromptTemplate.from_messages([    ("system", "你是OpenAI开发的聊天机器人，请根据用户的提问进行回复，我叫&#123;username&#125;")])human_prompt = ChatPromptTemplate.from_messages([    ("human", "&#123;query&#125;")])prompt = system_prompt + human_prompt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>模板复用</strong>:</p><p>对于复杂的提示模板,LangChain提供了PipelinePromptTemplate来实现模板的复用:</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 描述提示模板instruction_template = "你正在模拟&#123;person&#125;。"instruction_prompt = PromptTemplate.from_template(instruction_template)# 示例提示模板example_template = """下面是一个交互例子:Q: &#123;example_q&#125;A: &#123;example_a&#125;"""example_prompt = PromptTemplate.from_template(example_template)# 开始提示模板start_template = """现在开始对话:Q: &#123;input&#125;A:"""start_prompt = PromptTemplate.from_template(start_template)# 组合模板pipeline_prompt = PipelinePromptTemplate(    final_prompt=full_prompt,    pipeline_prompts=[        ("instruction", instruction_prompt),        ("example", example_prompt),        ("start", start_prompt),    ])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong>:</p><p>选择合适的格式化方式</p><ol><li><p>简单变量替换使用f-string</p></li><li><p>需要条件判断等复杂逻辑时使用jinja2</p></li></ol><p>提示模板设计</p><ol><li><p>保持模板的清晰和可维护性</p></li><li><p>合理使用系统消息和示例</p></li><li><p>避免过于复杂的嵌套结构</p></li></ol><p>错误处理</p><ol><li><p>验证必要的变量是否存在</p></li><li><p>处理格式化可能出现的异常</p></li></ol><p>性能优化</p><ol><li><p>重复使用的模板要缓存</p></li><li><p>避免不必要的模板拼接操作</p></li></ol><h4 id="Model组件"><a href="#Model组件" class="headerlink" title="Model组件"></a>Model组件</h4><p><strong>基本概念</strong>:</p><p>Models是LangChain的核心组件，提供了一个标准接口来封装不同类型的LLM进行交互，LangChain本身不提供LLM,而是提供了接口来集成各种模型。</p><p>LangChain支持两种类型的模型:</p><ul><li><p>LLM: 使用纯文本作为输入和输出的大语言模型</p></li><li><p>Chat Model: 使用聊天消息列表作为输入并返回聊天消息的聊天模型</p></li></ul><p><strong>组件架构</strong>:</p><p>LangChain中Models组件的基类结构如下:</p><p>BaseLanguageModel(基类)</p><p>BaseLLM(大语言模型基类)<br>    - SimpleLLM(简化大语言模型)<br>    - 第三方LLM集成(OpenAI、百度文心等)</p><p>BaseChatModel(聊天模型基类)<br>    - SimpleChatModel(简化聊天模型)<br>    - 第三方Chat Model集成</p><h4 id="Message组件"><a href="#Message组件" class="headerlink" title="Message组件"></a>Message组件</h4><ul><li><p>SystemMessage: 系统消息</p></li><li><p>HumanMessage: 人类消息</p></li><li><p>AIMessage: AI消息</p></li><li><p>FunctionMessage: 函数调用消息</p></li><li><p>ToolMessage: 工具调用消息</p></li></ul><p><strong>核心办法</strong>:</p><p>Models组件提供了几个关键方法:</p><p>invoke&#x2F;invoke_sync: 调用模型生成内容</p><pre class="line-numbers language-language-python"><code class="language-language-python">llm = ChatOpenAI(model="gpt-3.5-turbo-16k")response = llm.invoke("你好!")# 异步调用async def generate():    response = await llm.ainvoke("你好!")    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>batch&#x2F;abatch: 批量调用处理多个输入</p><pre class="line-numbers language-language-python"><code class="language-language-python">messages = [    "请讲一个关于程序员的笑话",    "请讲一个关于Python的笑话"]responses = llm.batch(messages)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>stream&#x2F;astream: 流式返回生成内容</p><pre class="line-numbers language-language-python"><code class="language-language-python">response = llm.stream("请介绍下LLM和LLMOps")for chunk in response:    print(chunk.content, end="")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Message组件使用</strong>:</p><p>消息组件用于构建与聊天模型的交互:</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.messages import SystemMessage, HumanMessage, AIMessage# 创建消息system_msg = SystemMessage(content="你是一个AI助手")human_msg = HumanMessage(content="你好!")ai_msg = AIMessage(content="你好!我是AI助手")# 构建消息列表messages = [system_msg, human_msg, ai_msg]# 使用消息与模型交互response = chat_model.invoke(messages)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实践示例</strong>:</p><p>基本对话示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# 创建聊天模型chat = ChatOpenAI()# 创建提示模板prompt = ChatPromptTemplate.from_messages([    ("system", "你是一位&#123;role&#125;"),    ("human", "&#123;query&#125;")])# 调用模型response = chat.invoke(    prompt.format_messages(        role="Python专家",        query="什么是装饰器?"    ))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>流式输出示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">prompt = ChatPromptTemplate.from_template("&#123;subject&#125;的发展历史是什么?")# 创建模型llm = ChatOpenAI()# 流式生成response = llm.stream(    prompt.format_messages(subject="人工智能"))# 处理输出for chunk in response:    print(chunk.content, end="")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong>:</p><p>选择合适的模型类型</p><ol><li><p>简单文本生成任务使用LLM</p></li><li><p>对话类任务使用Chat Model</p></li></ol><p>正确处理异步操作</p><ol><li><p>在异步环境中使用ainvoke&#x2F;astream</p></li><li><p>批量处理时考虑使用batch</p></li></ol><p>异常处理</p><ol><li><p>处理模型调用可能的超时</p></li><li><p>捕获API错误并适当处理</p></li></ol><p>性能优化</p><ol><li><p>合理使用批处理</p></li><li><p>适时使用流式输出</p></li></ol><p><strong>OutputParser 解析器组件</strong>:</p><p>为什么需要输出解析器</p><p>在使用大模型时,我们经常会遇到输出解析的问题。比如:</p><pre class="line-numbers language-language-python"><code class="language-language-python">llm = ChatOpenAI()# 示例1: 返回的是自然语言llm.invoke("1+1等于几?")  # 输出: 1 + 1 等于 2。# 示例2: 包含多余信息llm.invoke("告诉我3个动物的名字。")  # 输出: 好的，这里有三种动物的名字：\n1. 狮子\n2. 大熊猫\n3. 斑马# 示例3: 格式不统一llm.invoke("给我一个json数据,键为a和b")  # 输出: &#123;\n "a": 10,\n "b": 20\n&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>OutputParser就是为了解决这些问题而设计的。它通过:</p><ol><li><p>预设提示 - 告诉LLM需要的输出格式</p></li><li><p>解析功能 - 将输出转换成指定格式</p></li></ol><p><strong>Parser类型详解</strong>:</p><p>Langchain 提供了多种Parser：</p><ol><li><p>基础Parser：</p><ul><li>StrOutputParser: 最简单的Parser,原样返回文本</li><li>BaseOutputParser: 所有Parser的基类</li><li>BaseLLMOutputParser: 专门用于LLM输出的基类</li></ul></li><li><p>格式化Parser：</p><ul><li>JsonOutputParser: 解析JSON格式输出</li><li>XMLOutputParser: 解析XML格式输出</li><li>PydanticOutputParser: 使用Pydantic模型解析输出</li></ul></li><li><p>列表类Parser：</p><ul><li>CommaSeparatedListOutputParser: 解析逗号分隔的列表</li><li>NumberedListOutputParser: 解析数字编号的列表</li></ul></li></ol><p><strong>实践示例</strong>:</p><ol><li>StrOutputParser使用：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAI# 创建链chain = (    ChatPromptTemplate.from_template("&#123;query&#125;")    | ChatOpenAI()    | StrOutputParser())# 调用response = chain.invoke(&#123;"query": "你好!"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.JsonOutputParser使用：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import JsonOutputParserfrom langchain_core.pydantic_v1 import BaseModel, Field# 定义输出结构class Joke(BaseModel):    joke: str = Field(description="回答用户的冷笑话")    punchline: str = Field(description="冷笑话的笑点")# 创建Parserparser = JsonOutputParser(pydantic_object=Joke)# 创建提示模板prompt = ChatPromptTemplate.from_template(    "回答用户的问题。\n&#123;format_instructions&#125;\n&#123;query&#125;\n")# 添加格式说明prompt = prompt.partial(format_instructions=parser.get_format_instructions())# 创建链chain = prompt | ChatOpenAI() | parser# 使用response = chain.invoke(&#123;"query": "请讲一个关于程序员的冷笑话"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>错误处理</strong>:</p><p>1.解析失败的处理：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import OutputParserExceptiontry:    result = parser.parse(llm_output)except OutputParserException as e:    # 处理解析错误    print(f"解析错误: &#123;e&#125;")    # 可以选择重试或使用默认值<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.使用重试机制：</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 可以配置回调来处理重试from langchain_core.callbacks import BaseCallbackHandlerclass RetryHandler(BaseCallbackHandler):    def on_retry(self, retry_state):        print(f"重试次数: &#123;retry_state.attempt_number&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong>:</p><ol><li><p>选择合适的Parser</p><ul><li>简单文本使用StrOutputParser</li><li>结构化数据使用JsonOutputParser或PydanticOutputParser</li><li>列表数据使用专门的列表Parser</li></ul></li><li><p>提示设计</p><ul><li>在提示中明确指定输出格式</li><li>使用Parser提供的format_instructions</li></ul></li><li><p>异常处理</p><ul><li>总是处理可能的解析错误</li><li>考虑添加重试机制</li><li>提供合理的默认值</li></ul></li><li><p>性能优化</p><ul><li>避免过于复杂的解析逻辑</li><li>合理使用缓存</li></ul></li></ol><h4 id="LCEL表达式与Runnable协议"><a href="#LCEL表达式与Runnable协议" class="headerlink" title="LCEL表达式与Runnable协议"></a>LCEL表达式与Runnable协议</h4><p><strong>为什么需要LCEL</strong>:</p><p>传统的链式调用方式存在嵌套问题：</p><pre class="line-numbers language-language-python"><code class="language-language-python">content = parser.invoke(    llm.invoke(        prompt.invoke(            &#123;"query": req.query.data&#125;        )    ))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>LCEL 提供了更优雅的方式：</p><pre class="line-numbers language-language-python"><code class="language-language-python">chain = prompt | llm | parsercontent = chain.invoke(&#123;"query": req.query.data&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>Runnable协议核心方法</strong>:</p><ul><li><p>invoke&#x2F;ainvoke: 调用组件</p></li><li><p>batch&#x2F;abatch: 批量处理</p></li><li><p>stream&#x2F;astream: 流式输出</p></li><li><p>transform: 转换输入输出</p></li></ul><p><strong>两个核心类</strong>:</p><ol><li>RunnableParallel - 并行执行多个Runnable</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.runnables import RunnableParallel# 并行执行多个链chain = RunnableParallel(    joke=joke_chain,    poem=poem_chain)resp = chain.invoke(&#123;"subject": "程序员"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.RunnablePassthrough - 传递数据</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.runnables import RunnablePassthrough# 构建检索链chain = (    RunnablePassthrough.assign(        context=lambda query: retrieval(query)    )    | prompt     | llm     | parser)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实践示例</strong>:</p><ol><li>基础链构建：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import StrOutputParser# 创建组件prompt = ChatPromptTemplate.from_template("&#123;input&#125;")llm = ChatOpenAI()parser = StrOutputParser()# 构建链chain = prompt | llm | parser# 执行response = chain.invoke(&#123;"input": "Hello!"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.带检索的链：</p><pre class="line-numbers language-language-python"><code class="language-language-python">def retrieval(query: str) -> str:    return "相关文档内容..."# 构建链chain = (    &#123;        "context": retrieval,        "question": RunnablePassthrough()    &#125;    | prompt    | llm    | StrOutputParser())# 执行response = chain.invoke("问题")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong>:</p><ol><li><p>链的设计</p><ul><li>使用管道操作符(|)构建简单链</li><li>复杂逻辑使用RunnableParallel</li><li>数据传递用RunnablePassthrough</li></ul></li><li><p>错误处理</p><ul><li>合理使用try&#x2F;except</li><li>实现错误回调处理</li></ul></li><li><p>性能优化</p><ul><li>合适场景使用并行执行</li><li>批处理代替单个处理</li></ul></li><li><p>代码可维护性</p><ul><li>链结构保持清晰</li><li>适当拆分复杂链</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发组件深度解析</title>
      <link href="/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/"/>
      <url>/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在当今的AI应用开发中，检索增强生成(Retrieval-Augmented Generation, RAG)已经成为一种重要的技术范式。它通过将大语言模型与外部知识库结合，极大地提升了AI系统的知识获取能力和输出质量。而LangChain作为一个强大的框架，为RAG应用的开发提供了丰富的组件支持。本文将深入剖析LangChain中RAG应用开发的核心组件，帮助你更好地理解和使用这些工具。</p><h2 id="核心组件概览"><a href="#核心组件概览" class="headerlink" title="核心组件概览"></a>核心组件概览</h2><p>在开始深入学习之前,我们先来了解LangChain中RAG应用开发涉及的主要组件:</p><ol><li>Document组件与文档加载器 - 负责文档的加载和基础处理</li><li>文档转换器与分割器 - 处理文档转换和分块</li><li>VectorStore组件 - 实现向量存储和检索</li><li>Blob相关组件 - 处理二进制大对象数据</li></ol><h3 id="Document-组件与文档加载器详解"><a href="#Document-组件与文档加载器详解" class="headerlink" title="Document 组件与文档加载器详解"></a>Document 组件与文档加载器详解</h3><h4 id="Document-组件基础"><a href="#Document-组件基础" class="headerlink" title="Document 组件基础"></a>Document 组件基础</h4><p>Document 是 LangChain 的核心组件之一，它定义了一个通用的文档结构，包含两个基本要素：</p><pre class="line-numbers language-language-python"><code class="language-language-python">Document = page_content(页面内容) + metadata(元数据)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这种结构允许我们统一处理各种类型的文档，同时保留文档的元信息。</p><h4 id="文档加载器类型"><a href="#文档加载器类型" class="headerlink" title="文档加载器类型"></a>文档加载器类型</h4><p>LangChain 提供了多种文档加载器：</p><ol><li>通用文本加载器</li><li>CSV文件加载器</li><li>HTML网页加载器</li><li>PDF文档加载器</li><li>Markdown文档加载器</li></ol><p><img src="/medias/featureimages/blog/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/image1.png" alt="image"></p><p>每种加载器都专门处理特定类型的文档，但它们都会将文档转换成统一的Document格式。</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_community.document_loaders import TextLoader# 加载文本文件示例loader = TextLoader("./data.txt", encoding="utf-8")documents = loader.load()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="异步加载支持"><a href="#异步加载支持" class="headerlink" title="异步加载支持"></a>异步加载支持</h4><p>对于大型文档，LangChain提供了异步加载方式：</p><pre class="line-numbers language-language-python"><code class="language-language-python">async def load_documents():    async with aiofiles.open(file_path, encoding="utf-8") as f:        # 异步处理文档        yield Document(            page_content=line,            metadata=&#123;"source": file_path&#125;        )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="文档转换器与分割器"><a href="#文档转换器与分割器" class="headerlink" title="文档转换器与分割器"></a>文档转换器与分割器</h3><h4 id="DocumentTransformer-组件"><a href="#DocumentTransformer-组件" class="headerlink" title="DocumentTransformer 组件"></a>DocumentTransformer 组件</h4><p>文档转换器用于处理以下常见问题：</p><ol><li>文档太大导致的性能问题</li><li>原始文档格式不符合要求</li><li>文档内容需要标准化处理</li></ol><h4 id="文档转换器的工作原理"><a href="#文档转换器的工作原理" class="headerlink" title="文档转换器的工作原理"></a>文档转换器的工作原理</h4><p>DocumentTransformer组件的主要职责是对文档进行各种转换操作，包括：</p><ol><li>文档切割</li><li>文档层级提取</li><li>文档翻译</li><li>HTML标签处理</li><li>重排等多个功能</li></ol><p>在LangChain中，所有文档转换器都继承自BaseDocumentTransformer基类，它提供了两个核心方法：</p><pre class="line-numbers language-language-python"><code class="language-language-python">class BaseDocumentTransformer:    def transform_documents(self):         # 转换文档列表        pass            async def atransform_documents(self):        # 异步转换处理        pass<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="文档分割器详解"><a href="#文档分割器详解" class="headerlink" title="文档分割器详解"></a>文档分割器详解</h3><h4 id="字符分割器（CharacterTextSplitter）"><a href="#字符分割器（CharacterTextSplitter）" class="headerlink" title="字符分割器（CharacterTextSplitter）"></a>字符分割器（CharacterTextSplitter）</h4><p>CharacterTextSplitter 是基础的分割器，它有以下重要参数：</p><ol><li><code>separator</code>: 分割符,默认为’\n\n’</li><li><code>chunk_size</code>: 每块文本的最大大小,默认4000</li><li><code>chunk_overlap</code>: 块与块之间的重叠大小,默认200</li><li><code>length_function</code>: 计算文本长度的函数,默认len</li><li><code>keep_separator</code>: 是否在分割的块中保留分隔符</li></ol><p>使用示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter(    separator="\n\n",    chunk_size=500,    chunk_overlap=50,    add_start_index=True)# 使用分割器处理文档splits = text_splitter.split_documents(documents)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，有以下几点建议：</p><ol><li>选择合适的chunk_size<ul><li>太大会影响处理效率</li><li>太小可能破坏语义完整性</li><li>建议根据实际需求在400-1000之间调整</li></ul></li><li>合理设置overlap<ul><li>设置适当的重叠可以保持上下文连贯</li><li>通常设置为chunk_size的10%-20%</li></ul></li><li>注意分隔符的选择<ul><li>根据文档类型选择合适的分隔符</li><li>可以使用多级分隔符策略</li></ul></li></ol><h3 id="VectorStore组件与检索器"><a href="#VectorStore组件与检索器" class="headerlink" title="VectorStore组件与检索器"></a>VectorStore组件与检索器</h3><h4 id="VectorStore基础概念"><a href="#VectorStore基础概念" class="headerlink" title="VectorStore基础概念"></a>VectorStore基础概念</h4><p>VectorStore组件负责：</p><ol><li>存储文档的向量表示</li><li>提供相似性检索功能</li><li>支持不同的向量检索策略</li></ol><h4 id="检索器的使用"><a href="#检索器的使用" class="headerlink" title="检索器的使用"></a>检索器的使用</h4><p>LangChain 提供了多种检索策略：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain import VectorStore# 基础相似性检索results = vectorstore.similarity_search(query)# 带相似度分数的检索results = vectorstore.similarity_search_with_score(query)# MMR检索策略results = vectorstore.max_marginal_relevance_search(query)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="VectorStore实现细节"><a href="#VectorStore实现细节" class="headerlink" title="VectorStore实现细节"></a>VectorStore实现细节</h3><h4 id="支持的向量数据库"><a href="#支持的向量数据库" class="headerlink" title="支持的向量数据库"></a>支持的向量数据库</h4><ul><li>Chroma</li><li>FAISS</li><li>Pinecone</li><li>Milvus</li></ul><h4 id="检索策略详解"><a href="#检索策略详解" class="headerlink" title="检索策略详解"></a>检索策略详解</h4><ol><li>相似度检索(Similarity Search)<ul><li>基于余弦相似度</li><li>支持Top-K检索</li></ul></li><li>MMR检索(Maximum Marginal Relevance)<ul><li>平衡相关性和多样性</li><li>可配置lambda参数调整权重</li></ul></li><li>混合检索策略<ul><li>关键词+语义检索</li><li>支持自定义评分函数</li></ul></li></ol><h3 id="Blob与BlobParser组件"><a href="#Blob与BlobParser组件" class="headerlink" title="Blob与BlobParser组件"></a>Blob与BlobParser组件</h3><h4 id="Blob方案介绍"><a href="#Blob方案介绍" class="headerlink" title="Blob方案介绍"></a>Blob方案介绍</h4><p>Blob是LangChain处理二进制数据的解决方案，它具有以下特点：</p><ol><li>支持存储字节流数据</li><li>提供统一的数据访问接口</li><li>灵活的元数据管理</li></ol><p>基本使用示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.document_loaders import Blobfrom langchain_core.document_loaders.base import BaseBlobParser# 创建Blob对象blob = Blob.from_path("./data.txt")# 使用解析器parser = CustomParser()documents = list(parser.lazy_parse(blob))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Blob数据存储类详解"><a href="#Blob数据存储类详解" class="headerlink" title="Blob数据存储类详解"></a>Blob数据存储类详解</h4><p>LangChain中的Blob数据存储提供了丰富的属性和方法，让我们详细了解一下：</p><p>核心属性</p><ol><li><strong>data</strong>: 原始数据，支持存储字节，字符串数据</li><li><strong>mimetype</strong>: 文件的mimetype类型</li><li><strong>encoding</strong>: 文件的编码，默认utf-8</li><li><strong>path</strong>: 文件的原始路径</li><li><strong>metadata</strong>: 存储的元数据，通常包含source字段</li></ol><p>常用方法</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 字符串转换as_string(): # 将数据转换为字符串# 字节转换as_bytes(): # 将数据转换为字节数据# 字节流操作as_bytes_io(): # 将数据转换为字节流# 从路径加载from_path(): # 从文件路径加载Blob数据# 从原始数据加载from_data(): # 从原始数据加载Blob数据<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="BlobLoader实现"><a href="#BlobLoader实现" class="headerlink" title="BlobLoader实现"></a>BlobLoader实现</h4><p>BlobLoader是一个抽象接口，用于实现二进制数据的加载。以下是一个自定义BlobLoader的示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.document_loaders import Blobfrom langchain_core.document_loaders.base import BaseBlobParserclass CustomBlobLoader(ABC):    """自定义Blob加载器实现"""        @abstractmethod    def yield_blobs(        self,    ) -> Iterable[Blob]:        """加载并返回Blob数据流"""            def __init__(self, file_path: str):        self.file_path = file_path            def lazy_load(self):        """延迟加载实现"""        for blob in self.yield_blobs():            yield Document(                page_content=blob.as_string(),                metadata=&#123;"source": blob.source&#125;            )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="通用加载器使用最佳实践"><a href="#通用加载器使用最佳实践" class="headerlink" title="通用加载器使用最佳实践"></a>通用加载器使用最佳实践</h4><p>GenericLoader是LangChain提供的一个通用加载器，它结合了BlobLoader和BaseBlobParser的功能：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_community.document_loaders.generic import GenericLoader# 创建通用加载器loader = GenericLoader.from_filesystem(    "./",  # 文件系统路径    glob="*.txt",  # 文件匹配模式    show_progress=True  # 显示进度)# 使用加载器for idx, doc in enumerate(loader.lazy_load()):    print(f"当前加载第&#123;idx + 1&#125;个文件, 文件信息:&#123;doc.metadata&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="性能优化建议"><a href="#性能优化建议" class="headerlink" title="性能优化建议"></a>性能优化建议</h4><ol><li>使用延迟加载<ul><li>对于大文件优先使用lazy_load()</li><li>避免一次性加载全部内容</li></ul></li><li>合理配置缓存<ul><li>利用缓存减少重复加载</li><li>及时清理不需要的缓存</li></ul></li><li>错误处理<ul><li>实现适当的错误处理机制</li><li>记录加载过程中的异常</li></ul></li><li>进度监控<ul><li>对大规模数据处理添加进度显示</li><li>实现断点续传机制</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PostgreSQL 复习</title>
      <link href="/2025/01/02/postgresql-bi-ji/"/>
      <url>/2025/01/02/postgresql-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="PostgreSQL-复习"><a href="#PostgreSQL-复习" class="headerlink" title="PostgreSQL 复习"></a>PostgreSQL 复习</h1><h2 id="索引-Indexing"><a href="#索引-Indexing" class="headerlink" title="索引 (Indexing)"></a>索引 (Indexing)</h2><p>除了标准的 B-Tree 索引，你必须了解 PG 的特色索引。</p><ul><li><strong>GIN (Generalized Inverted Index)</strong>: 广义倒排索引。<ul><li><strong>用途</strong>：专门用于索引“组合”类型的数据，最典型的就是 <code>JSONB</code>、数组 (<code>Array</code>) 和全文搜索 (<code>tsvector</code>)。</li><li><strong>原理</strong>：它为元素集合中的每个元素（例如 JSON 的 key，数组的元素）创建索引条目，类似于倒排索引。</li><li><strong>面试问题</strong>：“如何加速对 JSONB 列中某个 key 的查询？” 答案就是使用 GIN 索引。</li></ul></li><li><strong>GiST (Generalized Search Tree)</strong>: 广义搜索树。<ul><li><strong>用途</strong>：用于索引更复杂的数据结构，如几何图形（二维、三维）和全文搜索。PostGIS 插件就大量使用 GiST 索引。</li><li><strong>面试问题</strong>：“如果要实现‘查找附近5公里内的所有门店’的功能，数据库层面可以做什么优化？” 答案是使用 PostGIS 扩展，并为地理位置列创建 GiST 索引。</li></ul></li><li><strong>BRIN (Block Range Index)</strong>: 块范围索引。<ul><li><strong>用途</strong>：适用于数据在物理存储上具有天然排序性的<strong>超大表</strong>（例如按时间戳排序的日志表）。</li><li><strong>原理</strong>：它只存储每个“块范围”（例如 128 个数据页）的最大值和最小值，因此索引本身非常小，维护成本极低。</li><li><strong>面试问题</strong>：“一张数百亿行的日志表，按时间分区，如何对时间戳列进行高效索引且占用空间尽可能小？” BRIN 索引是标准答案。</li></ul></li></ul><h2 id="JSONB-数据类型"><a href="#JSONB-数据类型" class="headerlink" title="JSONB 数据类型"></a><code>JSONB</code> 数据类型</h2><p>这是 PG 在现代应用开发中的一大亮点。</p><ul><li><strong>为什么用 <code>JSONB</code> 而不是 <code>JSON</code> 或 <code>TEXT</code>？</strong><ul><li><code>vs TEXT</code>: <code>JSONB</code> 会校验 JSON 格式的有效性，且内部二进制结构支持高效的字段提取和查询。</li><li><code>vs JSON</code>: <code>JSONB</code> 存的是解析后的二进制，插入时稍慢，但查询时快得多。更重要的是，<strong><code>JSONB</code> 可以被 GIN 索引高效地支持</strong>。</li></ul></li><li><strong>常用操作符</strong><ul><li><code>&gt;</code>: 按 key 获取字段，返回 <code>jsonb</code>。</li><li><code>&gt;&gt;</code>: 按 key 获取字段，返回 <code>text</code>。</li><li><code>@&gt;</code>: <code>A @&gt; B</code> 表示 A 是否“包含”B。常用于查询 JSON 中是否存在某个键值对。</li><li><code>?</code>: 判断顶层 key 是否存在。</li></ul></li></ul><h2 id="WITH-子句-Common-Table-Expressions-CTEs"><a href="#WITH-子句-Common-Table-Expressions-CTEs" class="headerlink" title="WITH 子句 (Common Table Expressions - CTEs)"></a><code>WITH</code> 子句 (Common Table Expressions - CTEs)</h2><p>虽然现代 MySQL 也支持，但 PG 对 CTE 的支持更早、更完善，并且支持<strong>递归查询</strong>。</p><ul><li><strong>优点</strong>：提高复杂 SQL 的可读性；可以被多次引用；支持递归（如查询组织架构树、物料清单等）。</li><li><strong>面试问题</strong>：“如何查询一个组织架构中某员工的所有下属（包括间接下属）？” 标准答案就是使用 <code>WITH RECURSIVE</code>。</li></ul><h2 id="窗口函数-Window-Functions"><a href="#窗口函数-Window-Functions" class="headerlink" title="窗口函数 (Window Functions)"></a>窗口函数 (Window Functions)</h2><p>同样，现代 MySQL 也支持，但这是衡量 SQL 掌握水平的经典问题。</p><ul><li><strong>核心</strong>：<code>OVER (PARTITION BY ... ORDER BY ...)</code></li><li><strong>常用函数</strong>：<code>ROW_NUMBER()</code>, <code>RANK()</code>, <code>DENSE_RANK()</code>, <code>LAG()</code>, <code>LEAD()</code> 等。</li><li><strong>面试问题</strong>：“如何查询每个部门工资前三名的员工？” 这是窗口函数的经典应用场景。</li></ul><hr><h2 id="运维与性能优化"><a href="#运维与性能优化" class="headerlink" title="运维与性能优化"></a>运维与性能优化</h2><p><strong><code>EXPLAIN ANALYZE</code></strong></p><p>这是 PG 性能分析的利器，比单纯的 <code>EXPLAIN</code> 更强大，因为它会<strong>实际执行</strong>查询。</p><ul><li><strong>关注点</strong>：<ul><li><strong>Scan Type</strong>: 是 <code>Seq Scan</code> (全表扫描) 还是 <code>Index Scan</code> &#x2F; <code>Bitmap Scan</code> (索引扫描)？</li><li><strong>Cost</strong>: 估算成本，<code>startup</code> (启动成本) 和 <code>total</code> (总成本)。</li><li><strong>Rows</strong>: 估算的行数 (<code>rows</code>) 和实际返回的行数 (<code>rows</code>)，如果两者差距巨大，说明统计信息可能已过时。</li><li><strong>Execution Time</strong>: 实际执行时间。</li></ul></li></ul><p><strong><code>VACUUM</code></strong></p><p>这是 PG 最具特色的运维操作，与 MVCC 紧密相关。</p><ul><li><strong>为什么需要 <code>VACUUM</code>？</strong><ul><li>当一行数据被 <code>UPDATE</code> 或 <code>DELETE</code> 时，旧版本的数据并不会立即从物理上删除，只是被标记为“死的”（dead tuple）。这些死元组会占用磁盘空间，导致表膨胀（Bloat）。</li><li><code>VACUUM</code> 的主要作用就是<strong>回收这些死元组占用的空间</strong>，并<strong>更新表的统计信息</strong>供查询优化器使用。</li></ul></li><li><strong><code>VACUUM</code> vs <code>VACUUM FULL</code></strong><ul><li><code>VACUUM</code>: 不会锁表，可以并发执行。它只是将空间标记为可重用，并不会将空间还给操作系统，表文件大小（High Water Mark）不变。</li><li><code>VACUUM FULL</code>: 会<strong>锁住整张表</strong>，将表中的有效数据重新写入一个新的物理文件，然后将空间还给操作系统。这是一个非常重的操作，<strong>线上环境需谨慎使用</strong>。</li></ul></li><li><strong>Autovacuum</strong><ul><li>PG 内置的后台进程，会自动检测并触发 <code>VACUUM</code> 和 <code>ANALYZE</code>。通常情况下，我们依赖它来做日常维护。你需要了解它的基本工作原理和相关配置参数。</li></ul></li></ul><p><strong>连接池 (Connection Pooling)</strong></p><p>由于 PG 的多进程模型，建立连接的成本相对较高。</p><ul><li><strong>为什么必须用连接池？</strong> 在高并发场景下，频繁创建和销毁数据库连接会严重消耗服务器资源，导致性能瓶颈。</li><li><strong>常用工具</strong>：<strong>PgBouncer</strong> 是最常用、最轻量级的外部连接池。面试时能提到它会很加分。</li></ul><p><strong>复制与高可用 (Replication &amp; HA)</strong></p><ul><li><strong>流复制 (Streaming Replication)</strong>: PG 原生的、最主要的复制方式（物理复制）。通过持续传输 WAL (Write-Ahead Logging) 日志来实现主备同步。可以配置为同步或异步。</li><li><strong>逻辑复制 (Logical Replication)</strong>: 基于发布&#x2F;订阅模型的复制方式。可以更灵活地选择只复制某些表，甚至可以在不同 PG 大版本之间进行复制。</li><li><strong>高可用方案</strong>: 提到 <strong>Patroni</strong> + <strong>Etcd&#x2F;Consul</strong> 组合是当前构建 PG 高可用集群的主流和最佳实践方案。</li></ul><hr><h2 id="PostgreSQL-中-CHAR-VARCHAR-TEXT-有什么区别？应该如何选择？"><a href="#PostgreSQL-中-CHAR-VARCHAR-TEXT-有什么区别？应该如何选择？" class="headerlink" title="PostgreSQL 中 CHAR, VARCHAR, TEXT 有什么区别？应该如何选择？"></a>PostgreSQL 中 <code>CHAR</code>, <code>VARCHAR</code>, <code>TEXT</code> 有什么区别？应该如何选择？</h2><table><thead><tr><th>特性</th><th><code>DELETE</code></th><th><code>TRUNCATE</code></th><th><code>DROP</code></th></tr></thead><tbody><tr><td><strong>操作对象</strong></td><td>表中的<strong>一行或多行</strong>数据</td><td>表中的<strong>所有行</strong>数据</td><td><strong>整个表对象</strong>（结构+数据）</td></tr><tr><td><strong>SQL类型</strong></td><td>DML (数据操作语言)</td><td>DDL (数据定义语言)</td><td>DDL (数据定义语言)</td></tr><tr><td><strong>WHERE 子句</strong></td><td><strong>可以使用</strong>，指定删除行</td><td><strong>不可以使用</strong></td><td><strong>不可以使用</strong></td></tr><tr><td><strong>事务与回滚</strong></td><td><strong>可回滚</strong> (Rollback)</td><td><strong>不可回滚</strong> (或隐式提交，取决于数据库)</td><td><strong>不可回滚</strong></td></tr><tr><td><strong>执行速度</strong></td><td><strong>慢</strong>（逐行删除，记录日志）</td><td><strong>非常快</strong>（通常是释放数据页）</td><td><strong>非常快</strong>（释放表空间，删除元数据）</td></tr><tr><td><strong>触发器 (Trigger)</strong></td><td><strong>会触发</strong>行级删除触发器</td><td><strong>不会触发</strong>删除触发器</td><td><strong>不会触发</strong>删除触发器</td></tr><tr><td><strong>空间回收</strong></td><td>通常不立即释放磁盘空间</td><td>通常立即释放磁盘空间</td><td>立即释放磁盘空间</td></tr><tr><td><strong>自增ID重置</strong></td><td><strong>不重置</strong></td><td><strong>重置</strong></td><td>表都没了，无从谈起</td></tr></tbody></table><p>这三者<strong>在性能上确实没有差别</strong>。它们的主要区别在于<strong>行为和约束</strong>：</p><ol><li><strong><code>TEXT</code></strong> 是无约束的变长字符串。</li><li><strong><code>VARCHAR(n)</code></strong> 是增加了<strong>长度上限约束</strong>的变长字符串。</li><li><strong><code>CHAR(n)</code></strong> 是会进行<strong>空格填充</strong>的定长字符串。</li></ol><h2 id="PostgreSQL-事务隔离级别"><a href="#PostgreSQL-事务隔离级别" class="headerlink" title="PostgreSQL 事务隔离级别"></a>PostgreSQL 事务隔离级别</h2><ol><li><strong>Read Uncommitted (读未提交)</strong>：<ul><li><strong>现象</strong>：一个事务可以读到另一个事务<strong>尚未提交</strong>的修改。这会导致<strong>脏读 (Dirty Read)</strong>。</li><li><strong>PostgreSQL 实现</strong>：<strong>在 PostgreSQL 中，这个级别实际上和下一个级别‘读已提交’的行为完全一样</strong>，也就是说 PG 不会出现脏读。这是一个重要的面试点。</li></ul></li><li><strong>Read Committed (读已提交)</strong>：<ul><li><strong>现象</strong>：一个事务只能读到其他事务<strong>已经提交</strong>的数据。这是 <strong>PostgreSQL 的默认隔离级别</strong>。</li><li><strong>问题</strong>：它能避免“脏读”，但无法避免“<strong>不可重复读 (Non-Repeatable Read)</strong>”。即在同一个事务内，两次执行相同的查询，可能因为其他事务在此期间提交了更新，而得到不同的结果。</li></ul></li><li><strong>Repeatable Read (可重复读)</strong>：<ul><li><strong>现象</strong>：保证在同一个事务内，多次读取同样的数据，结果总是一致的。它通过在事务开始时创建一个<strong>数据快照 (Snapshot)</strong> 来实现。</li><li><strong>问题</strong>：它能避免“脏读”和“不可重复读”，但在 PostgreSQL 中，它仍然可能出现“<strong>幻读 (Phantom Read)</strong>”的变种问题。虽然你看不到别的事务新插入的行，但如果你尝试 <code>INSERT</code> 或 <code>UPDATE</code> 的数据与另一个并发事务提交的数据冲突，你的事务会失败并回滚，提示 <code>serialization failure</code>。</li></ul></li><li><strong>Serializable (可串行化)</strong>：<ul><li><strong>现象</strong>：最高的隔离级别。它保证并发执行的多个事务，其最终结果与按某种顺序依次执行它们的结果完全相同。</li><li><strong>实现</strong>：它能避免包括脏读、不可重复读、幻读在内的所有并发异常。PostgreSQL 通过一种叫做 <strong>SSI (Serializable Snapshot Isolation)</strong> 的技术来实现，比传统的锁机制性能更好。如果检测到可能破坏串行化执行的依赖关系，其中一个事务会被强制回滚</li></ul></li></ol><h2 id="JSON-和-JSONB-的区别是什么？在什么场景下应该使用-JSONB？"><a href="#JSON-和-JSONB-的区别是什么？在什么场景下应该使用-JSONB？" class="headerlink" title="JSON 和 JSONB 的区别是什么？在什么场景下应该使用 JSONB？"></a>JSON 和 JSONB 的区别是什么？在什么场景下应该使用 JSONB？</h2><p><code>JSON</code> 和 <code>JSONB</code> 的核心区别在于它们的<strong>存储方式</strong>：</p><ol><li><strong><code>JSON</code></strong> 存储的是<strong>纯文本</strong>，它会原封不动地保留你插入的字符串，包括空格、键的顺序和重复的键。</li><li><strong><code>JSONB</code></strong> 存储的是一种<strong>分解后的二进制格式</strong>。它在存入时就已经被解析和优化，会去除不必要的空格，并且不保证键的顺序。</li></ol><p>这个区别导致了它们在性能和功能上的巨大差异：</p><ul><li><strong>性能</strong>：<code>JSONB</code> 的查询性能远超 <code>JSON</code>，因为它不需要在查询时反复解析文本。</li><li><strong>索引</strong>：<code>JSONB</code> 支持强大的 <strong>GIN 索引</strong>，可以高效地对 JSON 内部的键值进行索引和搜索，而 <code>JSON</code> 的索引能力非常有限。</li></ul><h2 id="PostgreSQL-和-MySQL-相比，你认为它最大的优势和劣势分别是什么？"><a href="#PostgreSQL-和-MySQL-相比，你认为它最大的优势和劣势分别是什么？" class="headerlink" title="PostgreSQL 和 MySQL 相比，你认为它最大的优势和劣势分别是什么？"></a>PostgreSQL 和 MySQL 相比，你认为它最大的优势和劣势分别是什么？</h2><p><strong>优势一：它是一个“数据平台”，而不仅仅是数据库 (Extensibility &amp; Advanced Features)</strong></p><p>这是 PostgreSQL 最核心的竞争力。它不满足于只做一个关系型数据库，它的目标是成为处理各种数据的统一平台。</p><ul><li><strong>强大的扩展性</strong>：通过插件（Extensions），PG 可以变身为各种专业的数据库。这是 MySQL 难以比拟的。<ul><li>需要处理地理空间数据？<code>CREATE EXTENSION postgis;</code>，它就成了业界顶级的 GIS 数据库。</li><li>需要处理时序数据？<code>CREATE EXTENSION timescaledb;</code>，它就成了高性能的时序数据库。</li><li>需要处理向量数据以支持 AI 应用？<code>CREATE EXTENSION pgvector;</code> 即可。</li></ul></li><li><strong>内置的“多模态”能力</strong>：原生的 <code>JSONB</code> 类型，让它拥有了媲美 MongoDB 的文档处理能力；原生的数组（Array）、范围（Range）等丰富类型，让数据建模的可能性远超传统关系型数据库。</li><li><strong>外部数据包装器 (FDW)</strong>：这是一个“神器”，允许你直接在 PostgreSQL 内部，用 SQL 查询其他数据源，比如另一个 PostgreSQL、MySQL、Redis 甚至是一个 CSV 文件，就好像它们是本地的表一样。</li></ul><p><strong>这个优势带来的直接好处是：</strong> 极大地简化了技术栈。一个原本需要“MySQL + MongoDB + Elasticsearch + GIS数据库”的复杂系统，现在可能只需要一个高度扩展的 PostgreSQL 就能搞定，显著降低了开发和运维的复杂性。</p><p>优势二：<strong>PostgreSQL</strong> 是一个<strong>强类型</strong>的数据库，而<strong>MySQL</strong> 则会执行<strong>隐式类型转换。</strong></p><ul><li><strong>可预测性高，Bug 更少</strong>：强类型检查能从根源上杜绝很多因类型不匹配而导致的潜在 bug。你不会因为一个地方传了数字，另一个地方传了字符串而得到意想不到的查询结果。代码的行为非常稳定可靠。</li><li><strong>有利于索引优化</strong>：当 WHERE 子句中的常量类型与列类型完全匹配时，查询优化器能更准确、更高效地使用索引。隐式转换有时会导致优化器放弃使用索引，造成性能问题。</li></ul><hr><h2 id="MySQL-换成-PostgreSQL需要考虑的事情"><a href="#MySQL-换成-PostgreSQL需要考虑的事情" class="headerlink" title="MySQL 换成 PostgreSQL需要考虑的事情"></a>MySQL 换成 PostgreSQL需要考虑的事情</h2><h3 id="一、表字段设计思维的转变"><a href="#一、表字段设计思维的转变" class="headerlink" title="一、表字段设计思维的转变"></a>一、表字段设计思维的转变</h3><h3 id="1-告别-“万物皆-VARCHAR”，拥抱丰富的数据类型"><a href="#1-告别-“万物皆-VARCHAR”，拥抱丰富的数据类型" class="headerlink" title="1. 告别 “万物皆 VARCHAR”，拥抱丰富的数据类型"></a>1. 告别 “万物皆 <code>VARCHAR</code>”，拥抱丰富的数据类型</h3><ul><li><strong>MySQL 习惯</strong>：对不确定的文本字段使用 <code>VARCHAR(255)</code>，标签、类别等用字符串或数字存储。</li><li><strong>PostgreSQL 思维转变</strong>：PG 鼓励你使用更精确、更具业务含义的类型，把数据校验工作下沉到数据库层面。<ul><li><strong>场景：用户角色</strong><ul><li><strong>MySQL 设计</strong>：<code>role VARCHAR(20)</code> 或者 <code>role_id INT</code>。</li><li><strong>PostgreSQL 设计</strong>：<code>CREATE TYPE user_role AS ENUM (&#39;admin&#39;, &#39;member&#39;, &#39;guest&#39;);</code> 然后字段类型直接设为 <code>user_role</code>。这样做的好处是，数据库层面就保证了角色字段不可能被插入一个无效的值，数据更干净。</li></ul></li><li><strong>场景：商品标签</strong><ul><li><strong>MySQL 设计</strong>：<code>tags VARCHAR(255)</code>，里面存 <code>&quot;tag1,tag2,tag3&quot;</code> 这样的逗号分隔字符串。</li><li><strong>PostgreSQL 设计</strong>：<code>tags TEXT[]</code> (数组类型)。这样做，你可以直接在数据库层面查询包含某个特定标签的商品，而不需要把字符串取到应用层再拆分处理。</li></ul></li><li><strong>场景：存储配置信息</strong><ul><li><strong>MySQL 设计</strong>：<code>settings TEXT</code>，里面存一个 JSON 字符串。</li><li><strong>PostgreSQL 设计</strong>：<code>settings JSONB</code>。这样做，你可以直接对 JSON 内部的某个键进行查询和索引，性能极高。</li></ul></li></ul></li></ul><h3 id="2-日期-时间设计的“最佳实践”"><a href="#2-日期-时间设计的“最佳实践”" class="headerlink" title="2. 日期&#x2F;时间设计的“最佳实践”"></a>2. 日期&#x2F;时间设计的“最佳实践”</h3><ul><li><strong>MySQL 习惯</strong>：经常使用 <code>DATETIME</code> 或 <code>TIMESTAMP</code>，对时区的处理有时会比较混乱，依赖应用层来保证。</li><li><strong>PostgreSQL 思维转变</strong>：<strong>默认使用 <code>TIMESTAMPTZ</code> (Timestamp with Time Zone)</strong>。这是一个“最佳实践”级别的改变。<code>TIMESTAMPTZ</code> 在存储时会将所有时间统一转换为 UTC，在取出时再根据当前会话的时区转换为本地时间。在一个全球化的、服务器可能部署在不同时区的项目中，使用 <code>TIMESTAMPTZ</code> 可以从根源上杜绝几乎所有的时区混乱问题。</li></ul><hr><h3 id="二、SQL-查询与代码编写的差异"><a href="#二、SQL-查询与代码编写的差异" class="headerlink" title="二、SQL 查询与代码编写的差异"></a>二、SQL 查询与代码编写的差异</h3><p>这是开发过程中最常遇到的“坑”。</p><h3 id="1-严格的类型匹配：“我的代码怎么报错了？”"><a href="#1-严格的类型匹配：“我的代码怎么报错了？”" class="headerlink" title="1. 严格的类型匹配：“我的代码怎么报错了？”"></a>1. 严格的类型匹配：“我的代码怎么报错了？”</h3><ul><li><strong>MySQL 习惯</strong>：<code>WHERE user_id = &#39;123&#39;</code> 这样的查询（数字列 vs 字符串常量）能正常工作，因为 MySQL 会帮你做隐式转换。</li><li><strong>PostgreSQL “坑”</strong>：同样的代码，无论是原生 SQL 还是通过 ORM 执行，都会在 PostgreSQL 中<strong>直接报错</strong> (<code>operator does not exist: integer = character varying</code>)。</li><li><strong>开发者的转变</strong>：你必须养成编写类型安全的代码的习惯。在代码中传递参数时，要确保其类型与数据库字段类型一致。这在一开始可能会觉得“麻烦”，但长期来看，它避免了无数因类型模糊而产生的潜在 Bug。</li></ul><h3 id="2-大小写敏感：“我的表-字段怎么找不到了？”"><a href="#2-大小写敏感：“我的表-字段怎么找不到了？”" class="headerlink" title="2. 大小写敏感：“我的表&#x2F;字段怎么找不到了？”"></a>2. 大小写敏感：“我的表&#x2F;字段怎么找不到了？”</h3><ul><li><strong>MySQL 习惯</strong>：在 Windows 上不区分大小写，在 Linux 上区分，行为不统一，但开发者通常不怎么在意。</li><li><strong>PostgreSQL “坑”</strong>：PostgreSQL 默认会将所有<strong>未用双引号<code>&quot;&quot;</code>包围</strong>的表名、字段名等标识符<strong>自动转换为小写</strong>。<ul><li><strong>场景</strong>：你的 ORM 框架根据你的 Java&#x2F;Python 实体类 <code>MyTable</code> 自动生成了一张带双引号的表 <code>&quot;MyTable&quot;</code>。然后你手动去写一个报表 SQL：<code>SELECT * FROM MyTable;</code>。这个查询会<strong>失败</strong>！因为 PostgreSQL 会去找 <code>mytable</code> 这张表，而不是 <code>&quot;MyTable&quot;</code>。</li></ul></li><li><strong>开发者的转变</strong>：<ol><li><strong>最佳实践</strong>：在团队内建立规范，<strong>所有数据库对象（表、字段）全部使用小写+下划线命名</strong>（例如 <code>my_table</code>, <code>user_name</code>），这样就永远不会踩到大小写的坑。</li><li><strong>被迫的习惯</strong>：如果必须使用大小写混合的命名，那么在所有手写的 SQL 中，都必须用双引号把它们包围起来，例如 <code>SELECT &quot;userName&quot; FROM &quot;MyTable&quot;;</code>。</li></ol></li></ul><h3 id="3-从“应用层计算”到“数据库层计算”"><a href="#3-从“应用层计算”到“数据库层计算”" class="headerlink" title="3. 从“应用层计算”到“数据库层计算”"></a>3. 从“应用层计算”到“数据库层计算”</h3><ul><li><strong>MySQL 习惯</strong>：受限于相对较弱的内置函数和特性，很多逻辑开发者习惯于“把数据先捞出来，再在 Java&#x2F;Python 里循环处理”。</li><li><strong>PostgreSQL 思维转变</strong>：在写业务逻辑前，先问自己一句：<strong>“这个功能数据库能不能直接做？”</strong><ul><li><strong>场景：字符串拼接&#x2F;聚合</strong><ul><li><strong>MySQL 方式</strong>：<code>GROUP_CONCAT()</code> 功能有限。很多时候是把多行结果查出来，在代码里拼接。</li><li><strong>PostgreSQL 方式</strong>：<code>STRING_AGG(name, &#39;,&#39; ORDER BY id)</code>，功能更强大，可以直接在数据库里完成高质量的聚合。</li></ul></li><li><strong>场景：复杂数据分析</strong><ul><li><strong>MySQL 方式</strong>：需要写复杂的子查询，或者把数据导出到其他系统。</li><li><strong>PostgreSQL 方式</strong>：利用强大的窗口函数（Window Functions）、公共表表达式（CTE），可以直接在数据库里完成非常复杂的数据分析和排名。</li></ul></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从 MySQL 切换到 PostgreSQL，对于后端开发者的核心转变是：</p><p><strong>从把数据库当成一个相对“笨”的“数据仓库”，转变为把它当成一个聪明、严谨、功能强大的“数据处理与计算引擎”。</strong></p><p>这意味着，你需要：</p><ul><li>在<strong>设计时</strong>，更倾向于利用 PG 丰富的类型来保证数据模型的健壮性。</li><li>在<strong>编码时</strong>，编写更严谨、类型更安全的 SQL。</li><li>在<strong>实现复杂功能时</strong>，优先考虑是否能用 PG 的高级特性（JSONB、数组、窗口函数等）在数据库端高效地完成，而不是把大量原始数据拉到应用层进行低效处理。</li></ul><h2 id="场景设定：一个面向开发者的-PostgreSQL-智能知识库"><a href="#场景设定：一个面向开发者的-PostgreSQL-智能知识库" class="headerlink" title="场景设定：一个面向开发者的 PostgreSQL 智能知识库"></a>场景设定：一个面向开发者的 PostgreSQL 智能知识库</h2><ul><li><strong>知识库内容</strong>：包含了 PostgreSQL 官方文档 v16 的所有内容、大量关于性能优化的博客文章、以及知名 PG 专家的演讲稿。</li><li><strong>技术栈</strong>：<ul><li><strong>后端应用</strong>：一个 Java&#x2F;Python&#x2F;Node.js 应用。</li><li><strong>大语言模型 (LLM)</strong>：背后调用了某个强大的 LLM（如 GPT-4, Llama, Claude, Gemini 等）。</li><li><strong>数据库</strong>：<strong>PostgreSQL 16</strong>，并安装了 <strong><code>pgvector</code></strong> 扩展。</li></ul></li><li><strong>目标</strong>：当开发者用自然语言提问时，系统能结合知识库的内容，提供精准、可靠的回答，而不是仅仅依赖 LLM 本身可能过时或不准确的“记忆”。</li></ul><h3 id="第一步：准备工作（知识的向量化存储）"><a href="#第一步：准备工作（知识的向量化存储）" class="headerlink" title="第一步：准备工作（知识的向量化存储）"></a>第一步：准备工作（知识的向量化存储）</h3><p>这个过程是离线的、提前完成的。</p><ol><li><p><strong>数据分块 (Chunking)</strong>：我们将所有文档（HTML, PDF, TXT）切分成有意义的小段落（Chunks），比如每段不超过 500 个字符。</p></li><li><p><strong>向量化 (Embedding)</strong>：调用一个文本嵌入模型（如 OpenAI 的 <code>text-embedding-3-small</code>），将每一个文本块（Chunk）转换成一个向量（比如一个包含 1536 个浮点数的数组）。</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">-- 首先，确保扩展已创建CREATE EXTENSION IF NOT EXISTS vector;-- 创建知识库表CREATE TABLE knowledge_chunks (    id SERIAL PRIMARY KEY,    content TEXT NOT NULL,          -- 原始文本块    source TEXT,                    -- 来源，如：'doc_v16/indexing.html'    embedding VECTOR(1536)          -- 存储文本对应的向量，维度为1536);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>存入 PostgreSQL</strong>：我们在 PostgreSQL 中创建一个表：</p></li><li><p><strong>建立索引</strong>：为了加速向量搜索，我们创建一个 IVFFlat 索引。SQL</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">CREATE INDEX ON knowledge_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>现在，我们的 PostgreSQL 数据库里装满了成千上万条“知识片段”及其对应的“数学表示”（向量）。</p><hr><h3 id="第二步：模拟对话与幕后流程"><a href="#第二步：模拟对话与幕后流程" class="headerlink" title="第二步：模拟对话与幕后流程"></a>第二步：模拟对话与幕后流程</h3><p><strong>时间：</strong> 2025年7月31日，星期四</p><p><strong>用户 (一个开发者，小张)</strong>：打开了公司的内部技术支持聊天机器人。</p><p><strong>小张 输入</strong>：</p><blockquote><p>我的 JSONB 查询很慢，尤其是查内部的嵌套对象，我该怎么优化？B-Tree索引好像没用。</p></blockquote><p><strong>AI 助手 (背后是我们的 PG 知识库系统) 正在思考…</strong></p><h3 id="幕后技术流程拆解-RAG-Retrieval-Augmented-Generation"><a href="#幕后技术流程拆解-RAG-Retrieval-Augmented-Generation" class="headerlink" title="幕后技术流程拆解 (RAG - Retrieval-Augmented Generation)"></a>幕后技术流程拆解 (RAG - Retrieval-Augmented Generation)</h3><ol><li><p><strong>查询向量化</strong>：</p><ul><li>后端应用接收到小张的问题。</li><li>它立刻调用与准备阶段相同的文本嵌入模型，将问题 <strong>“我的 JSONB 查询很慢…”</strong> 这段文本也转换成一个 1536 维的<strong>查询向量 (Query Vector)</strong>。</li></ul></li><li><p><strong>在 PostgreSQL 中进行向量相似度搜索</strong>：</p><ul><li><p>后端应用拿着这个“查询向量”，对 <code>knowledge_chunks</code> 表执行一次 SQL 查询。</p></li><li><p>这个 SQL 查询是 <code>pgvector</code> 的核心功能：</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">-- <=> 是 pgvector 提供的余弦距离操作符-- 它会计算查询向量和表中所有向量的相似度SELECT content, sourceFROM knowledge_chunksORDER BY embedding <=> '[-0.123, 0.456, ...]' -- 这里是刚生成的查询向量LIMIT 5; -- 取回最相似的 5 个知识片段<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>PostgreSQL 利用我们之前创建的 IVFFlat 索引，<strong>毫秒级</strong>就返回了 5 个与问题最相关的文本块。可能包含以下内容：</p><ul><li><strong>[Context 1]</strong>: “For JSONB, the default B-tree operator class only supports equality checks… To index expressions within a JSONB document, the GIN operator class is recommended…” (来源: <code>doc_v16/datatype-json.html</code>)</li><li><strong>[Context 2]</strong>: “GIN indexes are a type of inverted index. For a composite type like JSONB, it creates index entries for each key and value within the document…” (来源: <code>doc_v16/indexes-types.html</code>)</li><li><strong>[Context 3]</strong>: “…a common performance pitfall is using standard indexes on JSONB columns for containment queries. A GIN index can yield orders of magnitude improvement…” (来源: <code>blog/top_10_pg_mistakes.txt</code>)</li><li>… (另外两个相关的片段)</li></ul></li></ul></li><li><p><strong>构建增强提示 (Augmented Prompt)</strong>：</p><ul><li><p>后端应用将上一步检索到的 5 个文本块和用户的原始问题，组合成一个发给大语言模型 (LLM) 的新提示 (Prompt)。这个 Prompt 看上去是这样的：</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">--- CONTEXT ---Context 1: For JSONB, the default B-tree operator class only supports equality checks... To index expressions within a JSONB document, the GIN operator class is recommended... (Source: doc_v16/datatype-json.html)Context 2: GIN indexes are a type of inverted index. For a composite type like JSONB, it creates index entries for each key and value within the document... (Source: doc_v16/indexes-types.html)Context 3: ...a common performance pitfall is using standard indexes on JSONB columns for containment queries. A GIN index can yield orders of magnitude improvement... (Source: blog/top_10_pg_mistakes.txt)...--- QUESTION ---我的 `JSONB` 查询很慢，尤其是查内部的嵌套对象，我该怎么优化？B-Tree索引好像没用。--- INSTRUCTION ---Based ONLY on the context provided above, answer the user's question. Cite your sources using the provided source names.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><strong>LLM 生成回答</strong>：</p><ul><li>LLM 接收到这个“有备而来”的提示。它不需要去自己的“记忆”里瞎猜，而是像一个<strong>开卷考试的学生</strong>，根据你给它的“参考资料”来组织答案。</li></ul></li></ol><hr><p><strong>AI 助手 回复</strong>：</p><blockquote><p>您好！您遇到的问题非常典型。根据 PostgreSQL 的文档和最佳实践，对于 JSONB 内部元素的查询，标准的 B-Tree 索引确实无法有效工作，因为它只支持对整个 JSONB 文档的等值判断。</p><p><strong>核心建议是：使用 GIN 索引。</strong></p><p><strong>原因如下：</strong></p><ul><li><p><strong>工作原理</strong>：GIN (Generalized Inverted Index) 是一种倒排索引，它能深入到 <code>JSONB</code> 文档内部，为其中的每一个键和值都创建索引条目。这使得包含性查询（比如查询某个键是否存在，或某个键的值是否为特定内容）的效率能得到数量级的提升 (来源: <code>blog/top_10_pg_mistakes.txt</code>, <code>doc_v16/indexes-types.html</code>)。</p></li><li><p><strong>如何创建</strong>：您可以像这样为您的 <code>JSONB</code> 字段创建一个 GIN 索引：</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">CREATE INDEX a_descriptive_index_name ON your_table USING GIN (your_jsonb_column);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>总结来说，将您表上的 B-Tree 索引更换为 GIN 索引，应该能显著解决您 <code>JSONB</code> 查询慢的问题 (来源: <code>doc_v16/datatype-json.html</code>)。</p></blockquote><p>这个回答既准确又可靠，还给出了引用来源，完美解决了小张的问题。这就是 PostgreSQL 在现代 AI 应用中充当高效、可靠的知识库后台的典型工作流程。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PostgreSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx笔记</title>
      <link href="/2024/09/18/nginx-bi-ji/"/>
      <url>/2024/09/18/nginx-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Nginx-复习"><a href="#Nginx-复习" class="headerlink" title="Nginx 复习"></a>Nginx 复习</h1><h1 id="Nginx-概述"><a href="#Nginx-概述" class="headerlink" title="Nginx 概述"></a>Nginx 概述</h1><p>Nginx（发音为”engine x”）是一个高性能的HTTP和反向代理服务器，也是一个IMAP&#x2F;POP3&#x2F;SMTP代理服务器。Nginx由Igor Sysoev于2004年首次公开发布，用于解决C10K问题（即服务器同时处理10,000个客户端连接的问题）。</p><ul><li><strong>HTTP 服务器</strong>：它可以像 Apache、Tomcat 一样，直接把服务器上的静态文件（比如 HTML、图片、CSS、JS）返回给用户的浏览器。</li><li><strong>反向代理服务器</strong>：这是它最重要、最常见的角色。它作为后端服务（比如你的 Java、Go、Python 应用）的“门户”，接收所有用户的请求，然后再把请求转发给后端的具体服务去处理。</li></ul><h3 id="Nginx-核心特性总结"><a href="#Nginx-核心特性总结" class="headerlink" title="Nginx 核心特性总结"></a><strong>Nginx 核心特性总结</strong></h3><p><strong>1. 高并发、高性能 (High Concurrency &amp; High Performance)</strong></p><ul><li>这是 Nginx 最广为人知的标签。它能以极低的内存和 CPU 资源消耗，轻松维持数万甚至数十万级别的并发连接。</li><li><strong>核心原因</strong>：它从根本上解决了 <strong>C10K 问题</strong>，是为高并发场景而生的。</li></ul><p><strong>2. 事件驱动的异步非阻塞架构 (Event-Driven, Asynchronous Non-Blocking)</strong></p><ul><li>这是实现“高并发、高性能”的<strong>技术基石</strong>。</li><li><strong>工作方式</strong>：Nginx 在处理网络连接和请求时，不会因为某个任务（如等待客户端发送数据或等待后端返回结果）而“傻等”（阻塞）。它会注册一个“事件”，然后立即去处理其他任务。当等待的任务完成后，它会接收到通知，再回来处理后续步骤。</li><li><strong>结果</strong>：用极少数的工作进程就能“应付”海量的并发任务，大大节省了系统资源。</li></ul><p><strong>3. 强大的代理与网关层 (Robust Proxy and Gateway Layer)</strong></p><ul><li>Nginx 不仅仅是简单的请求转发，它是一个功能丰富的<strong>应用网关</strong>。</li><li><strong>体现在</strong>：<ul><li><strong>反向代理</strong>：隐藏和保护后端服务。</li><li><strong>负载均衡</strong>：将请求压力分摊到多个后端服务器。</li><li><strong>安全屏障</strong>：可配置访问控制、速率限制，抵御部分网络攻击。</li><li><strong>性能优化</strong>：能执行 SSL 加密卸载、内容缓存、Gzip 压缩等，为后端减负。</li></ul></li></ul><p><strong>4. 明确的职责分离 (Clear Separation of Concerns)</strong></p><ul><li>Nginx 完美地将 <strong>网络连接管理</strong> 与 <strong>业务逻辑处理</strong> 这两个职责分离开。</li><li><strong>职责划分</strong>：<ul><li><strong>Nginx</strong>：专心处理所有与客户端之间的<strong>网络 I&#x2F;O</strong>，无论连接是快是慢。</li><li><strong>后端服务</strong>：从繁杂的网络问题中解脱出来，专心处理<strong>业务逻辑</strong>和<strong>数据 I&#x2F;O</strong>（如数据库读写）。</li></ul></li><li><strong>结果</strong>：架构更清晰，使得各个部分可以独立优化和扩展。</li></ul><p><strong>5. 忠实的“问题报告者” (Faithful Problem Reporter)</strong></p><ul><li>当后端服务出现问题时，Nginx 不会隐藏问题，而是会明确地向客户端报告。</li><li><strong>典型例子</strong>：<ul><li><strong><code>502 Bad Gateway</code></strong>：后端服务不可达或已崩溃。</li><li><strong><code>504 Gateway Timeout</code></strong>：后端服务处理请求太慢，超时了。</li></ul></li></ul><h2 id="Nginx-的常见应用场景"><a href="#Nginx-的常见应用场景" class="headerlink" title="Nginx 的常见应用场景"></a>Nginx 的常见应用场景</h2><ul><li>**静态资源服务器：**高效地提供静态文件如HTML、CSS、JavaScript、图片等。</li><li>**反向代理服务器：**接收客户端请求并转发到后端服务器。</li><li>**负载均衡器：**分发请求到多个后端服务器，提高系统整体性能和可靠性。</li><li>**HTTP缓存：**缓存静态内容，减少后端服务器的负载。</li><li>**API网关：**作为API请求的入口点，处理路由、认证、限流等功能。</li></ul><h2 id="与其他Web服务器的比较"><a href="#与其他Web服务器的比较" class="headerlink" title="与其他Web服务器的比较"></a>与其他Web服务器的比较</h2><p>相比Apache等传统Web服务器，Nginx在处理高并发连接方面表现更佳，资源消耗更低。与此同时，Nginx的配置相对简单，学习曲线较平缓。</p><h2 id="Nginx-的工作模型"><a href="#Nginx-的工作模型" class="headerlink" title="Nginx 的工作模型"></a>Nginx 的工作模型</h2><p>Nginx 的高性能秘密，就藏在它独特的工作模型里。这个模型可以拆分成两个层面来理解：</p><ul><li><strong>宏观的进程架构</strong>：它是如何组织自己的进程的？(Master-Worker 模型)</li><li><strong>微观的工作方式</strong>：每个进程是如何处理海量连接的？(I&#x2F;O 多路复用)</li></ul><h3 id="Master-Worker-多进程模型"><a href="#Master-Worker-多进程模型" class="headerlink" title="Master-Worker 多进程模型"></a><strong>Master-Worker 多进程模型</strong></h3><p>当你在服务器上启动 Nginx 后，用 <code>ps -ef | grep nginx</code> 命令查看，通常会看到至少两个进程：一个 <strong>Master 进程</strong>和一个或多个 <strong>Worker 进程</strong>。</p><p><strong>1. Master 进程 (主进程 &#x2F; “管理者”)</strong></p><p>Master 进程的角色是“领导”和“管理员”，它<strong>不直接处理任何用户的网络请求</strong>。它的主要工作是：</p><ul><li><strong>读取并验证配置</strong>：启动时，它会读取 <code>nginx.conf</code> 配置文件，检查语法是否正确。</li><li><strong>创建和绑定端口</strong>：它会负责监听配置文件中指定的端口（如 80, 443）。</li><li><strong>管理 Worker 进程</strong>：<ul><li>根据配置（<code>worker_processes</code> 指令）或者服务器的 CPU核心数，创建指定数量的 Worker 进程。</li><li>监控 Worker 进程的健康状态，如果某个 Worker 意外死掉了，Master 会立刻重新拉起一个新的。</li><li>接收来自管理员的控制信号，实现 Nginx 的平滑升级、优雅重启、停止服务等。例如，当你执行 <code>nginx -s reload</code> 来热加载配置时，就是 Master 进程在工作。</li></ul></li></ul><p><strong>简单说，Master 进程就像一个“工头”，负责准备工作环境、招募并管理工人，但它自己不上一线干活。</strong></p><p><strong>2. Worker 进程 (工作进程 &#x2F; “工人”)</strong></p><p>Worker 进程才是真正处理用户请求的“一线员工”。</p><ul><li><strong>处理连接和请求</strong>：所有的用户连接和请求，都是由 Worker 进程来接收、处理和响应的。</li><li><strong>相互独立</strong>：每个 Worker 进程都是一个独立的、单线程的进程。它们之间几乎没有共享数据，处理请求时互不干扰，这保证了高可靠性，一个 Worker 的崩溃不会影响其他 Worker。</li><li><strong>共享监听端口</strong>：所有 Worker 进程都会共同监听由 Master 进程打开的端口。当一个新连接到来时，所有 Worker 会通过操作系统的机制（如 <code>accept_mutex</code> 锁）来“争抢”这个连接，最终只有一个 Worker 会成功接收并处理它。</li></ul><p><strong>这个 “一个管理者 + 多个高效工人” 的模型有几个巨大的好处：</strong></p><ul><li><strong>稳定性</strong>：Master 作为守护进程，保证了 Worker 的稳定运行。Worker 之间相互隔离，一个出问题不影响全局。</li><li><strong>高可用性</strong>：可以在不停止服务的情况下，通过向 Master 发送信号来平滑地升级程序、更新配置。</li><li><strong>利用多核</strong>：可以配置与 CPU 核心数相等的 Worker 进程数，充分利用多核 CPU 的处理能力。</li></ul><hr><p><strong>每个 Worker 进程都是单线程的，它凭什么能独自处理成千上万的并发连接呢？</strong></p><h3 id="I-O-多路复用-以-epoll-为例"><a href="#I-O-多路复用-以-epoll-为例" class="headerlink" title="I&#x2F;O 多路复用 (以 epoll 为例)"></a>I&#x2F;O 多路复用 (以 epoll 为例)</h3><p>Worker 进程高效的秘密武器，就是 <strong>I&#x2F;O 多路复用 (I&#x2F;O Multiplexing)</strong>。在 Linux 系统上，它具体的技术实现就是 **epoll，**应用程序可以通过调用 <code>epoll_create</code>, <code>epoll_ctl</code>, <code>epoll_wait</code> 这几个函数，来委托内核去帮它高效地管理大量的网络连接。</p><ol><li>一个 Worker 进程把自己关心的所有连接（Socket）都注册到 <code>epoll</code> 系统中。</li><li>然后 Worker 进程就休眠了，调用 <code>epoll_wait()</code> 等待事件发生，几乎不占 CPU。</li><li>当某个连接上有数据传来、或可以发送数据时（即 I&#x2F;O 事件就绪），操作系统会通知 Worker 进程，并把所有<strong>已就绪</strong>的连接列表告诉它。</li><li>Worker 进程被唤醒，拿到这个“就绪列表”，然后依次处理这些连接上的请求。处理完后，继续回去“睡觉”，等待下一次的事件通知。</li></ol><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h3><p>Nginx 的高性能模型 &#x3D; <strong>Master-Worker 多进程架构</strong> (充分利用多核、稳定可靠) + <strong>I&#x2F;O 多路复用 <code>epoll</code></strong> (单个 Worker 进程高效处理海量连接的核心技术)。</p><h2 id="Nginx-的配置与核心指令"><a href="#Nginx-的配置与核心指令" class="headerlink" title="Nginx 的配置与核心指令"></a>Nginx 的配置与核心指令</h2><h3 id="Nginx-的安装与目录结构"><a href="#Nginx-的安装与目录结构" class="headerlink" title="Nginx 的安装与目录结构"></a><strong>Nginx 的安装与目录结构</strong></h3><p>通常，我们通过包管理器（如 <code>yum</code> 或 <code>apt-get</code>）安装 Nginx。</p><p>记住以下几个核心位置至关重要：</p><ul><li><strong>主配置文件</strong>: <code>/etc/nginx/nginx.conf</code><ul><li>这是 Nginx 的“大脑”，几乎所有的配置都从这里开始。</li></ul></li><li><strong>可拆分的配置目录</strong>: <code>/etc/nginx/conf.d/</code><ul><li>在实践中，我们很少把所有配置都写在 <code>nginx.conf</code> 这一个文件里。通常，<code>nginx.conf</code> 会通过 <code>include /etc/nginx/conf.d/*.conf;</code> 这样的指令，把这个目录下的所有 <code>.conf</code> 文件都加载进来。</li><li><strong>最佳实践</strong>：每为一个网站或一个服务做代理，就在 <code>conf.d</code> 目录下新建一个独立的配置文件（如 <code>my_app.conf</code>）。这让配置管理变得非常清晰。</li></ul></li><li><strong>日志文件目录</strong>: <code>/var/log/nginx/</code><ul><li><strong><code>access.log</code></strong>: 访问日志。记录了每一个请求的详细信息（谁访问的、访问了什么、何时访问、响应状态等）。是排查业务问题和做数据分析的基础。</li><li><strong><code>error.log</code></strong>: 错误日志。记录了 Nginx 启动、运行过程中的所有错误信息。<strong>当 Nginx 启动失败或者出现 5xx 错误时，第一时间就应该来这里找线索。</strong></li></ul></li><li><strong>默认网站根目录</strong>: <code>/usr/share/nginx/html/</code><ul><li>当你安装完 Nginx 第一次启动，在浏览器里访问服务器 IP，看到的那个“Welcome to Nginx!”页面，就是存放在这个目录下的 <code>index.html</code>。</li></ul></li></ul><h3 id="Nginx-的基本控制命令"><a href="#Nginx-的基本控制命令" class="headerlink" title="Nginx 的基本控制命令"></a><strong>Nginx 的基本控制命令</strong></h3><p>我们通常使用 <code>nginx</code> 这个可执行文件（或者通过 <code>systemctl</code>），配合不同的参数来控制它。</p><ul><li><strong>测试配置</strong>：<code>nginx -t</code><ul><li><strong>极其重要！</strong> 在你修改了任何 <code>.conf</code> 配置文件后，<strong>永远不要直接重启 Nginx</strong>。先执行 <code>nginx -t</code>，它会检查所有配置文件的语法。如果没问题，它会提示 <code>syntax is ok</code> 和 <code>test is successful</code>。这能避免因为一个小小的语法错误（比如少个分号）导致整个网站宕机。</li></ul></li><li><strong>启动 Nginx</strong>：<code>nginx</code> 或 <code>systemctl start nginx</code><ul><li>直接执行 <code>nginx</code> 命令（或使用 <code>systemctl</code>）。</li></ul></li><li><strong>停止 Nginx</strong>：<ul><li><code>nginx -s stop</code>：快速停止，不管当前有无正在处理的请求，直接“一刀切”。</li><li><code>nginx -s quit</code>：优雅停止 (graceful stop)，会等待当前所有正在处理的请求完成后，再关闭进程。</li></ul></li><li><strong>重载配置 (热加载)</strong>：<code>nginx -s reload</code><ul><li><strong>核心功能！</strong> 这也是 Master-Worker 模型优势的体现。当执行 <code>reload</code> 时：<ol><li>Master 进程会先去检查配置（类似 <code>nginx -t</code>）。</li><li>如果配置无误，Master 会启动<strong>新的 Worker 进程</strong>（使用新配置）。</li><li>同时，Master 会向<strong>老的 Worker 进程</strong>发送“优雅停止”的信号。</li><li>老的 Worker 进程不再接受新的连接，并在处理完手头所有请求后自行退出。</li></ol></li><li>整个过程<strong>服务不中断</strong>，用户的访问完全无感知，这是生产环境中更新配置的标准操作。</li></ul></li></ul><h3 id="Nginx-核心指令解析"><a href="#Nginx-核心指令解析" class="headerlink" title="Nginx 核心指令解析"></a><strong>Nginx 核心指令解析</strong></h3><pre class="line-numbers language-language-json"><code class="language-language-json">#================ 全局块 (Global Block) ================# 这部分没有花括号，是最高层级的指令。user  nginx;worker_processes  auto; # 通常设置为 auto 或 CPU 核心数error_log /var/log/nginx/error.log warn;#====================================================#================ events 块 ================# 配置与网络连接相关的参数events &#123;    worker_connections  1024; # 每个 worker 进程能处理的最大连接数    use epoll; # 在 Linux 上使用 epoll 模型，性能最高&#125;#===========================================#================ http 块 ================# 这是配置 Web 服务和代理功能最核心的块http &#123;    # --- http 全局配置 ---    # 定义了可以在 http 块下所有 server 块中使用的公共配置    include       /etc/nginx/mime.types;    default_type  application/octet-stream;    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '                      '$status $body_bytes_sent "$http_referer" '                      '"$http_user_agent" "$http_x_forwarded_for"';    access_log  /var/log/nginx/access.log  main;    sendfile        on;    keepalive_timeout  65;    # --- server 块 (定义一个虚拟主机) ---    server &#123;        listen       80; # 监听 80 端口        server_name  www.example.com; # 对应的域名        # --- location 块 (定义请求路由规则) ---        location / &#123;            root   /usr/share/nginx/html; # 网站根目录            index  index.html index.htm;   # 默认首页文件        &#125;        location /api/ &#123;            # 这里的规则会匹配所有 "www.example.com/api/..." 的请求            # 比如，可以把请求转发给后端服务            proxy_pass http://backend_server_address;        &#125;    &#125;    # --- 可以定义第二个 server 块 ---    server &#123;        listen 80;        server_name another.example.com;        # ... 针对 another.example.com 的配置 ...    &#125;&#125;#===========================================<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>全局块 (Global Block)</strong></p><ul><li><code>*user**</code><br>指定 Nginx 的 Worker 进程以哪个用户和用户组的身份运行。出于安全考虑，通常会指定为一个权限较低的专用用户（如 <code>nginx</code> 或 <code>www-data</code>）。</li><li><code>*worker_processes**</code><br>设置 Worker 进程的数量。这是性能调优的关键，最佳实践是设置为 <code>auto</code>，Nginx 会自动检测服务器的 CPU 核心数并以此作为 Worker 数量。</li><li><code>*error_log**</code><br>定义全局的错误日志文件路径和记录级别。当 Nginx 遇到问题时（如配置错误、启动失败），这里是寻找线索的首要地点。</li><li><code>*pid**</code><br>指定存放 Master 进程 ID 的文件路径。这个文件主要用于脚本自动化控制 Nginx。</li></ul><p><strong><code>events</code> 块</strong></p><ul><li><code>*worker_connections**</code><br>设置<strong>每一个</strong> Worker 进程能够同时处理的最大连接数。这是 Nginx 并发能力的核心指标。服务器的总并发能力理论上等于 <code>worker_processes * worker_connections</code>。</li><li><code>*use**</code><br>指定 Nginx 使用的 I&#x2F;O 多路复用模型。如我们所讨论的，在 Linux 上 Nginx 会自动选择最高效的 <code>epoll</code>，所以此项一般无需配置。</li></ul><p><strong><code>http</code> 块</strong></p><ul><li><code>*include**</code><br>用于引入其他的配置文件，是保持主配置文件 <code>nginx.conf</code> 简洁的最佳实践。<code>include /etc/nginx/conf.d/*.conf;</code> 就是一个典型的例子。<code>mime.types</code> 这个文件里定义了文件扩展名和MIME类型（Content-Type）之间的对应关系。比如，它里面有这样的内容：<code>text/html html;</code>、<code>image/jpeg jpg jpeg;</code>。浏览器需要根据服务器返回的 <code>Content-Type</code> 头来决定如何展示一个文件（是当成网页渲染，还是当成图片显示）。</li><li><code>*access_log**</code><br>设置访问日志的存放路径和使用的格式。可以设置 <code>off</code> 来关闭日志记录。</li><li><code>*log_format**</code><br>自定义访问日志的格式。可以添加更多变量（如 <code>$request_time</code>, <code>$upstream_response_time</code>）来记录请求耗时等信息，对于性能分析非常有用。</li><li><code>*sendfile**</code><br>一个性能优化指令。设置为 <code>on</code> 会启用操作系统高效的文件传输模式（零拷贝），可以让 Nginx 在传输静态文件时更快、更节省资源。</li><li><code>*keepalive_timeout**</code><br>设置 HTTP 长连接的超时时间。合理的设置可以减少客户端与服务器之间频繁建立和断开 TCP 连接的开销，提升性能。</li><li><code>*client_max_body_size**</code><br>限制客户端请求体（Request Body）的最大尺寸。当用户需要上传文件时，如果文件大小超过这个值，Nginx 会返回 <code>413 Request Entity Too Large</code> 错误。这是控制资源滥用的重要指令。</li></ul><p><strong><code>server</code> 块</strong></p><ul><li><code>*listen**</code><br>指定虚拟主机监听的端口号。可以附加 <code>ssl</code> 参数来表示此端口用于 HTTPS 通信。例如：<code>listen 443 ssl;</code>。</li><li><code>*server_name**</code><br>指定虚拟主机的域名。Nginx 通过请求头中的 <code>Host</code> 字段来匹配对应的 <code>server_name</code>，从而决定由哪个 <code>server</code> 块来处理该请求。这是实现虚拟主机的关键。</li></ul><p><strong><code>location</code> 块</strong></p><p>这是 Nginx 配置中最核心、最灵活的部分，决定了如何处理具体的请求 URI。</p><ul><li><p><code>*root**</code><br>指定静态文件的根目录。当请求匹配到这个 <code>location</code> 时，Nginx 会将请求的 URI 拼接到 <code>root</code> 指定的路径后，去硬盘上寻找对应的文件。</p><pre class="line-numbers language-language-json"><code class="language-language-json">server &#123;    listen 80;    server_name example.com;    # 示例: 访问 http://example.com/images/logo.png    location /images/ &#123;        root /var/www/app;         # Nginx 寻找的物理路径是：        # /var/www/app   +   /images/logo.png        # 结果: /var/www/app/images/logo.png    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>*index**</code><br>当请求的 URI 是一个目录时，指定默认去查找的文件名。例如，访问 <code>http://example.com/</code> 时，Nginx 会依次尝试查找 <code>root</code> 目录下的 <code>index.html</code> 和 <code>index.htm</code> 文件。</p><ul><li><strong>访问 <code>http://example.com/</code></strong>：Nginx 会先寻找 <code>/var/www/app/index.html</code>，如果找不到，再寻找 <code>/var/www/app/index.php</code>。</li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">location / &#123;    root /var/www/app;    index index.html index.php; # 优先找 index.html, 找不到再找 index.php&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>*proxy_pass**</code><strong>反向代理的核心指令</strong>。它指定了要将请求转发到的后端服务器地址。地址可以是 IP:Port、域名，或者是一个通过 <code>upstream</code> 块定义的服务器组。</p><ul><li><strong>访问 <code>http://example.com/api/users</code></strong>：</li><li><strong>注意</strong>：<code>proxy_pass</code> 后面地址的 <code>/</code> 非常关键：<ul><li>带 <code>/</code> (<code>http://.../</code>)：表示将原始 URI 中匹配 <code>location</code> 的部分（<code>/api/</code>）去掉后再拼接；Nginx 会将请求转发到 <code>http://127.0.0.1:8080/users</code>。</li><li>不带 <code>/</code> (<code>http://...</code>)：表示将原始 URI 原封不动地拼接到后面；Nginx 会将请求转发到 <code>http://127.0.0.1:8080/api/users</code>。</li></ul></li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">location /api/ &#123;    # 将所有 /api/ 开头的请求转发给运行在 8080 端口的后端应用    proxy_pass http://127.0.0.1:8080/;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>*try_files**</code><br>按顺序检查指定的文件或目录是否存在。如果存在，就返回该文件；如果都不存在，就执行最后一个参数指定的操作（通常是内部重定向）。在配置单页应用（SPA）的路由时非常有用，例如 <code>try_files $uri $uri/ /index.html;</code>。</p><ul><li><strong>访问 <code>http://example.com/user/profile</code></strong>：<ol><li><code>$uri</code>：Nginx 尝试寻找文件 <code>/var/www/spa-app/user/profile</code>。<strong>失败</strong>。</li><li><code>$uri/</code>：Nginx 尝试寻找目录 <code>/var/www/spa-app/user/profile/</code> 下的 <code>index</code> 文件（即 <code>index.html</code>）。<strong>失败</strong>。</li><li><code>/index.html</code>：前面都失败后，Nginx 会发起一个内部重定向，去请求并返回 <code>/index.html</code>。</li></ol></li><li><strong>效果</strong>：无论用户访问哪个路径，最终都会返回 <code>/index.html</code>，浏览器 URL 保持不变，后续的路由交由前端 JS 来处理。</li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">location / &#123;    root /var/www/spa-app;    index index.html;    try_files $uri $uri/ /index.html;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>*alias**</code><br>与 <code>root</code> 类似，也是用来指定文件路径，但它的路径拼接行为不同。<code>alias</code> 会用指定的路径<strong>替换</strong>掉 <code>location</code> 匹配到的 URI 部分。</p><ul><li><strong><code>root</code> vs <code>alias</code> 关键区别</strong>：<code>root</code> 是拼接完整 URI，<code>alias</code> 是替换 location URI。使用 <code>alias</code> 时，路径末尾的 <code>/</code> 通常很重要。</li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">server &#123;    # 示例: 访问 http://example.com/assets/logo.png    # 注意，/assets/ 目录可能在项目目录之外    location /assets/ &#123;        alias /var/www/other_static_files/;        # Nginx 寻找的物理路径是：        # /var/www/other_static_files/   +   logo.png (注意 /assets/ 被替换掉了)        # 结果: /var/www/other_static_files/logo.png    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="Nginx核心功能三大支柱"><a href="#Nginx核心功能三大支柱" class="headerlink" title="Nginx核心功能三大支柱"></a>Nginx核心功能三大支柱</h2><h3 id="1-静态资源服务器-Static-Resource-Server"><a href="#1-静态资源服务器-Static-Resource-Server" class="headerlink" title="1. 静态资源服务器 (Static Resource Server)"></a>1. 静态资源服务器 (Static Resource Server)</h3><p>虽然我们现在更多地将 Nginx 用作反向代理，但它最初就是作为一个超高性能的 Web 服务器而设计的，其处理静态文件（图片、CSS、JS、HTML文件等）的能力至今仍然是业界标杆。</p><p>我们之前已经知道，通过 <code>root</code> 和 <code>index</code> 指令就可以让它工作起来。但面试中，面试官更想知道的是：</p><ol><li><strong>为什么 Nginx 处理静态文件那么快？</strong></li><li><strong>有哪些优化的手段？</strong></li></ol><hr><p><strong>A. Nginx 高性能的背后原理</strong></p><p>Nginx 的性能优势源于它的架构设计，主要体现在两点：</p><ul><li><strong>I&#x2F;O 多路复用 (<code>epoll</code>)</strong>: 我们在第一站已经深入探讨过。它让 Nginx 能用极少的进程处理海量的并发连接，避免了为每个连接创建新进程&#x2F;线程的巨大开销。</li><li><strong>sendfile 系统调用</strong>: 这是一个关键的性能“黑科技”。<ul><li><strong>传统方式</strong>：一个程序（如 Web 服务器）想把一个文件发送出去，需要先把文件内容从硬盘读到<strong>内核缓冲区</strong>，然后再从内核缓冲区复制到<strong>应用程序的内存</strong>中，最后再从应用程序内存复制回<strong>内核的 Socket 缓冲区</strong>，才能通过网卡发送。这个过程有两次不必要的数据拷贝，浪费 CPU 和内存。</li><li><strong>sendfile 方式</strong>：Nginx 使用 <code>sendfile</code> 系统调用，它告诉内核：“请把这个文件的内容直接从你的内核缓冲区发送到那个网络连接的 Socket 缓冲区去”。数据完全在<strong>内核空间</strong>内进行传输，不需要拷贝到应用程序（Nginx）的内存中。这个过程被称为**“零拷贝” (Zero-copy)**，极大地提升了文件传输效率，降低了 CPU 消耗。</li></ul></li></ul><p>正是因为 <code>epoll</code> + <code>sendfile</code> 这对组合拳，Nginx 才成为了处理静态文件的王者。</p><hr><p><strong>B. 静态资源服务的优化配置</strong></p><p>让 Nginx 工作起来很简单，但要把它配置到最优，还需要一些额外的指令。这些优化手段主要围绕两个目标：<strong>减轻服务器压力</strong>和<strong>加速客户端访问</strong>。</p><p><strong>开启高效传输模式</strong></p><p>这些指令通常放在 <code>http</code> 块中，它们能进一步发挥 Nginx 和操作系统的性能。</p><pre class="line-numbers language-language-json"><code class="language-language-json">http &#123;    # 启用 sendfile "零拷贝" 模式    sendfile on;    # 在 sendfile 开启的情况下，优化网络包的发送。    # 它会将响应头和文件内容一次性发送出去，而不是先发头再发内容。    tcp_nopush on;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-配置浏览器缓存-expires"><a href="#2-配置浏览器缓存-expires" class="headerlink" title="2. 配置浏览器缓存 (expires)"></a><strong>2. 配置浏览器缓存 (<code>expires</code>)</strong></h3><p>这是<strong>最重要</strong>的静态资源优化手段。它通过设置 HTTP 响应头，告诉浏览器这个文件可以在本地缓存多久，下次请求时就无需再从服务器下载了。</p><pre class="line-numbers language-language-json"><code class="language-language-json"># 匹配所有图片、CSS、JS 等静态资源location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ &#123;    root /path/to/your/assets;        # 设置缓存过期时间为 30 天    expires 30d; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>对于不常变化、且文件名唯一的静态资源（如带 hash 的 JS 文件 <code>main.a4b8c1.js</code>），可以设置非常长的缓存时间，如 <code>expires 1y;</code> (一年)。</li><li>对于像 <code>index.html</code> 这样可能随时更新的文件，通常不设置强缓存，或者设置为 <code>expires -1;</code>，让浏览器每次都来服务器验证。</li></ul><h3 id="3-开启-Gzip-压缩"><a href="#3-开启-Gzip-压缩" class="headerlink" title="3. 开启 Gzip 压缩"></a><strong>3. 开启 Gzip 压缩</strong></h3><p>对于文本类文件（HTML, CSS, JS, JSON），在传输前进行压缩，可以极大地减小文件体积，加快传输速度。图片（JPG&#x2F;PNG）和视频本身已经是压缩格式，不需要再用 Gzip。</p><pre class="line-numbers language-language-json"><code class="language-language-json">http &#123;    # 开启 Gzip    gzip on;        # 指定哪些类型的文件需要被压缩    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;    # 其他优化，比如设置最小压缩文件大小，避免压缩过小的文件    gzip_min_length 1k;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>gzip</code> 指令通常放在 <code>http</code> 块中，对所有网站生效。</p><p><strong>总结一下</strong>：</p><p>一个优化良好的静态资源服务器配置，会充分利用 Nginx 的 <code>sendfile</code> 机制，并通过 <code>expires</code> 和 <code>gzip</code> 指令，最大化地减少网络传输量和请求次数。</p><h2 id="反向代理与负载均衡"><a href="#反向代理与负载均衡" class="headerlink" title="反向代理与负载均衡"></a>反向代理与负载均衡</h2><h3 id="A-负载均衡-Load-Balancing"><a href="#A-负载均衡-Load-Balancing" class="headerlink" title="A. 负载均衡 (Load Balancing)"></a><strong>A. 负载均衡 (Load Balancing)</strong></h3><p><strong>是什么？</strong><br>负载均衡是将网络请求<strong>分发</strong>到<strong>多个后端服务器</strong>上去处理的机制。它是构建高可用、可扩展服务的基石。</p><p><strong>为什么？</strong></p><ol><li><strong>分担压力 (Scalability)</strong>：单台服务器性能有上限，通过增加服务器数量来线性提升处理能力。</li><li><strong>提高可用性 (High Availability)</strong>：当某台后端服务器宕机时，Nginx 会自动将其从分发列表中剔除，将请求发往其他健康的服务器，保证服务不中断。</li></ol><p><strong>怎么配？</strong><br>通过 <code>upstream</code> 和 <code>proxy_pass</code> 两个指令配合完成。</p><ol><li><strong><code>upstream</code> 块</strong>：用来定义一组后端服务器。这个块必须写在 <code>http</code> 块中。</li><li><strong><code>proxy_pass</code></strong>：在 <code>location</code> 块中，将其指向 <code>upstream</code> 定义的组名。</li></ol><p><strong>示例：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json">http &#123;    # 1. 定义一个名为 "backend_servers" 的后端服务器集群    upstream backend_servers &#123;        server 192.168.1.100:8080; # 服务器1        server 192.168.1.101:8080; # 服务器2    &#125;    server &#123;        listen 80;        server_name example.com;        location / &#123;            # 2. 将所有请求转发到 "backend_servers" 集群            proxy_pass http://backend_servers;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="B-负载均衡核心策略"><a href="#B-负载均衡核心策略" class="headerlink" title="B. 负载均衡核心策略"></a><strong>B. 负载均衡核心策略</strong></h3><p>Nginx 提供了多种策略来决定请求到底该发给哪台服务器。</p><ol><li><strong>轮询 (Round Robin)</strong> - <strong>默认策略</strong><ul><li><strong>行为</strong>：按顺序，一人一次。第一个请求给服务器1，第二个给服务器2，第三个再给服务器1…</li><li><strong>配置</strong>：无需任何额外配置。</li></ul></li><li><strong>权重 (Weight)</strong><ul><li><strong>行为</strong>：根据指定的权重比例来分配请求。权重越高的服务器，被分配到的请求就越多。适用于服务器性能不一的场景。</li><li><strong>配置</strong>：<code>server 192.168.1.100:8080 weight=3;</code></li></ul></li><li><strong>IP 哈希 (ip_hash)</strong><ul><li><strong>行为</strong>：根据客户端的 IP 地址进行哈希计算，确保<strong>同一个客户端</strong>的请求，总是被转发到<strong>同一台后端服务器</strong>。</li><li><strong>为什么</strong>：解决 Session 共享问题。如果用户在服务器A登录了，下一个请求被分到了服务器B，服务器B没有它的登录信息，就会导致用户需要重新登录。<code>ip_hash</code> 可以避免这种情况。</li><li><strong>配置</strong>：在 <code>upstream</code> 块里加上 <code>ip_hash;</code> 即可。</li></ul></li><li><strong>最少连接 (least_conn)</strong><ul><li><strong>行为</strong>：将请求优先分配给当前活跃连接数最少的后端服务器。适用于请求处理时间长短不一的场景，能者多劳。</li><li><strong>配置</strong>：在 <code>upstream</code> 块里加上 <code>least_conn;</code>。</li></ul></li></ol><h3 id="C-健康检查"><a href="#C-健康检查" class="headerlink" title="C. 健康检查"></a><strong>C. 健康检查</strong></h3><p><code>upstream</code> 块可以配置简单的健康检查机制。</p><ul><li><code>max_fails=3</code>：在 <code>fail_timeout</code> 时间内，请求后端失败达到3次，则认为该服务器宕机。</li><li><code>fail_timeout=10s</code>：暂停向该服务器转发请求10秒，10秒后会再次尝试。</li></ul><h2 id="进阶与优化专题"><a href="#进阶与优化专题" class="headerlink" title="进阶与优化专题"></a>进阶与优化专题</h2><h3 id="A-HTTPS-配置"><a href="#A-HTTPS-配置" class="headerlink" title="A. HTTPS 配置"></a><strong>A. HTTPS 配置</strong></h3><p><strong>是什么</strong>：为网站启用 SSL&#x2F;TLS 加密，使 HTTP 协议变为更安全的 HTTPS。<br><strong>怎么配</strong>：</p><ol><li>准备好你的域名证书（<code>.crt</code> 文件）和私钥（<code>.key</code> 文件）。</li><li>修改 <code>server</code> 块的配置。</li></ol><p><strong>示例</strong>：</p><pre class="line-numbers language-language-json"><code class="language-language-json">server &#123;    # 1. 监听 443 端口，并开启 ssl    listen 443 ssl http2; # http2 可以顺便开启，提升性能    server_name example.com;    # 2. 指定证书和私钥的路径    ssl_certificate /path/to/your/certificate.crt;    ssl_certificate_key /path/to/your/private.key;    # 可选：配置 SSL 协议和加密套件，增强安全性    ssl_protocols TLSv1.2 TLSv1.3;    ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:...';    # ... 其他 location 配置 ...&#125;# 可选但强烈推荐：将所有 HTTP 请求强制重定向到 HTTPSserver &#123;    listen 80;    server_name example.com;    return 301 https://$host$request_uri;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="B-Nginx-缓存"><a href="#B-Nginx-缓存" class="headerlink" title="B. Nginx 缓存"></a><strong>B. Nginx 缓存</strong></h3><p><strong>是什么</strong>：将后端服务器返回的内容在 Nginx 上缓存一份。对于不常变化的内容，Nginx 可以直接从缓存中返回，不必再请求后端，极大减轻后端压力。<br><strong>怎么配</strong>：</p><ol><li><strong><code>proxy_cache_path</code></strong>: 在 <code>http</code> 块中，定义一块缓存区域的物理路径和规则。</li><li><strong><code>proxy_cache</code></strong>: 在 <code>location</code> 块中，启用上面定义好的缓存。</li></ol><p><strong>示例</strong>：</p><pre class="line-numbers language-language-json"><code class="language-language-json">http &#123;    # 1. 定义一个名为 "my_cache" 的缓存区    # 路径在 /var/nginx/cache, 占用100m磁盘空间, 1天内未被访问则自动清除    proxy_cache_path /var/nginx/cache levels=1:2 keys_zone=my_cache:10m max_size=100m inactive=1d;    server &#123;        # ...        location /some/path/ &#123;            # 2. 在这个 location 启用名为 "my_cache" 的缓存            proxy_cache my_cache;            proxy_pass http://backend_servers;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="C-跨域问题处理（CORS）"><a href="#C-跨域问题处理（CORS）" class="headerlink" title="C. 跨域问题处理（CORS）"></a><strong>C. 跨域问题处理（CORS）</strong></h3><p>当前端应用和后端 API 不在同一个域名下时，浏览器会出于安全策略阻止请求。可以通过 Nginx 在响应中添加特定的头信息来解决。</p><pre class="line-numbers language-language-json"><code class="language-language-json">location /api/ &#123;    # 允许来自任何源的请求    add_header 'Access-Control-Allow-Origin' '*';    # 允许的请求方法    add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';    # 允许的请求头    add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,Content-Type';    # ... proxy_pass 等指令 ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="D-快速排错"><a href="#D-快速排错" class="headerlink" title="D. 快速排错"></a><strong>D. 快速排错</strong></h3><ol><li><strong>第一步</strong>：修改配置后，永远先执行 <code>nginx -t</code> 检查语法。</li><li><strong>第二步</strong>：遇到问题（如页面打不开），立刻查看日志：<ul><li><strong><code>error.log</code></strong>: 查看 Nginx 自身是否有错误，比如无法连接后端、权限问题等。</li><li><strong><code>access.log</code></strong>: 查看请求是否到达 Nginx，以及 Nginx 返回的状态码是什么。</li></ul></li><li><strong>理解关键状态码</strong>:<ul><li><strong>502 Bad Gateway</strong>: 网关坏了。Nginx <strong>无法连接到后端服务</strong>，或者后端服务挂了、返回了无效响应。问题在<strong>后端应用本身</strong>。</li><li><strong>504 Gateway Timeout</strong>: 网关超时。Nginx <strong>成功连接到了后端</strong>，但后端处理请求<strong>太慢</strong>，超过了 Nginx 的等待时间。问题是<strong>后端应用性能低下或卡死</strong>。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka笔记</title>
      <link href="/2024/09/08/kafka-bi-ji/"/>
      <url>/2024/09/08/kafka-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Kafka-复习"><a href="#Kafka-复习" class="headerlink" title="Kafka 复习"></a>Kafka 复习</h1><h2 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka"></a>什么是Kafka</h2><p>Apache Kafka是一个高性能的分布式流处理平台，最初由LinkedIn公司内部开发用于处理大规模实时数据流，后来于2011年成为Apache基金会的开源项目。它作为一个强大的消息中间件，专门设计用于处理高容量、低延迟的实时数据流处理场景。Kafka架构优雅且可扩展，能够无缝地处理数百万条消息，同时保持毫秒级的传输延迟。它具有分布式、可分区、可复制的特性，不仅提供了极高的吞吐量和可靠性，还支持数据持久化和容错能力，使其成为现代数据管道和流处理应用的基础组件。</p><h2 id="Kafka的核心概念"><a href="#Kafka的核心概念" class="headerlink" title="Kafka的核心概念"></a>Kafka的核心概念</h2><ul><li><strong>Producer（生产者）</strong>：发布消息到Kafka的客户端应用程序</li><li><strong>Consumer（消费者）</strong>：订阅并处理来自Kafka的消息流的客户端应用程序</li><li><strong>Topic（主题）</strong>：Topic 是 Kafka 中对消息进行逻辑分类的单元。所有发布到 Kafka 集群的消息都必须归属于某一个 Topic。</li><li><strong>Partition（分区）</strong>：每个topic可以分为多个分区，允许并行处理，提高吞吐量</li><li><strong>Broker（代理）</strong>：一个独立的 Kafka 服务器实例被称为一个 Broker。多个 Broker 组合在一起构成一个 Kafka 集群。</li><li><strong>Consumer Group（消费者组）</strong>：一组协同工作的消费者，共同消费主题中的消息</li><li><strong>Offset（偏移量）</strong>：分区内每条消息的唯一标识符</li><li><strong>Zookeeper</strong>：管理和协调Kafka broker的服务</li></ul><h2 id="Kafka的特点"><a href="#Kafka的特点" class="headerlink" title="Kafka的特点"></a>Kafka的特点</h2><ul><li><strong>高吞吐量</strong>：能够处理数百万条消息</li><li><strong>低延迟</strong>：消息传递的延迟可以控制在毫秒级</li><li><strong>可扩展性</strong>：可以无缝扩展为处理更多数据</li><li><strong>持久性</strong>：消息被持久化到磁盘，防止数据丢失</li><li><strong>容错性</strong>：在节点失败的情况下继续运行</li><li><strong>高并发</strong>：支持数千个客户端同时读写</li><li><strong>实时处理</strong>：能够实时处理流数据</li></ul><h2 id="Kafka的应用场景"><a href="#Kafka的应用场景" class="headerlink" title="Kafka的应用场景"></a>Kafka的应用场景</h2><ul><li><strong>消息队列</strong>：作为传统消息中间件的替代</li><li><strong>日志收集</strong>：收集分布式系统中的日志</li><li><strong>流处理</strong>：实时处理数据流</li><li><strong>事件溯源</strong>：记录状态变更事件</li><li><strong>活动跟踪</strong>：跟踪用户行为和活动</li><li><strong>指标监控</strong>：收集和监控运营数据</li></ul><h2 id="一个服务器只能有一个-Broker-吗？"><a href="#一个服务器只能有一个-Broker-吗？" class="headerlink" title="一个服务器只能有一个 Broker 吗？"></a>一个服务器只能有一个 Broker 吗？</h2><p>这是一个关于“理论”与“最佳实践”的问题。</p><ul><li><strong>从理论上讲：不是。</strong><br>你完全可以在一台物理或虚拟服务器上，启动多个不同的 Broker 进程。只要你为每个 Broker 进程配置不同的端口号（例如 9092, 9093, 9094）、不同的 <code>broker.id</code> 和不同的数据存储目录，它们就可以在同一台机器上共存。</li><li><strong>从生产实践上讲：是的，最佳实践是一台服务器只运行一个 Broker 实例。</strong><br>为什么呢？<ol><li><strong>资源隔离：</strong> Kafka 是一个重度依赖磁盘 I&#x2F;O 和内存的系统。将一个服务器的全部资源（CPU、内存、磁盘、网络带宽）都分配给一个 Broker 实例，可以避免多个 Broker 实例之间争抢资源，从而获得最稳定和可预测的性能。</li><li><strong>简化运维：</strong> 一台服务器对应一个 Broker，使得监控、管理和故障排查都变得非常简单明了。如果一台服务器上的某个 Broker 出了问题，你立刻就能定位到是哪台机器。</li><li><strong>故障隔离：</strong> 如果一台服务器因为硬件故障宕机了，你只会损失一个 Broker。如果你在一台服务器上运行了三个 Broker，那一次宕机就会同时损失三个 Broker，对集群的冲击更大。</li></ol></li></ul><p>小结：在生产环境中，标准的最佳实践是一台服务器只部署和运行一个 Broker 实例，以确保资源隔离和简化运维。虽然技术上可以在一台服务器上运行多个 Broker，但这是一种不被推荐的做法。</p><h2 id="Topic-只能存在于多-Broker-组合中的其中一个是吗？"><a href="#Topic-只能存在于多-Broker-组合中的其中一个是吗？" class="headerlink" title="Topic 只能存在于多 Broker 组合中的其中一个是吗？"></a>Topic 只能存在于多 Broker 组合中的其中一个是吗？</h2><ul><li>**准确地说：一个 Topic 的数据，不仅可以，而且通常就应该分布在集群中的多个 Broker 上。**这是通过 Topic 的 <strong>分区（Partition）机制</strong> 来实现的，这也是 Kafka 得以实现高吞吐量的关键。</li></ul><p>我们用一个具体的例子来说明：<br>假设你有一个由 3 个 Broker 组成的 Kafka 集群（Broker 1, Broker 2, Broker 3）。</p><p>现在，你创建了一个名为 <code>order-topic</code> 的主题，并设定它有 <strong>3 个分区</strong>（我们称之为 P0, P1, P2）。</p><p>Kafka 在创建这个 Topic 时，会自动将这些分区尽可能均匀地分布到集群的所有 Broker 上，以实现负载均衡。一个可能的结果是：</p><ul><li><code>order-topic</code> 的分区 <code>P0</code> 被存放在 <strong>Broker 1</strong> 上。</li><li><code>order-topic</code> 的分区 <code>P1</code> 被存放在 <strong>Broker 2</strong> 上。</li><li><code>order-topic</code> 的分区 <code>P2</code> 被存放在 <strong>Broker 3</strong> 上。</li></ul><p><strong>这样做的好处是：</strong><br>当生产者往 <code>order-topic</code> 发送大量消息时，这些消息可以被同时发往三个不同的分区（P0, P1, P2），也就是同时写入了三台不同的服务器（Broker 1, 2, 3）。消费者也可以同时从这三个分区读取数据。这样一来，读写的压力就被分摊到了整个集群，而不是集中在一台机器上，从而极大地提升了性能。</p><p>所以，<strong>Topic 是一个逻辑概念，它的物理存储（即它的所有分区）是分散在整个集群的多个 Broker 上的。</strong></p><h2 id="一个生产Broker的最佳实践是一个Partition吗？"><a href="#一个生产Broker的最佳实践是一个Partition吗？" class="headerlink" title="一个生产Broker的最佳实践是一个Partition吗？"></a>一个生产Broker的最佳实践是一个Partition吗？</h2><p>先说结论：一个 Broker 只承载一个 Partition”在生产环境中是<strong>不现实且低效的</strong>。</p><p>正确的生产实践恰恰相反：<strong>一个 Broker 通常会、也应该会承载成百上千个分区。</strong></p><p><strong>为什么会是这样？</strong></p><ol><li><strong>资源利用率问题：</strong><br>一个分区（本质上就是磁盘上的几个日志文件）远不足以利用满一台现代服务器的强大资源（多核CPU、几十G内存、高吞吐磁盘阵列、万兆网卡）。如果一台强大的服务器（Broker）只为一个分区服务，那将是极大的资源浪费，就像用一辆大卡车只运送一封信。</li><li><strong>成本和可扩展性问题：</strong><br>为了获得高吞吐量，一个 Topic 可能需要几十甚至几百个分区。如果一个 Broker 只能放一个分区，那么一个拥有 100 个分区的 Topic 就需要 100 个 Broker，也就是 100 台服务器！这在成本和运维上都是不可行的。<br>而现实中，我们可以用一个 10 台 Broker 的集群，轻松承载起成百上千个分区（这些分区可以来自不同的 Topic）。</li></ol><h3 id="重新理解“方便管理”和“故障隔离”"><a href="#重新理解“方便管理”和“故障隔离”" class="headerlink" title="重新理解“方便管理”和“故障隔离”"></a><strong>重新理解“方便管理”和“故障隔离”</strong></h3><ul><li><strong>方便管理和定位问题：</strong> 这正是“<strong>一台服务器一个Broker</strong>”这个原则解决的。如果监控系统显示 Broker 5 的CPU使用率100%，你不需要思考是哪个进程导致的，直接登录到承载 Broker 5 的那台服务器去排查问题即可。</li><li><strong>故障隔离：</strong> 这不是通过减少 Broker 上的分区数来实现的，而是通过我们之前深入讨论的<strong>副本机制 (Replication)</strong> 来实现的。<ul><li>假设一个 Broker 上承载了 100 个分区的 Leader。当这台 Broker 宕机时，Kafka 的高可用机制会启动，将这 100 个 Leader 的角色<strong>自动、快速地转移</strong>到它们各自的副本（Follower）所在的、其他健康的 Broker 上。</li><li>系统虽然会经历一次短暂的“重平衡”抖动，但数据不会丢失，服务也会迅速恢复。这才是 Kafka 在分区层面实现故障隔离的方式。</li></ul></li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h3><p>所以，我们需要建立一个清晰的、分层的生产架构认知：</p><ol><li><strong>物理层：</strong> 一台服务器 → 运行一个 Broker 进程。（为了资源隔离和管理）</li><li><strong>逻辑层：</strong> 一个 Broker → 同时管理<strong>多个</strong>分区。（为了资源利用率和成本效益）</li><li><strong>高可用层：</strong> 一个分区的多个副本（Leader 和 Followers）→ 必须分布在<strong>不同的</strong> Broker 上。（为了故障转移和数据安全）</li></ol><h2 id="分区-Partition-Kafka-高性能的基石"><a href="#分区-Partition-Kafka-高性能的基石" class="headerlink" title="分区 (Partition) - Kafka 高性能的基石"></a>分区 (Partition) - Kafka 高性能的基石</h2><p><strong>1. 技术定义</strong></p><p>每个 Topic 都可以被划分为一个或多个分区（Partition），分区（Partition）是一个逻辑概念，它的物理存在形式是副本（Replica），而每一个副本的本质，就是一个有序的、只能追加写入的日志文件（Append-only Log）。</p><p>当声称在要将消息写入 <code>分区0</code> 时，Kafka 内部会把它翻译成：“找到 <code>分区0</code> 的 <strong>Leader 副本</strong>，然后把消息追加到那个<strong>副本对应的日志文件</strong>的末尾</p><p><strong>2. 分区的核心特性</strong></p><ul><li><strong>有序性 (Ordering):</strong> 在<strong>单个 Partition 内部</strong>，消息是严格按照其被写入的顺序进行存储和读取的。这意味着，如果你发送消息 M1，然后发送 M2 到同一个 Partition，那么消费者也一定会先读到 M1，再读到 M2。<strong>这是 Kafka 提供的最重要的顺序保证。</strong></li><li><strong>不可变性 (Immutability):</strong> 一旦消息被写入 Partition，它就不能被修改。这种设计简化了系统，并提高了性能。</li><li><strong>偏移量 (Offset):</strong> Partition 中的每一条消息都有一个唯一的、从 0 开始单调递增的序列号，这个序列号被称为 <strong>Offset</strong>。Offset 唯一地标识了 Partition 中的一条消息。消费者通过 Offset 来追踪自己消费到了哪个位置。</li></ul><p><strong>3. 为什么要设计分区？</strong></p><p>设计分区的核心目的有两个：</p><ul><li><strong>可伸缩性&#x2F;水平扩展 (Scalability)：</strong><ul><li><strong>存储层面：</strong> 一个 Topic 的数据可以被分散存储在集群的多个 Broker 上（如我们上一步所讨论的）。这打破了单台服务器的磁盘容量和 I&#x2F;O 性能的限制。如果数据量增长，你可以通过增加 Broker 节点来水平扩展整个集群的存储能力。</li><li><strong>性能层面：</strong> 读写操作可以并行地在多个 Broker 上的多个 Partition 上进行，极大地提高了整个 Topic 的总吞吐量。</li></ul></li><li><strong>并行处理 (Parallelism)：</strong><ul><li>分区的存在，使得消费者可以实现并行消费。一个消费者组（Consumer Group）可以有多个消费者实例，每个实例负责消费一个或多个 Partition。这样，一个 Topic 的总消费负载就被分摊到了多个消费者上，大大提高了数据处理能力。</li></ul></li></ul><hr><p><strong>总结一下第三步的关键点：</strong></p><blockquote><p>Topic 是一个逻辑概念，而 Partition 是物理上的实现。一个 Topic 由多个 Partition 组成，这些 Partition 分布在不同的 Broker 上。消息在单个 Partition 内是有序的，并通过 Offset 进行唯一标识。分区的设计是 Kafka 实现高吞吐量和高可伸缩性的核心原因。</p></blockquote><h2 id="副本-Replika-与-Leader-Follower-模型-Kafka-可靠性的保障"><a href="#副本-Replika-与-Leader-Follower-模型-Kafka-可靠性的保障" class="headerlink" title="副本 (Replika) 与 Leader&#x2F;Follower 模型 - Kafka 可靠性的保障"></a>副本 (Replika) 与 Leader&#x2F;Follower 模型 - Kafka 可靠性的保障</h2><p><strong>1. 技术定义</strong></p><ul><li><strong>副本 (Replica):</strong> Kafka 允许为每个分区（Partition）创建多个数据副本，这些副本被分布在集群中不同的 Broker 上。这就是<strong>副本机制</strong>。副本的数量是可以配置的，这个数量被称为<strong>副本因子 (Replication Factor)</strong>。例如，如果副本因子为 3，那么每个分区就会有 1 个主副本和 2 个次副本。</li></ul><p><strong>2. Leader 与 Follower 的角色分工</strong></p><p>一个分区的多个副本之间，并不是平等的，它们有明确的主从关系。</p><ul><li><strong>Leader (领导者 &#x2F; 主副本)：</strong><ul><li><strong>职责：</strong> 每个分区在任意时刻，有且仅有一个副本是 Leader。它负责处理所有来自客户端（生产者和消费者）的<strong>读和写</strong>请求。</li><li><strong>类比：</strong> 它是唯一对外营业的“正本”柜台。</li></ul></li><li><strong>Follower (跟随者 &#x2F; 次副本)：</strong><ul><li><strong>职责：</strong> Leader 以外的其他副本都是 Follower。Follower 不与客户端直接交互。它们唯一的任务就是<strong>被动地、持续地</strong>从 Leader 那里拉取最新的数据，以保持和 Leader 的数据同步。</li><li><strong>类比：</strong> 它们是内部使用的“备份”柜台，只负责抄写正本柜台的数据，不对外服务。</li></ul></li></ul><p><strong>3. 数据同步与故障转移的流程</strong></p><p>这个主从模型是如何工作的？</p><ul><li><strong>数据写入流程 (正常情况):</strong><ol><li>生产者发送一条消息到某个分区。</li><li>请求被直接发送到该分区的 <strong>Leader</strong> 所在的 Broker 上。</li><li>Leader 将消息写入自己的本地日志。</li><li>各个 <strong>Follower</strong> 定期向 Leader 发送拉取请求，将 Leader 的新消息复制到自己的本地日志中。</li></ol></li><li><strong>故障转移流程 (Leader 宕机):</strong><ol><li>分区的 Leader 所在的 Broker 突然宕机。</li><li>Kafka 控制器 (Controller) 检测到这个情况。</li><li>控制器会从所有与 Leader 保持“同步”的 Follower 列表中，选举出一个新的 Leader。</li><li>选举成功后，这个曾经的 Follower 就成为了新的 Leader，开始对外提供读写服务。客户端会被告知新的 Leader 地址。整个过程对用户来说是无感知的，是自动完成的。</li></ol></li></ul><p>现在，我们用一个非常具体、直观的例子来说明这个过程。</p><p><strong>场景设定：</strong></p><ul><li><strong>集群：</strong> 我们有一个由 3 台服务器组成的 Kafka 集群，分别是 <strong>Broker 1</strong>、<strong>Broker 2</strong> 和 <strong>Broker 3</strong>。</li><li><strong>Topic：</strong> 我们创建一个名为 <code>payment_topic</code> 的主题，用来处理支付消息。</li><li><strong>分区和副本：</strong> 为了让例子简单清晰，我们假设 <code>payment_topic</code> 只有 <strong>1 个分区</strong>，就是 <code>Partition 0</code>。我们设置<strong>副本因子为 3</strong>，这意味着 <code>Partition 0</code> 会有 1 个 Leader 和 2 个 Follower。</li></ul><p><strong>阶段一：正常运行</strong></p><p>系统启动后，Kafka 会自动进行选举和分配。一个可能的结果是：</p><ul><li><code>Partition 0</code> 的 <strong>Leader</strong> 被分配在了 <strong>Broker 1</strong> 上。</li><li><code>Partition 0</code> 的两个 <strong>Follower</strong> 分别被分配在了 <strong>Broker 2</strong> 和 <strong>Broker 3</strong> 上。</li></ul><p><strong>现在，一个生产者要发送一条支付消息：<code>&#123; &quot;paymentId&quot;: &quot;abcde&quot;, &quot;amount&quot;: 50 &#125;</code></strong></p><ol><li>生产者的请求被直接发送到 <strong>Broker 1</strong>（因为它是 Leader）。</li><li><strong>Broker 1</strong> 收到消息，将它写入自己的磁盘。</li><li><strong>Broker 2</strong> 和 <strong>Broker 3</strong> 上的 Follower，像往常一样，从 Broker 1 拉取数据，发现有一条新消息，于是也把这条消息写入各自的磁盘。</li><li>当数据成功同步到足够数量的副本后（这个数量可以配置），<strong>Broker 1</strong> 向生产者返回一个“发送成功”的确认。</li></ol><p>在这个阶段，所有读写都由 Broker 1 处理，Broker 2 和 3 只是默默地在后台备份。</p><hr><p><strong>阶段二：故障发生</strong></p><p><strong>突然，Broker 1 所在的服务器因为电源故障，宕机了！</strong></p><p>现在的情况是：</p><ul><li><code>Partition 0</code> 的 Leader 消失了。</li><li>生产者和消费者无法再连接到 Broker 1。</li></ul><hr><p><strong>阶段三：自动故障转移 (Failover)</strong></p><p>Kafka 的高可用机制现在开始启动：</p><ol><li>集群的<strong>控制器 (Controller)</strong>（通常是集群中的某一个 Broker）通过心跳机制，很快就发现 <strong>Broker 1</strong> 失联了。</li><li>控制器立刻查看 <code>Partition 0</code> 的 Follower 列表，发现 <strong>Broker 2</strong> 和 <strong>Broker 3</strong> 上的副本都处于“同步”状态。</li><li>控制器在这两个 Follower 中发起一次新的选举。假设选举结果是 <strong>Broker 2</strong> 胜出。</li><li>控制器发布命令：<strong>“从现在起，Broker 2 成为 <code>Partition 0</code> 的新 Leader！”</strong></li><li><strong>Broker 2</strong> 的角色从 Follower 转变为 Leader。<strong>Broker 3</strong> 保持 Follower 不变，但它现在开始从新的 Leader（Broker 2）那里同步数据。</li><li>Kafka 会通知生产者客户端：“<code>payment_topic</code> 的 <code>Partition 0</code> 的 Leader 地址已经变更为 Broker 2。”</li></ol><p><strong>结果：</strong></p><ul><li>现在，当生产者要发送下一条消息 <code>&#123; &quot;paymentId&quot;: &quot;fghij&quot;, &quot;amount&quot;: 80 &#125;</code> 时，它会直接将请求发送到 <strong>Broker 2</strong>。</li><li>整个系统恢复了正常读写，整个切换过程是自动的，通常在几秒钟内完成，保证了服务的连续性。</li></ul><h2 id="Kafka-实现高并发的四大核心支柱"><a href="#Kafka-实现高并发的四大核心支柱" class="headerlink" title="Kafka 实现高并发的四大核心支柱"></a><strong>Kafka 实现高并发的四大核心支柱</strong></h2><h3 id="磁盘的高效利用"><a href="#磁盘的高效利用" class="headerlink" title="磁盘的高效利用"></a>磁盘的高效利用</h3><ul><li><strong>顺序读写</strong>：传统数据库为了更新数据，需要进行大量的随机磁盘读写，这非常慢，因为磁头需要不停地移动寻道。而 Kafka 的设计是<strong>日志追加 (Append-only Log)</strong>，所有新消息都只是简单地追加到文件的末尾。这种顺序写入的方式，速度几乎和写内存一样快，因为它省去了代价高昂的磁头寻道时间。</li><li><strong>操作系统页缓存的利用</strong>：Kafka 并不自己去设计复杂的缓存机制，而是把这个工作巧妙地“甩锅”给了操作系统。数据写入磁盘时，实际上是先写入了操作系统的页缓存（Page Cache），这是一个位于内存中的高速缓存区，然后由操作系统在后台异步地将数据刷到磁盘上。读取数据时，如果数据恰好在 Page Cache 中（对于热门数据很常见），就可以直接从内存中读取，避免了磁盘I&#x2F;O。</li></ul><h3 id="I-O优化：减少一切不必要的操作"><a href="#I-O优化：减少一切不必要的操作" class="headerlink" title="I&#x2F;O优化：减少一切不必要的操作"></a>I&#x2F;O优化：减少一切不必要的操作</h3><ul><li><strong>零拷贝（Zero-Copy）</strong>：<ul><li>在传统的数据发送流程中，数据需要从磁盘拷贝到内核空间（页缓存），再从内核空间拷贝到用户应用程序空间，然后应用程序再把数据拷贝回内核的 Socket 缓存区，最后才发送到网卡。这个过程涉及到多次数据拷贝和CPU上下文切换。</li><li>而Kafka使用了<strong>零拷贝技术</strong>，允许数据直接从内核空间的页缓存，发送到网卡，全程不经过用户应用程序。这极大地减少了数据拷贝次数和CPU开销，是Kafka实现超高数据发送性能的秘密武器之一。</li></ul></li><li><strong>批处理（Batching）</strong>：它体现在生产者和消费者两端。<ul><li><strong>生产者侧：</strong> 生产者不会来一条消息就立刻发送一条，而是会将多条消息收集成一个<strong>批次 (Batch)</strong>，然后再统一发送给 Broker，大大减少了网络请求的开销。</li><li><strong>消费者侧：</strong> 消费者也是一次性从 Broker 拉取一个批次的数据回来处理，而不是一条一条地去请求。</li></ul></li></ul><h3 id="并行化架构设计"><a href="#并行化架构设计" class="headerlink" title="并行化架构设计"></a>并行化架构设计</h3><p>**分区机制 (Partitioning):：**这是 Kafka 实现水平扩展和高并发的根本。一个 Topic 被分成多个分区，这些分区可以被分布在不同的 Broker 服务器上。</p><ul><li><strong>写并行：</strong> 生产者可以同时向多个分区写入数据。</li><li><strong>读并行：</strong> 消费者组内的多个消费者可以同时消费不同的分区。</li><li>一个 Topic 的总吞吐量，理论上是<strong>所有分区的吞吐量之和</strong>。如果觉得吞吐量不够，最直接的办法就是增加 Topic 的分区数（并相应增加消费者数量），以此来水平扩展整个系统的处理能力。</li></ul><h3 id="精简的协议与数据结构"><a href="#精简的协议与数据结构" class="headerlink" title="精简的协议与数据结构"></a><strong>精简的协议与数据结构</strong></h3><ul><li><strong>二进制协议:</strong> Kafka 客户端和服务器之间使用非常精简的二进制协议进行通信，开销小，序列化和反序列化效率高。</li><li><strong>简单的日志结构:</strong> 分区的数据文件就是简单的日志文件，查询时通过 Offset 进行，这是一个简单的数学计算，不需要像数据库那样复杂的索引结构，因此查找效率非常高。</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>Kafka之所以能实现这么高的并发和吞吐能力，主要得益于它在四个方面的精妙设计：</p><p><strong>第一，是对磁盘的极致高效利用。</strong> 它通过采用顺序读写（Append-only Log）的方式，将慢速的磁盘随机I&#x2F;O变成了接近内存速度的顺序I&#x2F;O。同时，它充分利用了操作系统的页缓存（Page Cache），实现了读写的进一步加速。</p><p><strong>第二，是极致的I&#x2F;O优化。</strong> 它使用了‘零拷贝’技术，在数据发送时避免了不必要的内核与用户空间的数据复制，大大降低了CPU开销。此外，无论是生产者还是消费者，都采用了‘批处理’的设计，将多次小的网络请求合并为一次大的请求，极大地提升了效率。</p><p><strong>第三，是它的并行化架构。</strong> Kafka通过‘分区’机制，将一个Topic的数据打散到多个Broker上，实现了读写的并行处理。Topic的整体吞吐量可以通过增加分区数来水平扩展。</p><p><strong>第四，是它精简的底层设计。</strong> 它使用了高效的二进制通信协议，并且其核心的数据结构（日志文件）非常简单，通过Offset进行数据定位，查询效率很高。</p><p>总的来说，Kafka并非凭空变快，而是通过这一系列精妙的设计，将压力分解、将操作合并、将数据流优化，最终实现了看似不可思议的高吞吐能力。”</p><h2 id="Kafka-消费之后的消息会被删除吗？"><a href="#Kafka-消费之后的消息会被删除吗？" class="headerlink" title="Kafka 消费之后的消息会被删除吗？"></a>Kafka 消费之后的消息会被删除吗？</h2><p><strong>答：绝对不会。这是 Kafka 与传统消息队列（如 RabbitMQ）最核心、最本质的区别之一。</strong></p><ul><li><strong>传统消息队列：</strong> 消息被消费（并确认）后，通常会从队列中删除，因为它被视为一个“待办事项”，办完了就销毁。</li><li><strong>Kafka：</strong> <strong>消费行为本身，绝对不会导致 Kafka 删除消息。</strong></li></ul><p><strong>为什么不删除？</strong><br>在 Kafka 的世界里，数据被看作是一条<strong>事实记录流（Stream of Facts）</strong>，而不是一次性的任务。消费者（Consumer）更像是一个“读者”，它只是从日志的某个位置（我们称之为 <strong>Offset</strong>）开始读取数据。</p><ul><li>消费者的消费进度，仅仅是移动它自己的书签（Offset）。</li><li>多个不同的消费者组（比如“实时风控组”、“数据分析组”、“日志监控组”）可以<strong>独立地、反复地</strong>消费同一份数据，它们的消费进度互不影响。一个组读完了，另一个组可能才刚开始读。</li></ul><p><strong>那么，数据到底什么时候被删除？</strong></p><p>数据删除<strong>只和 Topic 配置的“数据保留策略 (Log Retention Policy)”有关</strong>。Kafka 提供两种主要的保留策略：</p><ol><li><strong>基于时间的保留 (Time-based Retention):</strong><ul><li>这是最常用的策略。你可以为一个 Topic 设置一个保留时长，比如 7 天。</li><li><strong>配置示例：</strong> <code>retention.ms=604800000</code> (7天的毫秒数)</li><li><strong>效果：</strong> Kafka 会启动一个后台线程，定期检查并<strong>自动删除</strong>那些发布时间超过 7 天的旧数据，<strong>无论这些数据是否被消费过。</strong></li></ul></li><li><strong>基于大小的保留 (Size-based Retention):</strong><ul><li>你也可以为一个 Topic 设置一个总的存储大小阈值，比如 50 GB。</li><li><strong>配置示例：</strong> <code>retention.bytes=53687091200</code> (50 GB的字节数)</li><li><strong>效果：</strong> 当这个 Topic 的总数据量将要超过 50 GB 时，Kafka 会<strong>从最老的数据开始删除</strong>，以确保总大小不超过这个限制。</li></ul></li></ol><p>通常，你可以同时设置这两个参数，任何一个条件先被触发，都会导致数据被删除。</p><h2 id="消费者组-Consumer-Group-和消费过程"><a href="#消费者组-Consumer-Group-和消费过程" class="headerlink" title="消费者组 (Consumer Group) 和消费过程"></a>消费者组 (Consumer Group) 和消费过程</h2><h3 id="消费者组-Consumer-Group-Kafka-如何实现并行处理"><a href="#消费者组-Consumer-Group-Kafka-如何实现并行处理" class="headerlink" title="消费者组 (Consumer Group) - Kafka 如何实现并行处理"></a>消费者组 (Consumer Group) - Kafka 如何实现并行处理</h3><p><strong>1. 技术定义</strong></p><ul><li><strong>消费者组 (Consumer Group):</strong> 一个消费者组由一个或多个消费者实例（进程）组成。它们共享同一个 <code>group.id</code>，并作为一个整体，共同来消费一个或多个 Topic 的数据。</li></ul><p><strong>2. 核心工作原则（这是关键）</strong></p><p>Kafka 通过消费者组来实现两件事：<strong>负载均衡</strong>和<strong>并行消费</strong>。它遵循一条黄金法则：</p><blockquote><p>一个 Topic 的同一个分区，在任意时刻，只能被同一个消费者组里的一个消费者实例所消费。</p></blockquote><p>这条规则的<strong>作用域，仅限于单个消费者组（Consumer Group）内部</strong>。不同的消费者组之间是完全独立的、互不干扰的。</p><p>换句话说，分区是消费的最小并行单元。Kafka 会把一个 Topic 的所有分区，尽可能均匀地分配给一个消费者组里的所有成员。</p><p><strong>3. 分配规则与场景示例</strong></p><p>我们来看一个 Topic，名为 <code>log_topic</code>，它有 <strong>4 个分区</strong>（P0, P1, P2, P3）。</p><ul><li><strong>场景 A：组里只有 1 个消费者 (C1)</strong><ul><li><strong>分配结果：</strong> 消费者 C1 会获得全部分区的消费权，即它需要自己处理 P0, P1, P2, P3 的所有消息。</li></ul></li><li><strong>场景 B：组里有 2 个消费者 (C1, C2)</strong><ul><li><strong>分配结果：</strong> Kafka 会进行负载均衡。C1 可能会被分配 P0 和 P1，C2 会被分配 P2 和 P3。消费能力理论上翻了一倍。</li></ul></li><li><strong>场景 C：组里有 4 个消费者 (C1, C2, C3, C4)</strong><ul><li><strong>分配结果：</strong> 这是最理想的并行状态。每个消费者刚好分配到 1 个分区。C1-&gt;P0, C2-&gt;P1, C3-&gt;P2, C4-&gt;P3。</li></ul></li><li><strong>场景 D：组里有 5 个消费者 (C1, C2, C3, C4, C5)</strong><ul><li><strong>分配结果：</strong> 由于分区总共只有 4 个，根据“某一时刻一个分区只能被一个消费者消费”的原则，有 4 个消费者会被分配到分区。<strong>而第 5 个消费者 (C5) 将会处于空闲（idle）状态，接收不到任何消息。</strong></li></ul></li></ul><p><strong>4. 消费者组的意义</strong></p><ul><li><strong>高吞吐 &#x2F; 并行处理：</strong> 如果你觉得消息处理得太慢，只需要在同一个消费者组里增加更多的消费者实例，就可以线性地提升处理能力（但消费者数量超过分区数就无效了）。</li><li><strong>高可用 &#x2F; 故障转移：</strong> 如果组里的某个消费者实例突然宕机，它之前负责消费的分区，会被 Kafka 自动地重新分配给组里其他存活的消费者。这个过程被称为<strong>重平衡 (Rebalance)</strong>。</li></ul><h2 id="消费者组-Consumer-Group-的“总控-调度”模型"><a href="#消费者组-Consumer-Group-的“总控-调度”模型" class="headerlink" title="消费者组 (Consumer Group) 的“总控-调度”模型"></a>消费者组 (Consumer Group) 的“总控-调度”模型</h2><ul><li><strong>Group Coordinator (总控中心):</strong> 这是集群中某一个 Broker 担当的角色。它专门负责管理某一个消费者组，比如维护组成员列表、监控成员心跳，以及最重要的——<strong>主持和启动重平衡 (Rebalance)</strong>，也就是“分配任务”。</li><li><strong>Consumers (一线员工):</strong> 组里的每个消费者实例就是 worker。它们负责执行具体的工作（消费分区），并定期向 Coordinator 汇报心跳（表示自己还活着）。</li></ul><h3 id="1-“部分消费者没有分配到消息”（员工闲置问题）"><a href="#1-“部分消费者没有分配到消息”（员工闲置问题）" class="headerlink" title="1. “部分消费者没有分配到消息”（员工闲置问题）"></a>1. “部分消费者没有分配到消息”（员工闲置问题）</h3><p>这种情况确实会发生，其根本原因是：<strong>组内的消费者实例数量 &gt; Topic 的分区数量</strong>。</p><ul><li><strong>核心原则：</strong> Kafka 的黄金法则是，一个分区在同一时刻只能被组内的一个消费者消费。分区是最小的工作单元。</li><li><strong>举例：</strong> 一个 Topic 有 5 个分区，但你的消费组里却启动了 8 个消费者实例。</li><li><strong>结果：</strong> Coordinator 会将 5 个分区一对一地分配给 5 个消费者。剩下的 3 个消费者因为“无任务可领”，就会处于<strong>空闲 (idle)</strong> 状态，它们不会收到任何消息。</li></ul><p><strong>要点：</strong> 这说明通过无限增加消费者来提升处理能力是有上限的，上限就是 Topic 的分区数。</p><h3 id="2-“消息分配不均匀”（工作量不均问题）"><a href="#2-“消息分配不均匀”（工作量不均问题）" class="headerlink" title="2. “消息分配不均匀”（工作量不均问题）"></a>2. “消息分配不均匀”（工作量不均问题）</h3><p>这种情况也很常见，通常发生在<strong>分区数不能被消费者数整除</strong>的情况下。</p><ul><li><strong>举例：</strong> 一个 Topic 有 7 个分区，但组里有 3 个消费者。</li><li><strong>结果：</strong> Coordinator 没法做到完美平均分配。根据分配策略（比如默认的 <code>RangeAssignor</code>），分配结果可能是：<ul><li>消费者 A：负责 P0, P1, P2 (3个分区)</li><li>消费者 B：负责 P3, P4 (2个分区)</li><li>消费者 C：负责 P5, P6 (2个分区)</li></ul></li><li><strong>后果：</strong> 消费者 A 的负载会比 B 和 C 更重。在设计系统时，需要考虑到这种不均衡的可能性。</li></ul><h3 id="3-“重复分配消息”（任务交接问题）"><a href="#3-“重复分配消息”（任务交接问题）" class="headerlink" title="3. “重复分配消息”（任务交接问题）"></a>3. “重复分配消息”（任务交接问题）</h3><p>Kafka 的 Coordinator <strong>绝对不会在同一时刻，将同一个分区分配给两个不同的消费者</strong>。这会造成严重的数据混乱。</p><p>您感受到的“重复”，实际上指的是我们之前讨论过的 <strong>“重复消费 (Duplicate Processing)”</strong>，它发生在 <strong>Rebalance</strong> 的过程中。</p><ul><li><strong>场景：</strong><ol><li>消费者 A 正在处理分区 P0 的消息，它已经处理到 Offset 150，但还没来得及提交这个进度。</li><li>此时，消费者 A 突然崩溃。</li><li>Coordinator 检测到 A 死亡，触发 Rebalance，将分区 P0 这个“孤儿任务”<strong>重新分配</strong>给了消费者 B。</li><li>消费者 B 接手 P0 后，它会从<strong>上一次成功提交的 Offset</strong>（比如是100）开始消费。</li></ol></li><li><strong>结果：</strong> 消费者 B 会重新处理一遍 Offset 100 到 150 的消息。从旁观者角度看，消息好像被“重复分配”了，但实际上是<strong>任务被干净地交接了，只是接手者从上次的存档点开始工作而已</strong>。</li></ul><h3 id="4-“消费者在消费时是处于不可用状态的”"><a href="#4-“消费者在消费时是处于不可用状态的”" class="headerlink" title="4. “消费者在消费时是处于不可用状态的”"></a>4. “消费者在消费时是处于不可用状态的”</h3><p>这个描述同样非常精准，它指的正是 <strong>Rebalance 期间的“Stop-the-World”现象</strong>。</p><p>在正常情况下，消费者是一直在工作的。但一旦 Rebalance 被触发（比如有成员加入或退出），就会发生以下情况：</p><ol><li><strong>全体暂停：</strong> Coordinator 会通知组内<strong>所有</strong>的消费者：“全体注意，停止拉取新消息，准备重新分配任务！”</li><li><strong>放弃任务：</strong> 所有消费者会放弃对自己当前负责的分区的“所有权”。</li><li><strong>等待分配：</strong> 所有消费者都进入等待状态，直到 Coordinator 完成新的分配方案。</li><li><strong>恢复工作：</strong> 消费者收到新的任务分配后，才开始根据新的分配方案去消费。</li></ol><p>这个从“暂停”到“恢复工作”的整个过程，就是消费者组的“<strong>全体不可用状态</strong>”。频繁的 Rebalance 会严重影响消费组的整体吞吐量，是生产环境中需要尽量避免的。</p><h2 id="深入理解“某一时刻”：重平衡-Rebalance"><a href="#深入理解“某一时刻”：重平衡-Rebalance" class="headerlink" title="深入理解“某一时刻”：重平衡 (Rebalance)"></a>深入理解“某一时刻”：重平衡 (Rebalance)</h2><h3 id="什么是重平衡？"><a href="#什么是重平衡？" class="headerlink" title="什么是重平衡？"></a><strong>什么是重平衡？</strong></h3><p><strong>重平衡</strong>，本质上就是 Kafka 将一个 Topic 的全部分区，在<strong>同一个消费者组</strong>的所有成员之间，进行<strong>重新分配</strong>的过程。</p><p>它的最终目的是为了确保：</p><ol><li>组里的每个消费者都分到了合理的工作量。</li><li>Topic 的每个分区都有一个明确的负责人。</li></ol><h3 id="重平衡的三大触发场景"><a href="#重平衡的三大触发场景" class="headerlink" title="重平衡的三大触发场景"></a><strong>重平衡的三大触发场景</strong></h3><p><strong>1. 消费者组成员数量发生变化</strong></p><p>这完全是<strong>和消费者直接相关</strong>的。</p><ul><li><strong>新消费者加入：</strong><ul><li><strong>场景：</strong> 你觉得消费速度太慢，于是启动了一个新的消费者实例，并让它加入到现有的消费者组中。</li><li><strong>原因：</strong> 新来的成员需要被分配工作（分区）。为了公平，不能让它闲着，所以需要从其他成员那里拿走一些分区，重新进行“大锅饭”式的分配。</li></ul></li><li><strong>旧消费者离开：</strong><ul><li><strong>场景：</strong> 组里的一个消费者实例因为正常关闭或意外崩溃而下线。</li><li><strong>原因：</strong> 它之前负责的分区现在成了“孤儿”，无人处理。为了保证这些分区的数据能被继续消费，必须将它们分配给组里其他还存活的成员。我们之前讨论的 <code>session.timeout.ms</code> 和 <code>max.poll.interval.ms</code> 就是用来检测这种“意外离开”的。</li></ul></li></ul><p><strong>2. 订阅的 Topic 分区数量发生变化</strong></p><p>“<strong>分区导致</strong>”的情况。</p><ul><li><strong>场景：</strong> 一个 Topic <code>my-topic</code> 原本有 5 个分区，一个消费者组正在稳定地消费它。这时，运维管理员为了提升吞吐量，执行命令将 <code>my-topic</code> 的分区数增加到了 8 个。</li><li><strong>原因：</strong> 集群中突然多出来了 3 个新的分区（P5, P6, P7），这些新分区目前没有消费者负责。为了让这些新分区的数据也能被消费，Kafka 必须触发一次重平衡，将全部的 8 个分区在所有消费者之间重新分配。</li></ul><p><strong>3. 订阅的 Topic 本身发生变化</strong></p><ul><li><strong>场景：</strong> 一个消费者组原本只订阅了 <code>topic-A</code>。现在你修改了消费者的代码，让它同时订阅 <code>topic-A</code> 和 <code>topic-B</code>。</li><li><strong>原因：</strong> 整个消费者组需要消费的总分区列表发生了变化（增加了 <code>topic-B</code> 的所有分区）。旧的分配方案已经过时，因此必须触发重平衡，以制定一个包含两个 Topic 所有分区的新分配方案。</li></ul><h3 id="重平衡期间会发生什么？"><a href="#重平衡期间会发生什么？" class="headerlink" title="重平衡期间会发生什么？"></a><strong>重平衡期间会发生什么？</strong></h3><p>这是一个“<strong>Stop-the-World</strong>”的过程，对消费有一定影响：</p><ol><li><strong>暂停消费：</strong> 当 Rebalance 触发时，该消费组内的<strong>所有消费者</strong>都会暂停拉取和处理消息。</li><li><strong>重新分配：</strong> 由组协调器（Group Coordinator，一个 Broker）来主持，根据分配策略，为每个消费者成员重新分配分区。</li><li><strong>恢复消费：</strong> 分配完成后，每个消费者只消费新分配给自己的分区。</li></ol><h2 id="Offset-的提交机制"><a href="#Offset-的提交机制" class="headerlink" title="Offset 的提交机制"></a>Offset 的提交机制</h2><h3 id="1-概念回顾与定义"><a href="#1-概念回顾与定义" class="headerlink" title="1. 概念回顾与定义"></a><strong>1. 概念回顾与定义</strong></h3><ul><li><strong>Offset (位移):</strong> 我们之前提过，分区（Partition）内的每一条消息都有一个唯一的、从 0 开始递增的序列号，这就是 Offset。它就是消息的地址。</li><li><strong>提交位移 (Commit Offset):</strong> 消费者在消费消息的过程中，需要定期地向 Kafka 集群中的一个特殊内部 Topic（名为 <code>__consumer_offsets</code>）报告：“对于我负责的某个分区，我已经成功处理到哪个 Offset 了”。这个“报告”或“记录”的动作，就叫做<strong>提交位移</strong>。</li></ul><p><strong>它的核心目的，就像在书里夹一个书签：</strong></p><p>当消费者重启，或者分区被重新分配给另一个消费者时，新的消费者可以先去<code>__consumer_offsets</code>这个地方查找这个分区对应的“书签”，从而准确地知道应该从哪里开始继续阅读，以保证数据处理的连续性。</p><h3 id="2-两种核心的提交方式"><a href="#2-两种核心的提交方式" class="headerlink" title="2. 两种核心的提交方式"></a><strong>2. 两种核心的提交方式</strong></h3><p>提交“书签”的方式主要有两种：</p><ul><li><strong>方式一：自动提交 (Auto Commit)</strong><ul><li><strong>工作方式：</strong> 这是 Kafka 消费者的<strong>默认行为</strong>。你只需要在配置中设置 <code>enable.auto.commit=true</code>（默认就是 true），并设定一个时间间隔（<code>auto.commit.interval.ms</code>，默认是 5 秒）。消费者客户端会每隔 5 秒，自动地将它拉取到的<strong>最新 Offset</strong> 提交上去。</li><li><strong>优点：</strong> 非常简单，用户基本无感知，不用自己写提交代码。</li><li><strong>缺点（非常致命）：</strong> <strong>时机不精确，可能导致消息丢失或重复消费。</strong><ul><li><strong>场景1（重复消费）：</strong> 你的程序在 3 秒内处理完了 100 条消息，但还没到 5 秒的提交时间点，你的程序突然崩溃了。重启后，由于上次的进度没有被提交，它只能从 100 条消息之前的位置开始重新消费，导致这 100 条消息被重复处理。</li><li><strong>场景2（消息丢失，更危险）：</strong> 你的程序拉取了 100 条消息，还没开始处理，5 秒的提交时间就到了，客户端自动把这 100 条消息的进度提交了。紧接着，你的程序在处理第 10 条消息时崩溃了。重启后，由于进度已经被提交，它会从第 101 条消息开始消费，导致第 10 到 100 条消息永远没有被处理，造成了数据丢失。</li></ul></li></ul></li><li><strong>方式二：手动提交 (Manual Commit)</strong><ul><li><strong>工作方式：</strong> 在配置中设置 <code>enable.auto.commit=false</code>，然后在你的代码逻辑中，<strong>在你确认消息已经被业务逻辑完全成功处理之后</strong>（比如，数据已经成功写入数据库），再手动调用提交方法来更新 Offset。</li><li><strong>优点：</strong> <strong>控制权完全在开发者手中，非常可靠。</strong> 你可以确保只有当业务逻辑成功完成后，才去移动“书签”。这极大地保证了消息处理的准确性。</li><li><strong>常用的手动提交方法：</strong><ul><li><code>commitSync()</code> (同步提交): 它会阻塞程序，直到位移被成功提交才继续往下走。如果提交失败会一直重试。简单可靠，但会牺牲一点性能。</li><li><code>commitAsync()</code> (异步提交): 它不会阻塞程序，提交请求后立刻返回。性能更高，但如果提交失败，它不会自动重试，需要开发者在回调函数中自行处理。</li></ul></li></ul></li></ul><h3 id="1-解构消费者的位置指针：四个核心-Offset"><a href="#1-解构消费者的位置指针：四个核心-Offset" class="headerlink" title="1. 解构消费者的位置指针：四个核心 Offset"></a><strong>1. 解构消费者的位置指针：四个核心 Offset</strong></h3><p>为了精确控制消费，我们需要了解消费者在工作时涉及的四种“位移”或“指针”：</p><ul><li><strong>Committed Offset (已提交位移):</strong><ul><li><strong>定义：</strong> 这是我们之前讨论的“书签”。它被持久化地存储在 Kafka 的 <code>__consumer_offsets</code> 主题中。它代表消费者组<strong>确认已经成功处理完成</strong>的最后一个 Offset + 1。</li><li><strong>作用：</strong> 这是消费者<strong>故障恢复的依据</strong>。当消费者重启或 Rebalance 后，它会从这个位置开始消费。</li></ul></li><li><strong>Current Offset &#x2F; Position (当前消费位置):</strong><ul><li><strong>定义：</strong> 这是一个<strong>内存中</strong>的指针，代表消费者<strong>下一次调用 <code>poll()</code> 方法时应该获取的消息的起始位置</strong>。</li><li><strong>关系：</strong> 消费者调用 <code>poll()</code> 拉取一批数据后，这个 <code>Position</code> 指针就会向前移动。所以，<code>Position</code> 总是领先于 <code>Committed Offset</code>。(<code>Position</code> &gt;&#x3D; <code>Committed Offset</code>)</li></ul></li><li><strong>Log End Offset (LEO，日志末端位移):</strong><ul><li><strong>定义：</strong> 这不属于消费者，而是<strong>分区（Partition）本身的属性</strong>。它代表该分区中<strong>下一条待写入消息</strong>的 Offset。可以理解为分区当前“最末尾”的位置。</li><li><strong>作用：</strong> 消费者通过比较自己的 <code>Committed Offset</code> 和分区的 <code>LEO</code>，可以计算出<strong>消费延迟 (Lag)</strong>，即还有多少消息没被消费。<code>Lag = LEO - Committed Offset</code>。</li></ul></li><li><strong>Start Offset &#x2F; Beginning Offset (起始位移):</strong><ul><li><strong>定义：</strong> 这也是<strong>分区本身的属性</strong>，代表该分区中<strong>现存的最旧的一条消息</strong>的 Offset。</li><li><strong>作用：</strong> 通常是 0，但如果因为数据保留策略删除了旧数据，这个值就会变大。</li></ul></li></ul><h3 id="2-auto-offset-reset：什么时候-Kafka-会‘找不到-Offset’？"><a href="#2-auto-offset-reset：什么时候-Kafka-会‘找不到-Offset’？" class="headerlink" title="2. auto.offset.reset：什么时候 Kafka 会‘找不到 Offset’？"></a><strong>2. <code>auto.offset.reset</code>：<strong>什么时候 Kafka 会‘找不到 Offset’</strong>？</strong></h3><p><strong>触发时机（找不到 Offset 的两种情况）：</strong></p><ol><li><strong>全新的消费者组：</strong> 一个全新的 <code>group.id</code> 第一次启动，它在 Kafka 中没有任何历史记录，自然也就没有已提交的 <code>Committed Offset</code>。</li><li><strong>位移已过期：</strong> 消费者提交了 Offset（比如 500），然后下线了很长时间（例如一周）。在这一周里，Kafka 的数据保留策略已经将旧数据删除，现在分区里最老的 Start Offset 已经是 1000 了。当这个消费者再次上线时，它想从 500 开始消费，但这个位置的数据早已不存在，因此“找不到 Offset”。</li></ol><p>当这些情况发生时，<code>auto.offset.reset</code> 参数就决定了消费者的行为：</p><ul><li><code>earliest</code>：“从头读起”。消费者会从该分区当前可用的<strong>最早的 Offset</strong> (Start Offset) 开始消费；但是需要注意的是，earliest 会存在重复处理的可能性。<ul><li>情况一：正常下线（先 Commit，后下线）<ul><li><strong>处理完成：</strong> 消费者处理完一批消息（比如到 Offset 150）。</li><li><strong>成功提交：</strong> 调用 <code>commitSync()</code> 或 <code>commitAsync()</code> 成功，将“书签”更新到 151。</li><li><strong>服务下线：</strong> 程序正常关闭。</li><li><strong>重新上线：</strong> 程序重启后，去 Kafka 读取“书签”，发现是 151。</li><li><strong>结果：</strong> 从 Offset 151 开始消费，一切正常，<strong>没有重复，也没有遗漏</strong>。</li></ul></li><li>情况二：异常下线（来不及 Commit 就下线）<ul><li><strong>处理完成：</strong> 消费者处理完一批消息（比如到 Offset 150），数据已经写入数据库。</li><li><strong>提交前崩溃：</strong> 在更新“书签”到 151 <strong>之前</strong>，程序因意外（如服务器断电、进程被强制杀死）而崩溃。</li><li><strong>重新上线：</strong> 程序重启后，去 Kafka 读取“书签”，发现书签还停留在<strong>上一次成功提交的位置</strong>（比如 Offset 100）。</li><li><strong>结果：</strong> 消费者只能从旧的“书签”位置 100 开始重新拉取数据。因此，它会<strong>再一次</strong>处理 100 到 150 的消息，这就造成了<strong>重复处理</strong>。</li></ul></li></ul></li><li><code>latest</code>：（默认值）“只读新的”。消费者会直接跳到分区的<strong>最末尾</strong> (Log End Offset) 开始消费。它会忽略掉所有在它启动之前就已经存在的数据。</li><li><code>none</code>：“直接报错”。如果找不到有效的 Offset，消费者会直接抛出 <code>NoOffsetForPartitionException</code> 异常，将问题交给程序自己处理。</li></ul><h3 id="3-如何选择-auto-offset-reset-的值？——-一个基于业务场景的决策指南"><a href="#3-如何选择-auto-offset-reset-的值？——-一个基于业务场景的决策指南" class="headerlink" title="3. 如何选择 auto.offset.reset 的值？—— 一个基于业务场景的决策指南"></a><strong>3. 如何选择 <code>auto.offset.reset</code> 的值？—— 一个基于业务场景的决策指南</strong></h3><p>选择哪个值，取决于你的应用<strong>能否容忍数据丢失</strong>。</p><p><strong>1. 选择 <code>latest</code> (默认值)</strong></p><ul><li><strong>适合的业务场景：</strong><ul><li>实时监控和告警系统</li><li>展示最新状态的仪表盘 (Dashboard)</li><li>对时效性要求极高，但对历史数据完整性不敏感的应用</li></ul></li><li><strong>决策逻辑：</strong><br>这类应用的核心是“<strong>关注当下</strong>”。如果系统中断了一小时，它最关心的是尽快跟上当前的实时数据流，而不是回头去处理一小时前的旧告警。因此，它选择“忽略历史，从最新的开始”，这是最合理的。</li><li><strong>需要承担的风险：潜在的数据丢失风险。</strong> 如果服务因为任何原因（无论是新加入还是位移过期）发生了位移重置，它会<strong>永久性地跳过</strong>所有在它离线期间产生的数据。</li></ul><p><strong>2. 选择 <code>earliest</code></strong></p><ul><li><strong>适合的业务场景：</strong><ul><li>数据同步任务（例如，将数据从 Kafka 同步到数据库或数据仓库）</li><li>离线数据分析和报表系统</li><li>所有<strong>数据完整性</strong>至关重要的核心业务系统</li></ul></li><li><strong>决策逻辑：</strong><br>这类应用的核心是“<strong>一条都不能少</strong>”。数据的任何缺失都会导致最终结果的错误。因此，即使代价是可能需要处理大量历史数据，也必须选择从最早的位置开始，以确保数据的完整性。</li><li><strong>需要承担的风险：潜在的数据重复处理和启动延迟。</strong> 如果一个全新的消费组错误地配置为 <code>earliest</code>，它可能会试图从分区中现存的最早（可能是几天甚至几周前）的数据开始消费，这可能不是预期的行为，并可能给下游系统带来巨大压力。</li></ul><p><strong>3. 选择 <code>none</code></strong></p><ul><li><strong>适合的业务场景：</strong><ul><li>极其敏感的金融交易或审计系统</li><li>你希望对位移丢失的情况有完全的、自定义的控制权</li></ul></li><li><strong>决策逻辑：</strong><br>这类应用的核心是“<strong>杜绝任何不确定性</strong>”。<code>earliest</code> 和 <code>latest</code> 都是一种“猜测”，而 <code>none</code> 则拒绝猜测。它会直接抛出异常，强制程序或运维人员介入，搞清楚到底发生了什么，然后再手动决定从哪里开始消费。这是最安全、但自动化程度最低的选择。</li><li><strong>需要承担的代价：</strong><br>需要编写额外的异常处理代码，并可能需要人工介入，降低了系统的自动化程度，但换来了最高的确定性。</li></ul><hr><p><strong>结论与经验法则</strong></p><p>所以，不存在“最好”的设置，只有“<strong>最适合你的业务场景</strong>”的设置。</p><ul><li><strong>经验法则 (Rule of Thumb):</strong><ul><li>如果你的应用<strong>能容忍数据丢失</strong>，但需要尽快跟上实时流，用 <code>latest</code>。</li><li>如果你的应用<strong>绝不能丢失任何数据</strong>，用 <code>earliest</code>，并为可能处理大量历史数据做好准备。</li><li>如果你的应用处理的是<strong>极其敏感的数据</strong>，任何不确定性都需要报警并由人来决策，用 <code>none</code>。</li></ul></li></ul><h3 id="4-会话管理与-max-poll-interval-ms：“你还活着吗？”"><a href="#4-会话管理与-max-poll-interval-ms：“你还活着吗？”" class="headerlink" title="4. 会话管理与 max.poll.interval.ms：“你还活着吗？”"></a><strong>4. 会话管理与 <code>max.poll.interval.ms</code>：“你还活着吗？”</strong></h3><p>Kafka 有一套心跳机制来判断消费者是否存活。</p><ul><li><strong><code>session.timeout.ms</code> (会话超时):</strong> Broker（组协调器）会认为，如果一个消费者在这个时间内没有发送任何心跳，那么它就死了，会触发 Rebalance。</li><li><strong><code>heartbeat.interval.ms</code> (心跳间隔):</strong> 消费者客户端在后台会按照这个间隔，持续地向 Broker 发送心跳。这个值必须远小于 <code>session.timeout.ms</code>。</li></ul><p><strong>但是，心跳只能证明消费者的进程还活着，万一它卡在业务逻辑里（比如死循环），无法继续处理消息怎么办？</strong></p><p>这就是 <code>max.poll.interval.ms</code> 的作用。</p><ul><li><strong><code>max.poll.interval.ms</code> (处理时长上限):</strong> 这个参数定义了<strong>两次调用 <code>consumer.poll()</code> 方法之间的最大时间间隔</strong>。消费者的业务逻辑必须在这个时间内处理完并返回，再次调用 <code>poll()</code>。如果超过了这个时间，消费者客户端会主动认为自己“失联”，并离开消费组，同样也会触发 Rebalance。这可以防止“假活”的僵尸消费者拖垮整个消费进度。</li></ul><h3 id="4-如何避免长时间处理导致消费者被踢出？"><a href="#4-如何避免长时间处理导致消费者被踢出？" class="headerlink" title="4. 如何避免长时间处理导致消费者被踢出？"></a><strong>4. 如何避免长时间处理导致消费者被踢出？</strong></h3><p>这是 <code>max.poll.interval.ms</code> 带来的实际问题：如果我的业务逻辑确实很耗时，超过了默认的5分钟怎么办？</p><ol><li><strong>简单粗暴：调大参数。</strong> 直接增加 <code>max.poll.interval.ms</code> 的值。但这会延长真正检测到消费者死亡的时间，不是最佳方案。</li><li><strong>减少单次处理量：</strong> 调小 <code>max.poll.records</code> 参数，让每次 <code>poll()</code> 返回更少的消息，这样你的处理循环就能更快地结束。</li><li><strong>最佳实践：解耦处理（异步化）。</strong><ul><li>创建一个专门的 Kafka 消费线程，它的<strong>唯一工作</strong>就是快速地调用 <code>poll()</code>，将拉取到的消息塞进一个内存队列（例如 <code>BlockingQueue</code>）。这个线程的循环非常快，永远不会超时。</li><li>另外创建<strong>一个或多个工作线程池</strong>，由它们从内存队列中取出消息，慢慢地执行耗时的业务逻辑。</li><li><strong>注意：</strong> 这种模式下，Offset 管理变得更复杂。你必须在工作线程<strong>真正处理完</strong>数据后，再由消费线程去手动提交对应的 Offset。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 命令完整笔记</title>
      <link href="/2024/08/23/git-bi-ji/"/>
      <url>/2024/08/23/git-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Git-配置"><a href="#一、Git-配置" class="headerlink" title="一、Git 配置"></a>一、Git 配置</h2><ol><li><p><strong>配置用户信息</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git config --global user.name "Your Name"git config --global user.email "your.email@example.com"# 查看配置git config --listgit config user.name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>配置别名</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git config --global alias.co checkoutgit config --global alias.br branchgit config --global alias.ci commitgit config --global alias.st statusgit config --global alias.lg "log --oneline --graph --decorate"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>配置编辑器和工具</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git config --global core.editor "vim"git config --global merge.tool vimdiffgit config --global diff.tool vimdiff<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="二、基础操作"><a href="#二、基础操作" class="headerlink" title="二、基础操作"></a>二、基础操作</h2><ol><li><p><strong>初始化仓库</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git init                    # 在当前目录初始化git init my-project        # 创建目录并初始化<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>克隆仓库</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git clone https://github.com/user/repo.gitgit clone -b develop https://github.com/user/repo.git    # 克隆特定分支git clone --depth 1 https://github.com/user/repo.git     # 浅克隆，只克隆最新提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看状态</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git status                  # 查看工作区状态git status -s              # 简短输出<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>添加文件到暂存区</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git add file.txt           # 添加特定文件git add .                  # 添加所有更改git add *.js              # 添加所有JS文件git add -p                 # 交互式添加文件块git add -u                 # 添加已跟踪文件的更改<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>提交更改</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git commit -m "Commit message"              # 提交带消息git commit -am "Add and commit message"     # 添加并提交所有更改git commit --amend                          # 修改最后一次提交git commit --amend --no-edit                # 修改但不更改提交信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="三、远程仓库操作"><a href="#三、远程仓库操作" class="headerlink" title="三、远程仓库操作"></a>三、远程仓库操作</h2><ol><li><p><strong>设置远程仓库的URL</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git remote set-url origin https://github.com/your-username/xiaohub.gitgit remote add upstream https://github.com/original/repo.git    # 添加上游仓库git remote -v                                                   # 查看远程仓库git remote show origin                                          # 查看远程仓库详情git remote rename origin old-origin                             # 重命名远程仓库git remote remove old-origin                                    # 删除远程仓库<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>拉取远程更改</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git fetch origin           # 从远程获取最新，但不合并git pull origin master     # 拉取并合并git pull --rebase         # 拉取并变基<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>推送更改到GitHub</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git push origin master                 # 推送到远程master分支git push -u origin feature-branch      # 推送并设置上游git push origin --tags                 # 推送所有标签git push origin v1.0.0                 # 推送特定标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>强制推送到远程仓库</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git push origin --force --all          # 强制推送所有分支git push origin --force master         # 强制推送特定分支git push --force-with-lease           # 更安全的强制推送<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="四、分支管理"><a href="#四、分支管理" class="headerlink" title="四、分支管理"></a>四、分支管理</h2><ol><li><p><strong>创建和切换分支</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git branch                    # 查看本地分支git branch -a                 # 查看所有分支（包括远程）git branch feature-login      # 创建新分支git checkout feature-login    # 切换分支git checkout -b feature-new   # 创建并切换分支git switch -c feature-new     # 新版本创建并切换<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>合并分支</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git merge feature-branch      # 合并分支git merge --no-ff feature     # 禁用快进合并git merge --squash feature    # 压缩合并<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>变基操作</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git rebase master            # 变基到mastergit rebase -i HEAD~3         # 交互式变基最近3个提交git rebase --continue        # 解决冲突后继续变基git rebase --abort           # 放弃变基<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>删除分支</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git branch -d feature-branch          # 删除已合并分支git branch -D feature-branch          # 强制删除分支git push origin --delete feature      # 删除远程分支<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="五、文件管理"><a href="#五、文件管理" class="headerlink" title="五、文件管理"></a>五、文件管理</h2><ol><li><p><strong>从Git仓库中删除文件（可以使用 <code>-f</code>（force）选项来强制移除这些文件的跟踪）</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git rm --cached <file>                # 从索引中删除，保留工作区文件git rm -r --cached <directory>        # 递归删除目录git rm file.txt                       # 删除文件（同时从工作区删除）git rm --cached -r node_modules       # 移除node_modules的跟踪<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>移动或重命名文件</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git mv oldname.txt newname.txt        # 重命名文件git mv file.txt directory/            # 移动文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>.gitignore文件</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 示例.gitignore文件内容*.lognode_modules/.env.DS_Store# 清除已跟踪但现在要忽略的文件git rm -r --cached .git add .git commit -m "Update .gitignore"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="六、提交历史管理"><a href="#六、提交历史管理" class="headerlink" title="六、提交历史管理"></a>六、提交历史管理</h2><ol><li><p><strong>回滚到之前的提交</strong></p><ul><li><p><strong>使用 <code>git reset --hard HEAD^</code></strong>: 这个命令将撤销最后一次提交，并且丢弃所有的更改。请注意，这将彻底删除最后一次提交的所有更改，所以请确保你没有需要保留的其他更改。</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git reset --hard HEAD^git reset --hard HEAD~2         # 回退2个提交git reset --hard commit-hash    # 回退到特定提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>使用 <code>git reset --soft HEAD^</code></strong>: 如果你想保留更改但撤销提交，可以使用这个命令。这将把所有的更改放回暂存区，你可以重新编辑并再次提交。</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git reset --soft HEAD^git reset --soft HEAD~1git reset --mixed HEAD^         # 默认选项，保留工作区更改，重置暂存区<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><strong>撤销更改</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git checkout -- file.txt        # 撤销工作区更改git restore file.txt            # 新版本撤销更改git clean -fd                   # 删除未跟踪的文件和目录git clean -n                    # 查看将要删除的文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>回滚已推送的提交</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git revert HEAD                 # 创建一个新提交来撤销最后的提交git revert commit-hash          # 撤销特定提交git revert -n HEAD~3..HEAD      # 撤销多个提交但不自动提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="七、查看日志和差异"><a href="#七、查看日志和差异" class="headerlink" title="七、查看日志和差异"></a>七、查看日志和差异</h2><ol><li><p><strong>检查Git历史记录</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git log --all -- <file>              # 查看文件的所有历史git log --graph --oneline --all      # 图形化显示所有分支git log --stat                       # 显示文件修改统计git log --patch                      # 显示具体修改内容git log --author="John"              # 按作者查找提交git log --grep="fix"                 # 按提交信息查找git log --since="2 weeks ago"        # 查看最近两周的提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看最近的提交</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git log -n 10                        # 查看最近10次提交git log --oneline -5                 # 简短格式显示最近5次提交git log --pretty=format:"%h - %an, %ar : %s"   # 自定义格式<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看差异</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git diff                             # 查看工作区和暂存区差异git diff --staged                    # 查看暂存区和最新提交差异git diff HEAD                        # 查看工作区和最新提交差异git diff branch1..branch2            # 比较两个分支git diff commit1 commit2             # 比较两个提交git diff --name-only                 # 只显示更改的文件名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="八、标签管理"><a href="#八、标签管理" class="headerlink" title="八、标签管理"></a>八、标签管理</h2><ol><li><p><strong>创建标签</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git tag v1.0.0                       # 创建轻量标签git tag -a v1.0.0 -m "Version 1.0"   # 创建带注释的标签git tag v1.0.0 commit-hash           # 给特定提交打标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看和管理标签</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git tag                              # 列出所有标签git tag -l "v1.*"                    # 列出匹配的标签git show v1.0.0                      # 查看标签信息git tag -d v1.0.0                    # 删除本地标签git push origin --delete v1.0.0      # 删除远程标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="九、储藏和清理"><a href="#九、储藏和清理" class="headerlink" title="九、储藏和清理"></a>九、储藏和清理</h2><ol><li><p><strong>储藏工作区</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git stash                            # 储藏当前更改git stash save "work in progress"    # 带描述的储藏git stash list                       # 列出所有储藏git stash pop                        # 应用并删除最新储藏git stash apply stash@&#123;2&#125;            # 应用特定储藏git stash drop stash@&#123;1&#125;             # 删除特定储藏git stash clear                      # 清空所有储藏<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>清理工作区</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git clean -n                         # 查看将被清理的文件git clean -f                         # 清理未跟踪文件git clean -fd                        # 清理文件和目录git clean -xfd                       # 清理包括.gitignore忽略的文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十、查找和调试"><a href="#十、查找和调试" class="headerlink" title="十、查找和调试"></a>十、查找和调试</h2><ol><li><p><strong>定位Bug引入</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git bisect start                     # 开始二分查找git bisect bad                       # 标记当前提交为有buggit bisect good commit-hash          # 标记某个提交为正常git bisect reset                     # 结束查找<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看文件每行最后修改</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git blame file.txt                   # 查看文件每行的最后修改信息git blame -L 10,20 file.txt         # 查看特定行范围<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h2 id="十一、Git工作流"><a href="#十一、Git工作流" class="headerlink" title="十一、Git工作流"></a>十一、Git工作流</h2><ol><li><p><strong>特性分支工作流</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 创建特性分支git checkout -b feature/login# 开发完成后git add .git commit -m "Add login feature"# 更新主分支git checkout mastergit pull origin master# 合并特性分支git merge --no-ff feature/logingit push origin master# 删除特性分支git branch -d feature/login<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>GitFlow工作流</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 创建开发分支git checkout -b develop master# 创建功能分支git checkout -b feature/login develop# 完成功能分支git checkout developgit merge --no-ff feature/logingit branch -d feature/login# 创建发布分支git checkout -b release/1.0.0 develop# 完成发布git checkout mastergit merge --no-ff release/1.0.0git tag -a v1.0.0git checkout developgit merge --no-ff release/1.0.0git branch -d release/1.0.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十二、高级操作"><a href="#十二、高级操作" class="headerlink" title="十二、高级操作"></a>十二、高级操作</h2><ol><li><p><strong>子模块管理</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git submodule add https://github.com/user/repo.git path/to/submodulegit submodule initgit submodule updategit submodule update --remotegit submodule foreach git pull origin master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>获取指定文件版本</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git show HEAD:file.txt               # 查看最新版本文件git show branch:file.txt             # 查看特定分支文件git checkout commit-hash -- file.txt  # 恢复特定版本文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>导出归档</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git archive --format=zip HEAD > archive.zipgit archive --format=tar.gz --prefix=project/ v1.0.0 > project.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h2 id="十三、常见问题解决"><a href="#十三、常见问题解决" class="headerlink" title="十三、常见问题解决"></a>十三、常见问题解决</h2><ol><li><p><strong>解决合并冲突</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 出现冲突后git status                  # 查看冲突文件# 编辑冲突文件git add resolved-file.txtgit commit -m "Resolve merge conflict"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>修改历史提交</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 修改最近N个提交git rebase -i HEAD~3# 在编辑器中将pick改为edit, reword, squash等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>找回丢失的提交</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">git reflog                   # 查看所有操作记录git checkout commit-hash     # 恢复到特定提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h2 id="十四、Git别名和快捷命令"><a href="#十四、Git别名和快捷命令" class="headerlink" title="十四、Git别名和快捷命令"></a>十四、Git别名和快捷命令</h2><ol><li><strong>常用别名设置</strong><pre class="line-numbers language-language-bash"><code class="language-language-bash">git config --global alias.co checkoutgit config --global alias.br branchgit config --global alias.ci commitgit config --global alias.st statusgit config --global alias.unstage 'reset HEAD --'git config --global alias.last 'log -1 HEAD'git config --global alias.visual '!gitk'git config --global alias.lg "log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十五、Git最佳实践"><a href="#十五、Git最佳实践" class="headerlink" title="十五、Git最佳实践"></a>十五、Git最佳实践</h2><ol><li><p><strong>提交信息规范</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 好的提交信息feat: 添加用户登录功能fix: 修复导航栏在移动端显示异常docs: 更新README文件style: 格式化代码refactor: 重构用户模块test: 添加用户登录单元测试chore: 更新依赖版本<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>保持提交原子性</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 每个提交只做一件事git add file1.jsgit commit -m "Add user validation"git add file2.jsgit commit -m "Add password encryption"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>定期推送和拉取</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 每天工作开始git pull origin develop# 完成功能后git push origin feature-branch<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch笔记</title>
      <link href="/2024/08/20/elasticsearch-bi-ji/"/>
      <url>/2024/08/20/elasticsearch-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="ElasticSearch-复习"><a href="#ElasticSearch-复习" class="headerlink" title="ElasticSearch 复习"></a>ElasticSearch 复习</h1><h1 id="什么是ElasticSearch"><a href="#什么是ElasticSearch" class="headerlink" title="什么是ElasticSearch"></a>什么是ElasticSearch</h1><p>ElasticSearch是一个基于Lucene的开源分布式搜索引擎，由Elastic公司开发。它具有以下特点：</p><h2 id="核心特性"><a href="#核心特性" class="headerlink" title="核心特性"></a>核心特性</h2><ul><li><strong>分布式搜索引擎</strong>：基于Lucene，提供分布式的全文搜索功能</li><li><strong>RESTful API</strong>：通过HTTP使用JSON进行数据交互</li><li><strong>实时分析</strong>：支持实时数据分析</li><li><strong>高可用性</strong>：分布式架构确保无单点故障</li><li><strong>可扩展性</strong>：可以从小规模扩展到PB级数据</li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li><strong>站内搜索</strong>：网站或应用内的搜索功能</li><li><strong>日志分析</strong>：与Logstash和Kibana组成ELK栈，用于日志收集和分析</li><li><strong>数据分析</strong>：结构化数据的快速分析</li><li><strong>全文检索</strong>：文档、商品描述等全文内容的检索</li><li><strong>监控系统</strong>：用于系统性能指标的存储和分析</li></ul><h2 id="主要概念"><a href="#主要概念" class="headerlink" title="主要概念"></a>主要概念</h2><table><thead><tr><th>Elasticsearch 概念</th><th>关系型数据库 (MySQL) 类比</th><th>一句话解释</th></tr></thead><tbody><tr><td><strong>Index (索引)</strong></td><td><code>Database</code> (数据库)</td><td>一个存放一类数据的“库”。</td></tr><tr><td><strong>Document (文档)</strong></td><td><code>Row</code> (一行数据)</td><td>一条可被搜索的记录。</td></tr><tr><td><strong>Field (字段)</strong></td><td><code>Column</code> (一列)</td><td>一条记录中的一个数据项。</td></tr><tr><td><strong>Mapping (映射)</strong></td><td><code>Schema</code> (表结构定义)</td><td>定义每个字段类型的规则。</td></tr><tr><td><strong>Cluster (集群)</strong></td><td>整个数据库服务</td><td>协同工作的整个 ES 服务。</td></tr><tr><td><strong>Node (节点)</strong></td><td>一个数据库服务器实例</td><td>集群中的一台服务器。</td></tr></tbody></table><p><strong>1. Document (文档)</strong></p><ul><li><strong>是什么：</strong> Elasticsearch 中最<strong>基本的、可被索引的信息单元</strong>。</li><li><strong>类比：</strong> 数据库表中的一<strong>行 (Row)</strong> 数据。</li><li><strong>解释：</strong> 它是一个用 JSON 格式表示的数据对象。你存入 ES 的每一条商品信息、每一篇博客、每一条日志，都是一个独立的文档。这是你进行搜索和分析的最小对象。</li></ul><p><strong>示例：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json">&#123;  "product_id": "A-123-B-456",  "title": "华为 Mate 60 Pro",  "price": 6999.00&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2. Field (字段)</strong></p><ul><li><strong>是什么：</strong> 组成文档的一个个<strong>键值对 (Key-Value Pair)</strong>。</li><li><strong>类比：</strong> 数据库表中的一<strong>列 (Column)</strong>。</li><li><strong>解释：</strong> 每个字段都有一个特定的数据类型（比如 <code>text</code> 用于全文搜索，<code>keyword</code> 用于精确值，<code>integer</code> 用于整数，<code>date</code> 用于日期等）。你在 Mapping 里定义的就是每个字段的类型和规则。</li></ul><p><strong>示例：</strong> 在上面的文档中，<code>&quot;title&quot;: &quot;华为 Mate 60 Pro&quot;</code> 就是一个字段。<code>title</code> 是字段名，<code>&quot;华为 Mate 60 Pro&quot;</code> 是字段值。</p><p><strong>3. Index (索引)</strong></p><ul><li><strong>是什么：</strong> 一个<strong>文档的集合</strong>。它拥有相似的特征，是 ES 中数据管理的最高层级单位。</li><li><strong>类比：</strong> 一个<strong>数据库 (Database)</strong>。</li><li><strong>解释：</strong> 比如，你可以创建一个 <code>products</code> 索引来存放所有商品文档，再创建一个 <code>logs</code> 索引来存放所有日志文档。我们所有的查询、更新、删除操作，都是针对一个或多个索引来进行的。索引的名称必须是小写。</li></ul><p><strong>4. Shard (分片)</strong></p><ul><li><strong>是什么：</strong> <strong>索引的物理拆分</strong>。每个分片都是一个功能齐全、独立的“子索引”。</li><li><strong>类比：</strong> 把一个巨大的数据库表<strong>水平分区 (Partitioning)</strong> 成多个小表。</li><li><strong>解释：</strong> 这是 Elasticsearch 实现<strong>水平扩展和高并发</strong>的核心。当一个索引的数据量太大，单个节点存不下或处理不过来时，ES 会将这个索引拆分成多个分片，并将这些分片分布到不同的<strong>节点</strong>上。这样，一个查询请求可以同时在多个分片上并行执行，极大地提升了处理能力。<ul><li><strong>主分片 (Primary Shard</strong>)： 索引的每个文档都只属于一个主分片。主分片的数量在索引创建时就必须固定，之后不能修改。</li></ul></li></ul><p><strong>5. Replica (副本)</strong></p><ul><li><strong>是什么：</strong> <strong>分片的一份或多份拷贝</strong>。</li><li><strong>类比：</strong> 数据库的<strong>主从复制</strong>中的“从库”。</li><li><strong>解释：</strong> 副本的主要作用有两个，我们之前也提到过：<ol><li><strong>高可用性 (High Availability)：</strong> 副本和它的主分片永远不会被分配在同一个节点上。如果持有主分片的节点宕机，ES 会立即将一个副本“提升”为新的主分片，保证服务不中断，数据不丢失。</li><li><strong>提升读性能 (Increase Read Throughput)：</strong> 查询请求（读操作）可以由主分片或任何一个副本分片来处理，从而将读请求的压力分摊到更多的节点上。</li></ol></li></ul><p><strong>6. Node (节点)</strong></p><ul><li><strong>是什么：</strong> 集群中的一个<strong>服务器实例</strong>。</li><li><strong>类比：</strong> 一台<strong>运行着 MySQL 实例的服务器</strong>。</li><li><strong>解释：</strong> 它是构成集群的单个成员，负责存储数据、参与索引和搜索。每个节点都有自己的名字，并通过 <code>cluster.name</code> 加入到指定的集群中。我们之前讨论的选举和心跳检测，都是在节点之间进行的。</li></ul><p><strong>7. Cluster (集群)</strong></p><ul><li><strong>是什么：</strong> 由<strong>一个或多个节点</strong>组成的集合。</li><li><strong>类比：</strong> 整个<strong>高可用的数据库集群</strong>。</li><li><strong>解释：</strong> 它将所有节点的数据和计算能力汇集在一起，对外提供统一的服务。你与 ES 的所有交互，都是通过集群中的某个节点进行的。集群负责管理所有索引、分片、副本的健康和分布，确保整个系统的稳定运行。</li></ul><h2 id="为什么说“索引的每个文档都只属于一个主分片。主分片的数量在索引创建时就必须固定，之后不能修改。”"><a href="#为什么说“索引的每个文档都只属于一个主分片。主分片的数量在索引创建时就必须固定，之后不能修改。”" class="headerlink" title="为什么说“索引的每个文档都只属于一个主分片。主分片的数量在索引创建时就必须固定，之后不能修改。”"></a>为什么说“索引的每个文档都只属于一个主分片。主分片的数量在索引创建时就必须固定，之后不能修改。”</h2><p><strong>我们先来理解第一部分：“索引的每个文档都只属于一个主分片”</strong></p><p>这句话意味着，当你保存一个文档时，Elasticsearch 必须有一个<strong>确定的、可重复的</strong>方法来决定这个文档应该存到哪个主分片里。它不能这次存到分片1，下次又存到分片2，那样就乱套了，永远也找不到数据。</p><p>这个决定过程，就叫做 <strong>“路由” (Routing)</strong>。</p><p><strong>路由是如何工作的？</strong></p><p>Elasticsearch 使用一个非常简单的公式：</p><p><code>shard_number = hash(routing_value) % number_of_primary_shards</code></p><p>我们来解释这个公式里的每一项：</p><ul><li><code>routing_value</code> (路由值)：默认情况下，这个值就是文档的 <strong><code>_id</code></strong>。你也可以手动指定一个值。</li><li><code>hash()</code>：一个哈希函数，它能把任意一个字符串（比如 <code>_id</code>）转换成一个固定的数字。</li><li><code>%</code>：取余数运算符。</li><li><code>number_of_primary_shards</code>：你创建索引时设定的主分片数量。</li></ul><p><strong>举个例子：</strong></p><ol><li>你创建了一个 <code>products</code> 索引，并设定它有 <strong>3 个主分片</strong>。</li><li>现在，你要存入一个新商品文档，它的 <code>_id</code> 是 <strong>“A-123-B-456”</strong>。</li><li>ES 会对 “A-123-B-456” 这个 <code>_id</code> 进行哈希计算，假设得到一个数字 <code>2096</code>。</li><li>然后，ES 计算 <code>2096 % 3</code>，结果是 <code>2</code>。</li><li><strong>结论：</strong> ES 就知道了，这个文档必须被存到<strong>主分片2 (Shard 2)</strong> 上。</li></ol><p>下次你根据这个 <code>_id</code> 来获取或更新这个文档时，ES 会重复一遍完全相同的计算，再次得到 <code>2</code>，然后直接去主分片2上找，而不需要去问分片0和分片1，效率极高。</p><hr><p>现在，我们来理解第二部分，也是最关键的部分：</p><p><strong>“主分片的数量在索引创建时就必须固定，之后不能修改”，为什么不能修改？</strong></p><p>我们继续用上面的例子。假设 ES 允许你把主分片数量从 <strong>3</strong> 修改成 <strong>4</strong>。</p><p>现在，你还是想找那个 <code>_id</code> 是 “A-123-B-456” 的文档。ES 再次执行路由计算：</p><ol><li><code>hash(&quot;A-123-B-456&quot;)</code> 的结果依然是 <code>2096</code>。</li><li>但是，现在主分片数量变成了 4，所以公式变成了 <code>2096 % 4</code>。</li><li>计算结果是 <strong><code>0</code></strong>！</li></ol><p><strong>灾难发生了！</strong></p><p>ES 现在认为这个文档应该在<strong>主分片0 (Shard 0)</strong> 上。但实际上，它当初被存放在了<strong>主分片2</strong>上。ES 去分片0上找，结果肯定是“找不到”。</p><p>如果你修改了主分片的数量，<strong>所有之前文档的路由规则就全部失效了</strong>，整个索引的数据就陷入了彻底的混乱，你将无法通过 <code>_id</code> 定位到任何一个文档。</p><p><strong>这就是为什么主分片数量一旦设定，就不能再修改的根本原因：为了保证路由规则的永久稳定。</strong></p><hr><p><strong>那如果我确实需要扩容怎么办？</strong></p><p>这是一个非常实际的问题。如果你的索引真的因为数据增长需要更多的分片来分担压力，正确的做法不是去修改现有索引，而是使用 <strong><code>Reindex API</code></strong>：</p><ol><li><strong>创建一个新的索引</strong> (比如 <code>products_v2</code>)，并为它设置一个<strong>更多的主分片数量</strong>（比如 6 个）。</li><li>使用 <code>Reindex API</code>，让 Elasticsearch 自动地、高效地将旧索引 (<code>products</code>) 中的所有数据读取出来，并重新写入到新索引 (<code>products_v2</code>) 中。</li><li>在重新写入的过程中，ES 会为每一条数据应用<strong>新的路由规则</strong>（比如 <code>% 6</code>），将它们正确地分布到新的 6 个分片中。</li><li>数据迁移完成后，你可以将你的应用程序指向新的索引 <code>products_v2</code>，然后安全地删除旧索引。</li></ol><h2 id="搜索的流程是什么？"><a href="#搜索的流程是什么？" class="headerlink" title="搜索的流程是什么？"></a>搜索的流程是什么？</h2><p>具体来说，它分为几个关键步骤：</p><ol><li><strong>分析查询 (Analyze Query):</strong> 首先，Elasticsearch 会使用与索引时<strong>相同的分析器 (Analyzer)</strong> 来处理用户的查询语句。例如，用户搜索“华为手机”，分析器会将其处理成词元（Tokens） <code>[&quot;华为&quot;, &quot;手机&quot;]</code>。</li><li><strong>查找词典 (Term Dictionary Lookup):</strong> 接着，系统会拿着处理好的每一个词元（“华为”、“手机”），去那个已经建好的、巨大的<strong>词典 (Term Dictionary)</strong> 中快速查找。</li><li><strong>获取文档列表 (Posting List Retrieval):</strong> 词典本身不存储文档ID，但它会告诉系统去哪里找到包含了这个词元的<strong>文档列表 (Posting List)</strong>。<ul><li>找到“华为” -&gt; 获取到它的文档列表，比如 <code>[Doc1, Doc5, Doc10]</code>。</li><li>找到“手机” -&gt; 获取到它的文档列表，比如 <code>[Doc1, Doc8, Doc10]</code>。</li></ul></li><li><strong>合并与计算 (Merge &amp; Calculate):</strong> 这是非常关键的一步。<ul><li><strong>布尔逻辑：</strong> 系统会对这些文档列表进行布尔运算。对于“华为手机”这个查询，默认是 AND 逻辑，所以它会取两个列表的<strong>交集</strong>，得到最终匹配的文档ID：<code>[Doc1, Doc10]</code>。</li><li><strong>相关性评分 (<code>_score</code>)：</strong> 与此同时（对于 <code>match</code> 查询），ES 还会计算每个匹配文档的<strong>相关性分数</strong>。它会考虑词频（TF，词在一个文档里出现的次数）、逆文档频率（IDF，词在所有文档中是否罕见）等因素。比如，如果 Doc1 的标题就是“华为手机”，而 Doc10 的描述里只提了一句，那么 Doc1 的得分就会更高。</li></ul></li><li><strong>返回结果 (Return Results):</strong> 最后，系统根据 <code>_score</code> 从高到低进行排序，然后根据这些文档 ID 去获取完整的文档内容（<code>_source</code>），并将最终排好序的结果返回给用户。</li></ol><p><strong>总结一下，更精确的说法是：</strong></p><ul><li><strong>直接修改主分片数会导致找不到文档，所以 ES 禁止这样做。</strong></li><li><strong>正确的扩容方式（Reindex）是一个“先建新，再搬家，最后换门牌”的过程，它通过创建全新的索引来应用新的路由规则，从而保证在任何时候都不会出现找不到文档的情况。</strong></li></ul><h2 id="倒排索引-Inverted-Index-的工作原理是什么？"><a href="#倒排索引-Inverted-Index-的工作原理是什么？" class="headerlink" title="倒排索引 (Inverted Index) 的工作原理是什么？"></a>倒排索引 (Inverted Index) 的工作原理是什么？</h2><p>倒排索引是 Elasticsearch 实现快速全文搜索的核心数据结构，它的核心思想是“词到文档”的映射。传统的关系型数据库是“文档到词”，即根据一条记录（文档）找到里面的内容（词），而倒排索引反了过来。</p><p>而像MySQL 和 MongoDB也是做某种“值到记录”的映射，但是跟ElasticSearch有一个根本性的区别，就是<strong>分词</strong>。</p><p><strong>MySQL &#x2F; MongoDB 的标准索引 (通常是 B-Tree 索引)</strong></p><ul><li><strong>索引对象：</strong> 字段的 <strong>完整值 (Entire Value)</strong>。</li><li><strong>工作方式：</strong> 它们为一整个字段的值创建索引。比如你有一个 <code>product_name</code> 字段，值为 “Apple iPhone 15 Pro”。MySQL 会为 <code>&quot;Apple iPhone 15 Pro&quot;</code> 这个<strong>完整的字符串</strong>建立一个索引条目，指向它所在的行。</li><li><strong>查询场景：</strong><ul><li><strong>精确匹配 (<code>=</code>)：</strong> 当你 <code>WHERE product_name = &#39;Apple iPhone 15 Pro&#39;</code> 时，索引效率极高，能瞬间定位。</li><li><strong>前缀匹配 (<code>LIKE &#39;Apple%&#39;</code>)：</strong> B-Tree 索引也能高效支持。</li><li><strong>中间或后缀匹配 (<code>LIKE &#39;%iPhone%&#39;</code>)：</strong> <strong>这正是它的痛点！</strong> 索引完全无法使用，数据库不得不进行全表扫描（或索引扫描），逐行检查 <code>product_name</code> 字段是否包含 “iPhone” 这个子字符串。数据量一大，查询就会变得极慢。</li></ul></li></ul><p><strong>ElasticSearch 的标准索引-倒排索引</strong></p><p>建立倒排索引主要有两个步骤：</p><ol><li><strong>分词</strong>：将文档内容拆分成一个个独立的词（term）。例如，”Elasticsearch is fast” 会被拆分成 “elasticsearch”, “is”, “fast”。</li><li><strong>建立映射关系</strong>：创建一个“词典（term dictionary）”，记录所有文档中出现过的词，并为每个词建立一个列表（Posting List），这个列表包含了所有出现该词的文档ID。</li></ol><p>假设我们有三个文档：</p><ul><li><strong>Doc 1:</strong> “The quick brown fox”</li><li><strong>Doc 2:</strong> “The lazy brown dog”</li><li><strong>Doc 3:</strong> “The quick brown dog and fox”</li></ul><p>经过分词和索引后，得到的倒排索引会是这样的：</p><table><thead><tr><th>词 (Term)</th><th>文档列表 (Posting List)</th></tr></thead><tbody><tr><td>the</td><td>[Doc 1, Doc 2, Doc 3]</td></tr><tr><td>quick</td><td>[Doc 1, Doc 3]</td></tr><tr><td>brown</td><td>[Doc 1, Doc 2, Doc 3]</td></tr><tr><td>fox</td><td>[Doc 1, Doc 3]</td></tr><tr><td>lazy</td><td>[Doc 2]</td></tr><tr><td>dog</td><td>[Doc 2, Doc 3]</td></tr><tr><td>and</td><td>[Doc 3]</td></tr></tbody></table><p><strong>追问：“那当用户搜索 <code>quick brown</code> 时，ES 是怎么做的？”</strong></p><ol><li><strong>查询词典：</strong> ES 在词典中查找 “quick”，得到文档列表 <code>[Doc 1, Doc 3]</code> <strong>(Posting List)</strong> 文件。</li><li><strong>查询词典：</strong> 接着查找 “brown”，得到文档列表 <code>[Doc 1, Doc 2, Doc 3]</code> <strong>(Posting List)</strong> 文件。</li><li><strong>合并结果：</strong> ES 对这两个列表进行交集运算，<code>[Doc 1, Doc 3]</code> 和 <code>[Doc 1, Doc 2, Doc 3]</code> 的交集是 <code>[Doc 1, Doc 3]</code>。</li><li><strong>返回结果：</strong> ES 就知道 Doc 1 和 Doc 3 是符合搜索条件的。</li></ol><p>因为词典通常可以被完全加载到<strong>内存</strong>中，查找速度非常快，而对文档列表的布尔运算（交、并、差）也非常高效，这就是倒排索引能实现秒级甚至毫秒级搜索的根本原因。</p><p>这里说的内存指的是<strong>JVM堆内存</strong>（ElasticSearch是Java应用）和<strong>操作系统文件内存</strong>。</p><p><strong>JVM 堆内存 (JVM Heap)</strong></p><ul><li><strong>它存放什么？</strong><ul><li>Elasticsearch 应用程序本身的运行时数据。</li><li>一些被显式缓存的数据结构，比如 <strong>Filter Cache</strong>。当你在 <code>filter</code> 上下文中使用查询时，其结果（一个包含匹配文档 ID 的位图）会被缓存到这里，以便后续相同的过滤请求能极速返回。</li><li>集群管理、节点通信、聚合计算的中间结果等。</li></ul></li></ul><p><strong>操作系统文件系统缓存 (OS File System Cache &#x2F; Page Cache)</strong></p><ul><li><strong>这是什么？</strong> 这是服务器物理 RAM 中，除了被 JVM 堆等应用程序占用的部分之外，由操作系统内核管理的一部分内存。</li><li><strong>它如何工作？</strong> 现代操作系统（如 Linux）非常智能。当你或一个程序（如 ES）读取磁盘上的文件时，操作系统会把文件的内容复制到这个缓存里。下次你再读取同一个文件时，操作系统会直接从这个高速缓存（RAM）中返回数据，而不是再次去访问慢速的磁盘。这个过程对上层应用是透明的。</li><li><strong>它与 ES 的关系？</strong> 这正是 Elasticsearch 和其底层库 Lucene 设计的精妙之处！Lucene 被设计成能够<strong>极度依赖和善用操作系统的文件系统缓存</strong>。倒排索引的各个部分（词典、Posting List 等）都是以特定格式的文件存储在磁盘上的。当 ES 需要查找一个词时，它会向操作系统请求读取索引文件。由于“热”的索引数据（经常被查询的数据）已经被操作系统自动缓存到了文件系统缓存中，这个“读”操作实际上是在访问内存，速度飞快。</li></ul><p>数据其实还是存在于磁盘上的，只不过是热点数据或者是说查询过的数据就缓存到了系统内存当中，所以后续的查询效率就会增加。</p><ul><li><strong>粒度精细：</strong> 缓存的是构成所有查询的基础“积木块”（词的索引信息），而不是成千上万种可能的查询组合。</li><li><strong>自动智能：</strong> 越是高频的词（比如 <code>我</code>、<code>爱</code>、<code>是</code>、<code>的</code>，或者某个领域的热门词汇），它们对应的索引数据就越会“常驻”在内存中，从而让整个搜索系统的平均性能得到巨大提升。</li></ul><p> 此外，Elasticsearch 全文搜索能够工作的核心匹配原则：<strong>用同样的方式处理输入和查询。</strong></p><p><strong>第一步：存入数据（索引时）</strong></p><ol><li><strong>收到文档：</strong> 你给 Elasticsearch 一个文档，比如 <code>&#123; &quot;title&quot;: &quot;我爱打篮球&quot; &#125;</code>。</li><li><strong>分析和拆解：</strong> Elasticsearch 会查看 <code>title</code> 字段的设置，找到为它配置的分析器（Analyzer）。这个分析器（比如一个标准的中文分词器）接收到 “我爱打篮球” 这个字符串。</li><li><strong>执行分词：</strong> 分析器将字符串拆解成一系列独立的词元（Tokens），也就是 <code>[&quot;我&quot;, &quot;爱&quot;, &quot;打&quot;, &quot;篮球&quot;]</code>。</li><li><strong>写入倒排索引：</strong> Elasticsearch <strong>不会</strong>在倒排索引里记录 “我爱打篮球” 这个原始字符串。它会为<strong>每一个词元</strong>建立索引，将它们分别指向这个文档的ID。最终在倒排索引里形成类似这样的记录：<ul><li><code>我</code> -&gt; <code>[文档ID_1]</code></li><li><code>爱</code> -&gt; <code>[文档ID_1]</code></li><li><code>打</code> -&gt; <code>[文档ID_1]</code></li><li><code>篮球</code> -&gt; <code>[文档ID_1]</code></li></ul></li></ol><p><strong>第二步：进行搜索（查询时）</strong></p><ol><li><strong>收到查询：</strong> 你发起一个 <code>match</code> 查询，搜索 “我爱打篮球”。</li><li><strong>分析和拆解（用同样的方式）：</strong> <code>match</code> 查询是一种<strong>分析查询（Analyzed Query）</strong>。这意味着 Elasticsearch 会对你的<strong>查询词</strong>也执行<strong>完全相同</strong>的分析过程。它会用 <code>title</code> 字段在索引时用的那个<strong>同一个分析器</strong>，将查询词 “我爱打篮球” 也拆解成 <code>[&quot;我&quot;, &quot;爱&quot;, &quot;打&quot;, &quot;篮球&quot;]</code>。</li><li><strong>匹配暗号：</strong> 现在，Elasticsearch 不再是去寻找一个叫 “我爱打篮球” 的大海捞针，而是拿着 <code>[&quot;我&quot;, &quot;爱&quot;, &quot;打&quot;, &quot;篮球&quot;]</code> 这几个拆解后的词元，去倒排索引里查找。<ul><li>它找到所有包含 <code>我</code> 的文档。</li><li>找到所有包含 <code>爱</code> 的文档。</li><li>…以此类推。</li><li>最后，它会计算哪些文档<strong>同时包含</strong>所有这些词元，并根据相关性评分，最终找到 <code>文档ID_1</code> 是最佳匹配。</li></ul></li></ol><h2 id="ES-查询参数的区别"><a href="#ES-查询参数的区别" class="headerlink" title="ES 查询参数的区别"></a>ES 查询参数的区别</h2><h3 id="match-查询：智能的全文搜索"><a href="#match-查询：智能的全文搜索" class="headerlink" title="match 查询：智能的全文搜索"></a><strong><code>match</code> 查询：智能的全文搜索</strong></h3><ul><li><strong>核心功能：</strong> 这是进行<strong>全文搜索</strong>的标准和首选方式。它的目标是找到“相关的”文档，并计算相关性得分 <code>_score</code>。</li><li><strong>是否分析查询词？</strong> <strong>是！</strong> 这是它最重要的特点。它会使用字段在 Mapping 中指定的<strong>同一个分析器</strong>来分析你的查询词。</li><li><strong>工作流程：</strong><ol><li>你搜索 <code>&quot;华为 手机&quot;</code>。</li><li><code>match</code> 查询拿到这个字符串，用分析器将它分词成 <code>[&quot;华为&quot;, &quot;手机&quot;]</code>。</li><li>然后它去倒排索引中查找同时包含这两个词元的文档。</li></ol></li><li><strong>适用场景：</strong> 几乎所有的<strong>搜索框</strong>功能，用户输入的自然语言搜索。最适合用于 <code>text</code> 类型的字段。</li><li><strong>示例：</strong></li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "match": &#123;      "title": "华为 手机"     &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="term-查询：精确的、未经分析的匹配"><a href="#term-查询：精确的、未经分析的匹配" class="headerlink" title="term 查询：精确的、未经分析的匹配"></a><strong><code>term</code> 查询：精确的、未经分析的匹配</strong></h3><ul><li><strong>核心功能：</strong> 查找<strong>未经分析</strong>的、与你输入内容<strong>完全相等</strong>的词元。</li><li><strong>是否分析查询词？</strong> <strong>否！</strong> 这是它与 <code>match</code> 最根本的区别。你给它什么，它就拿着什么去倒排索引里找一模一样的词元。</li><li><strong>“陷阱”说明：</strong><ul><li>假设一个 <code>text</code> 字段的内容是 <code>&quot;Apple iPhone&quot;</code>，经过分析后，索引里存的词元是 <code>[&quot;apple&quot;, &quot;iphone&quot;]</code>（如果是keyword类型就没事了）。</li><li>如果你用 <code>term</code> 查询 <code>&quot;Apple iPhone&quot;</code>，<strong>绝对会失败</strong>，因为索引里根本没有 <code>&quot;Apple iPhone&quot;</code> 这个完整的词元。</li><li>如果你用 <code>term</code> 查询 <code>&quot;Apple&quot;</code>，也<strong>可能会失败</strong>，因为索引里的词元是小写的 <code>&quot;apple&quot;</code>。<code>term</code> 查询是区分大小写的。</li></ul></li><li><strong>适用场景：</strong> 它几乎<strong>不用于 <code>text</code> 字段</strong>。它被广泛用于<strong>精确值</strong>的过滤，比如 <code>keyword</code>（标签、ID）、<code>numeric</code>（数字）、<code>boolean</code>（布尔值）和 <code>date</code> 类型的字段。我们之前在 <code>filter</code> 上下文中用的就是它。</li><li><strong>示例：</strong></li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "term": &#123;      "tags": "卫星通话"  // 假设 tags 是 keyword 类型    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="prefix-查询：“以…开头”的查询"><a href="#prefix-查询：“以…开头”的查询" class="headerlink" title="prefix 查询：“以…开头”的查询"></a><strong><code>prefix</code> 查询：“以…开头”的查询</strong></h3><ul><li><strong>核心功能：</strong> 查找以你指定的<strong>前缀</strong>开头的词元。</li><li><strong>是否分析查询词？</strong> <strong>否！</strong> 它将你输入的字符串作为一个完整的前缀。</li><li><strong>适用场景：</strong> 用于需要“前缀匹配”的场景，比如商品编码、URL、或者简单的输入提示。通常用于 <code>keyword</code> 类型的字段。</li><li><strong>示例：</strong> 查找所有 <code>product_id</code> 以 “A-123” 开头的商品。</li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "prefix": &#123;      "product_id": "A-123"     &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="wildcard-查询：通配符查询"><a href="#wildcard-查询：通配符查询" class="headerlink" title="wildcard 查询：通配符查询"></a><strong><code>wildcard</code> 查询：通配符查询</strong></h3><ul><li><strong>核心功能：</strong> 允许你使用  (匹配任意多个字符) 和 <code>?</code> (匹配一个字符)来进行模式匹配。</li><li><strong>是否分析查询词？</strong> <strong>否！</strong> 它将你输入的带通配符的模式作为一个整体进行匹配。</li><li><strong>适用场景：</strong> 当你不确定部分内容时进行搜索。但因其性能问题，应谨慎使用。</li><li><strong>性能警告：</strong> 这个查询<strong>性能通常较差</strong>，因为它需要遍历词典中大量的词元来寻找匹配项。<strong>尤其要避免将通配符  放在开头</strong>，那将是性能灾难。</li><li><strong>示例：</strong></li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "wildcard": &#123;      "title.raw": "华? Ma*e" // 注意：通常对不分词的 .raw 字段使用    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结对比表"><a href="#总结对比表" class="headerlink" title="总结对比表"></a><strong>总结对比表</strong></h3><table><thead><tr><th>查询类型</th><th>是否分析查询词？</th><th>主要用途</th><th>适用字段类型</th><th>性能</th></tr></thead><tbody><tr><td><strong><code>match</code></strong></td><td><strong>是</strong></td><td>全文搜索、相关性搜索</td><td><code>text</code></td><td><strong>高</strong></td></tr><tr><td><strong><code>term</code></strong></td><td><strong>否</strong></td><td>精确值过滤、完全匹配</td><td><code>keyword</code>, <code>numeric</code>, <code>boolean</code></td><td><strong>非常高</strong></td></tr><tr><td><strong><code>prefix</code></strong></td><td><strong>否</strong></td><td>“以…开头”的查询</td><td><code>keyword</code></td><td><strong>中等</strong></td></tr><tr><td><strong><code>wildcard</code></strong></td><td><strong>否</strong></td><td>通配符模式匹配</td><td><code>keyword</code></td><td><strong>低</strong>（慎用）</td></tr></tbody></table><h3 id="关于-prefix-和-wildcard-的区别，以及为什么-Elasticsearch-要单独提供一个-prefix-查询呢？"><a href="#关于-prefix-和-wildcard-的区别，以及为什么-Elasticsearch-要单独提供一个-prefix-查询呢？" class="headerlink" title="关于 prefix 和 wildcard 的区别，以及为什么 Elasticsearch 要单独提供一个 prefix 查询呢？"></a>关于 <code>prefix</code> 和 <code>wildcard</code> 的区别，以及为什么 Elasticsearch 要单独提供一个 <code>prefix</code> 查询呢？</h3><ul><li><strong><code>prefix</code>：</strong> 是专门用于<strong>前缀匹配</strong>的。</li><li><strong><code>wildcard</code>：</strong> 功能更强大，可以进行任意位置的<strong>模糊匹配</strong>（前缀、后缀、中间）。</li></ul><p>从<strong>功能效果</strong>上来看，使用 <code>wildcard</code> 来做前缀匹配（如 <code>&quot;abc*&quot;</code>) 和使用 <code>prefix</code> 查询（如 <code>&quot;abc&quot;</code>) 得到的结果是<strong>一模一样</strong>的。</p><p>那为什么我们还要用 <code>prefix</code> 呢？</p><p><strong>因为即使是在做完全相同的前缀匹配工作时，<code>prefix</code> 的效率也比 <code>wildcard</code> 更高。</strong></p><p>我们可以用一个“工具箱”的比喻来理解这个性能差异：</p><ul><li><strong><code>prefix</code> 查询</strong> 就像一把 <strong>专门用来拧六角螺丝的、尺寸完全匹配的扳手</strong>。它的设计目标就是做这一件事，因此它的结构简单、贴合度高、发力直接，效率是最高的。</li><li><strong><code>wildcard</code> 查询</strong> 就像一把 <strong>可以调节开口大小的活动扳手</strong>。它非常强大，不仅能拧六角螺丝，还能拧四角螺丝，甚至一些不规则形状的螺母。但是，当你用它来拧那个标准的六角螺丝时，你需要先调节开口、确保卡紧，整个操作过程和内部机械结构总会比专用扳手要稍微复杂和慢一点。</li></ul><p><strong>深入到技术层面，为什么会这样？</strong></p><p>Elasticsearch 底层的 Lucene 搜索引擎对这两种查询的处理方式不同：</p><ol><li><strong><code>prefix</code> 的执行路径（专用通道）</strong>：Lucene 内部的词典是按字母顺序排序的。当它收到一个 <code>prefix</code> 查询时，它有一个高度优化的“捷径”可走。它可以非常快地定位到词典中这个前缀的起始位置，然后简单地顺序向后扫描，直到不符合前缀为止。 这是一个非常直接、开销很小的操作。</li><li><strong><code>wildcard</code> 的执行路径（通用通道）</strong>：当 Lucene 收到一个 <code>wildcard</code> 查询时，它会先将这个通配符模式（即使是简单的 <code>&quot;abc*&quot;</code>）编译成一个更通用的内部状态机（Automaton）。 然后用这个状态机去匹配词典中的词元。虽然对于 <code>&quot;abc*&quot;</code> 这个简单的模式，最终效果和 <code>prefix</code> 一样，但启动和运行这个更“通用”和“强大”的状态机引擎，本身就会带来一点额外的计算开销。</li></ol><h2 id="query-和-filter-的区别"><a href="#query-和-filter-的区别" class="headerlink" title="query 和 filter 的区别"></a><code>query</code> 和 <code>filter</code> 的区别</h2><ol><li><p><strong>核心目的：相关性评分 (Scoring) vs. 是&#x2F;否 (Yes&#x2F;No)</strong></p><p> <strong><code>query</code> 上下文 (Query Context)</strong></p><ul><li><strong>计算相关性分数 (<code>_score</code>)</strong>：<code>query</code> 的主要任务是判断文档与查询的匹配程度，并计算出一个浮点数类型的 <code>_score</code>。分数越高，代表文档越相关。比如，搜索“篮球鞋”，标题中出现 3 次“篮球鞋”的文档会比只出现 1 次的文档得分更高。</li><li><strong>用于全文搜索</strong>：当你需要进行全文搜索，并希望最相关的结果排在最前面时，就应该使用 <code>query</code>。典型的例子是网站的搜索框。</li><li><strong>常见子句</strong>：<code>match</code>, <code>multi_match</code>, <code>query_string</code> 等。</li></ul><p> <strong><code>filter</code> 上下文 (Filter Context)</strong></p><ul><li><strong>不计算分数</strong>：<code>filter</code> 只关心“是”或“否”的匹配，它不会计算 <code>_score</code>。对于 <code>filter</code> 来说，所有匹配的文档都是平等的，没有哪个更“好”。</li><li><strong>用于精确匹配</strong>：当你需要根据精确值进行筛选时，就用 <code>filter</code>。例如，“产品分类是否为‘电子产品’？”、“价格是否小于 100 元？”、“创建日期是否在 2024 年之后？”。</li><li><strong>常见子句</strong>：<code>term</code>, <code>terms</code>, <code>range</code>, <code>exists</code>, <code>bool</code> 查询中的 <code>filter</code> 部分。</li></ul></li><li><p><strong>性能与缓存 (Performance &amp; Caching)</strong></p><p> 这是两者在技术实现上最重要的区别，直接影响查询性能。</p></li></ol><ul><li><strong><code>filter</code>：非常高效，且可以被缓存</strong><ul><li>由于 <code>filter</code> 只做简单的“是&#x2F;否”判断，其计算成本远低于 <code>query</code>。</li><li><strong>可以被高频缓存</strong>：Elasticsearch 会自动缓存 <code>filter</code> 的结果。它会生成一个包含匹配文档的位集（bitset），这个位集可以被存储在内存中（我们之前讨论的 JVM 堆内存中的 Filter Cache）。当下一个查询包含完全相同的 <code>filter</code> 条件时，ES 会直接重用这个缓存的位集，而无需再次扫描倒排索引。这使得 <code>filter</code> 的速度极快。</li></ul></li><li><strong><code>query</code>：计算昂贵，通常不被缓存</strong><ul><li>相关性分数的计算是一个复杂的过程（涉及 TF&#x2F;IDF、BM25 等算法），非常消耗 CPU 资源。</li><li>由于每次查询的上下文都可能不同，导致分数也不同，所以 <code>query</code> 的结果<strong>通常不会被缓存</strong>。每一次执行 <code>query</code> 都需要实时计算。</li></ul></li></ul><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "bool": &#123;      "must": [        &#123; "match": &#123; "description": "轻便 跑鞋" &#125; &#125;  // <--- Query 上下文，需要计算相关性      ],      "filter": [        &#123; "term": &#123; "brand": "Nike" &#125; &#125;,             // <--- Filter 上下文，精确匹配，可缓存        &#123; "range": &#123; "price": &#123; "gte": 500 &#125; &#125; &#125;    // <--- Filter 上下文，范围匹配，可缓存      ]    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>简单来说，就是“<strong>先应用 filter 查询，过滤出来的结果再使用 query 查询</strong>”，它会优先执行 <code>filter</code> 部分，利用缓存快速缩小文档范围，然后再对这个小范围的文档执行 <code>query</code> 部分来进行复杂的相关性计算。</p><p>让我们把 <code>bool</code> 查询想象成一个可以组合不同逻辑条件的容器，它主要有**四种工具（子句）**供您使用：</p><ol><li><code>must</code> (必须匹配)</li><li><code>filter</code> (必须匹配，但更快)</li><li><code>should</code> (可以匹配)</li><li><code>must_not</code> (必须不匹配)</li></ol><p>您可以根据您的需求，任意选用和组合它们。下面我们看几个例子，您就会明白它的灵活性了。</p><hr><p><strong>场景1：纯粹的筛选，不需要相关性得分</strong></p><p><strong>需求：</strong> 找到所有品牌是 “Nike” <strong>并且</strong> 价格低于 800 元的商品。</p><p>在这里，我们不需要评估“有多好”，只需要一个“是&#x2F;否”的列表。所以，我们<strong>只用 <code>filter</code></strong>。</p><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "bool": &#123;      "filter": [        &#123; "term":  &#123; "brand": "Nike" &#125; &#125;,        &#123; "range": &#123; "price": &#123; "lt": 800 &#125; &#125; &#125;      ]    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>分析：</strong> 这个查询效率极高，因为它完全运行在“小筛”（Filter）模式下，可以充分利用缓存。</p><hr><p><strong>场景2：纯粹的相关性搜索</strong></p><p><strong>需求：</strong> 找到描述里<strong>同时包含</strong> “夏季”、“透气”和“跑鞋”的商品，并按相关性排序。</p><p>在这里，我们不关心硬性指标，只关心匹配度。所以，我们<strong>只用 <code>must</code></strong>。</p><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "bool": &#123;      "must": [        &#123; "match": &#123; "description": "夏季" &#125; &#125;,        &#123; "match": &#123; "description": "透气" &#125; &#125;,        &#123; "match": &#123; "description": "跑鞋" &#125; &#125;      ]    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>分析：</strong> 这个查询会为所有匹配的文档计算一个 <code>_score</code>。</p><hr><p><strong>场景3：包含 “或者” (OR) 逻辑的搜索</strong></p><p><strong>需求：</strong> 寻找品牌是 “Nike” <strong>或者</strong> “Adidas” 的商品。</p><p>“或者”逻辑，我们使用 <strong><code>should</code></strong>。当 <code>bool</code> 查询中只有 <code>should</code> 子句时，它表示至少要满足其中一个条件。</p><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "bool": &#123;      "should": [        &#123; "term": &#123; "brand": "Nike" &#125; &#125;,        &#123; "term": &#123; "brand": "Adidas" &#125; &#125;      ],      "minimum_should_match": 1 // 表示should中的条件至少要满足1个    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p><strong>场景4：包含 “不包含” (NOT) 逻辑的搜索</strong></p><p><strong>需求：</strong> 寻找所有 “跑鞋”，但<strong>不能是</strong> “Nike” 品牌的。</p><p>这里我们组合使用 <code>must</code> 和 <code>must_not</code>。</p><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "query": &#123;    "bool": &#123;      "must": [        &#123; "match": &#123; "category": "跑鞋" &#125; &#125;      ],      "must_not": [        &#123; "term": &#123; "brand": "Nike" &#125; &#125;      ]    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>分析：</strong> <code>must_not</code> 也运行在 filter 上下文中，因为它不计算分数，所以效率也很高。</p><h2 id="分析器-Analyzer-是什么"><a href="#分析器-Analyzer-是什么" class="headerlink" title="分析器 (Analyzer)是什么"></a><strong>分析器 (Analyzer)是什么</strong></h2><p><strong>工作原理</strong>：它的工作是把一段原始的、杂乱的文本，加工成一系列规整、统一的、适合搜索的最小单元——<strong>词元 (Token)</strong>。</p><p>固定的流程分别是：</p><ol><li><strong>字符过滤器 (Character Filter)</strong> - <em>预处理，负责“净化”</em></li><li><strong>分词器 (Tokenizer)</strong> - <em>核心工序，负责“拆分”</em></li><li><strong>词元过滤器 (Token Filter)</strong> - <em>后处理，负责“美化”</em></li></ol><p><strong>原始文本：</strong> <code>&lt;p&gt;The 2 QUICK Brown-Foxes jump over the lazy dog&#39;s bone.&lt;/p&gt;</code></p><hr><p><strong>工序 1：字符过滤器 (Character Filter)</strong></p><p>它的任务是在文本被拆分<strong>之前</strong>，对原始字符串进行清理和替换。你可以配置零个或多个字符过滤器。</p><ul><li><strong>常见功能：</strong><ul><li>去除 HTML 标签 (e.g., <code>&lt;p&gt;</code>, <code>&lt;b&gt;</code>)。</li><li>替换特殊字符 (e.g., <code>&amp;</code> 替换成 <code>and</code>)。</li><li>自定义字符映射 (e.g., 将 <code>:)</code> 替换成 <code>_happy_</code>)。</li></ul></li></ul><p><strong>经过【HTML标签去除过滤器】处理后，文本变为：</strong><code>The 2 QUICK Brown-Foxes jump over the lazy dog&#39;s bone.</code></p><hr><p><strong>工序 2：分词器 (Tokenizer)</strong></p><p>这是流水线的核心，它负责将经过“净化”的连续字符串，按照一定的规则<strong>拆分</strong>成独立的词元（Tokens）。<strong>一个分析器里必须有且只能有一个分词器。</strong></p><ul><li><strong>常见分词器：</strong><ul><li><strong>Standard Tokenizer</strong>：默认的分词器，按词的边界（如空格、标点符号）来拆分，对大多数西方语言效果很好。</li><li><strong>Whitespace Tokenizer</strong>：只按空格来拆分。</li><li><strong>Letter Tokenizer</strong>：只保留字母，遇到非字母字符就拆分。</li><li><strong>ik_analyzer</strong>：一个非常流行的开源中文分析器插件，需要单独安装。它基于词典进行分词，支持自定义词典和停用词典，分词粒度更灵活（有 <code>ik_smart</code> 和 <code>ik_max_word</code> 两种模式）</li></ul></li></ul><p><strong>经过【Standard Tokenizer】处理后，我们得到一个词元流：</strong><code>[ The, 2, QUICK, Brown-Foxes, jump, over, the, lazy, dog&#39;s, bone ]</code><em>(注意：<code>Brown-Foxes</code> 和 <code>dog&#39;s</code> 此时还保持原样)</em></p><hr><p><strong>工序 3：词元过滤器 (Token Filter)</strong></p><p>它的任务是在词元被拆分<strong>之后</strong>，对每一个词元进行修改、添加或删除。你可以配置零个或多个词元过滤器，它们会按顺序执行。</p><ul><li><strong>常见功能：</strong><ul><li><strong>Lowercase</strong>: 将所有词元转为小写。</li><li><strong>Stop Words</strong>: 移除常见的、对搜索意义不大的“停用词”（如 <code>the</code>, <code>a</code>, <code>is</code>）。</li><li><strong>Stemming</strong>: 将词语简化为它的词干（e.g., <code>running</code>, <code>ran</code> 都变成 <code>run</code>）。</li><li><strong>Synonym</strong>: 添加同义词 (e.g., 遇到 <code>quick</code>，可以额外添加一个 <code>fast</code> 词元)。</li></ul></li></ul><p><strong>让我们依次应用两个词元过滤器：</strong></p><ol><li><strong>经过【Lowercase Token Filter】处理后，词元流变为：</strong><code>[ the, 2, quick, brown-foxes, jump, over, the, lazy, dog&#39;s, bone ]</code></li><li><strong>经过【Stop Words Token Filter】处理后，词元流变为：</strong><code>[ 2, quick, brown-foxes, jump, over, lazy, dog&#39;s, bone ]</code><em>(两个 <code>the</code> 被移除了)</em></li></ol><p><strong>最终，被写入倒排索引的就是这个最终的词元流。</strong></p><h2 id="用-IK-分析器时，遇到英文会怎样？"><a href="#用-IK-分析器时，遇到英文会怎样？" class="headerlink" title="用 IK 分析器时，遇到英文会怎样？"></a>用 IK 分析器时，遇到英文会怎样？</h2><p><strong>当您为一个字段指定了 <code>ik_analyzer</code>，那么无论这个字段里是中文、英文还是数字，都会由 <code>ik_analyzer</code> 来全权处理。它不会自动切换到 <code>standard</code> 分析器。</strong></p><p>那么，IK 是如何处理英文的呢？</p><p>它的处理逻辑通常是：</p><ol><li><strong>遇到中文字符：</strong> 启用它的核心算法，根据内置的中文词典进行最大匹配或最细粒度的分词。</li><li><strong>遇到英文字符或数字：</strong> 它会切换到一种类似于 <code>standard</code> 分析器的模式。它会把连续的英文字母或数字识别为一个完整的词元（Token），然后根据空格和标点符号进行切分，并且通常会执行转小写的操作。</li></ol><p><strong>举个例子：</strong></p><p>如果你的文本是 <code>&quot;IK分析器 very good&quot;</code></p><p>使用 <code>ik_analyzer</code> 分析后，得到的词元流会是：<br><code>[ik, 分析器, very, good]</code></p><p>你可以看到，它既正确地处理了中文的“分析器”，也正确地处理了英文的“ik”、“very”和“good”。</p><h2 id="如何为中英文提供各自最优的分析？"><a href="#如何为中英文提供各自最优的分析？" class="headerlink" title="如何为中英文提供各自最优的分析？"></a><strong>如何为中英文提供各自最优的分析？</strong></h2><p>虽然 IK 能很好地处理英文，但它毕竟不是专门为英文设计的（比如它缺少英文中很重要的<strong>词干提取&#x2F;Stemming</strong>功能，不能把 <code>running</code> 和 <code>run</code> 视为同一个词）。</p><p>如果想达到极致的效果，最优的方案是使用 <strong>Multi-fields (多字段)</strong>。</p><p>您可以在字段映射（Mapping）中这样设置：</p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /my_index&#123;  "mappings": &#123;    "properties": &#123;      "title": &#123;        "type": "text",        "analyzer": "ik_max_word",         "fields": &#123;          "english": &#123;            "type": "text",            "analyzer": "english"           &#125;        &#125;      &#125;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个设置意味着：</p><ul><li><code>title</code> 字段本身使用 <code>ik_max_word</code> 分析器，最适合处理中文和基本的中英文混合搜索。</li><li>同时，ES 会额外创建一个 <code>title.english</code> 子字段，这个子字段专门使用 <code>english</code> 分析器（ES内置的、带词干提取等高级功能的英文分析器-<strong>Standard Tokenizer</strong>）来处理同样的内容。</li></ul><p><strong>这样使用：</strong></p><ul><li>当你的用户主要用中文搜索时，你查询 <code>title</code> 字段。</li><li>当你判断用户的查询是英文时，你可以去查询 <code>title.english</code> 字段。</li></ul><h2 id="索引设计讲解"><a href="#索引设计讲解" class="headerlink" title="索引设计讲解"></a>索引设计讲解</h2><p>Mapping 就是索引的“模式定义 (Schema)”，它告诉 Elasticsearch 如何处理和索引你文档中的每一个字段。一个精心设计的 Mapping 能带来诸多好处：</p><ul><li><strong>节省存储空间</strong></li><li><strong>提升索引和查询性能</strong></li><li><strong>确保查询的准确性</strong></li></ul><p><strong>场景：一个电商商品 (<code>product</code>) 文档</strong></p><p>我们先看一个商品文档的例子：</p><pre class="line-numbers language-language-json"><code class="language-language-json">&#123;  "product_id": "A-123-B-456",  "title": "华为 Mate 60 Pro 智能手机",  "description": "这是华为最新款的旗舰手机，拥有强大的卫星通话功能和拍照能力。",  "price": 6999.00,  "tags": ["新款", "旗舰", "卫星通话"],  "on_sale": true,  "created_at": "2024-09-10 10:00:00",  "stock_info": &#123;    "warehouse_id": "WH-SH-01",    "quantity": 500  &#125;,  "reviews": [    &#123; "username": "Alice", "rating": 5, "comment": "太棒了！" &#125;,    &#123; "username": "Bob", "rating": 4, "comment": "信号很好。" &#125;  ]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果让 ES 自动创建 Mapping (Dynamic Mapping)，它能工作，但绝不是最优的。下面是我们手动设计的<strong>最佳实践 Mapping</strong>：</p><p><strong>最佳实践 Mapping (<code>product</code> 索引)</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /products&#123;  "mappings": &#123;    "properties": &#123;      "product_id": &#123;        "type": "keyword"       &#125;,      "title": &#123;        "type": "text",         "analyzer": "ik_max_word",        "fields": &#123;          "raw": &#123;            "type": "keyword"           &#125;        &#125;      &#125;,      "description": &#123;        "type": "text",        "analyzer": "ik_max_word",        "index": true       &#125;,      "price": &#123;        "type": "scaled_float",         "scaling_factor": 100      &#125;,      "tags": &#123;        "type": "keyword"       &#125;,      "on_sale": &#123;        "type": "boolean"       &#125;,      "created_at": &#123;        "type": "date",        "format": "yyyy-MM-dd HH:mm:ss||epoch_millis"       &#125;,      "stock_info": &#123;        "type": "object",        "enabled": false       &#125;,      "reviews": &#123;        "type": "nested",         "properties": &#123;          "username": &#123; "type": "keyword" &#125;,          "rating": &#123; "type": "byte" &#125;,          "comment": &#123; "type": "text", "analyzer": "ik_smart" &#125;        &#125;      &#125;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>逐个解释设计思想</strong></p><p><strong>1. <code>product_id</code>: <code>keyword</code></strong></p><ul><li><strong>为什么是 <code>keyword</code>?</strong> ID、编码、UUID 这类数据，我们只会用它进行<strong>精确匹配</strong> (<code>term</code> 查询)，而绝不会进行分词搜索或范围查询。<code>keyword</code> 类型专门用于此目的，它将整个 “A-123-B-456” 视为一个不可分割的词元，效率远高于 <code>text</code>。即使它是纯数字，用 <code>keyword</code> 也比用 <code>integer/long</code> 更合适，因为它不参与数学计算。</li></ul><p><strong>2. <code>title</code>: <code>text</code> + <code>fields</code> (多字段)</strong></p><ul><li><strong>为什么是 <code>text</code>?</strong> <code>title</code> 是用来给用户进行全文搜索的，所以必须是 <code>text</code> 类型，并为其指定一个合适的分词器，比如 <code>ik_max_word</code>。</li><li>简单来说：<ul><li><strong>进行分词搜索</strong> (通过 <code>text</code> 类型的主字段 <code>title</code>)</li><li><strong>又能进行完整字符串的排序和分组</strong> (通过 <code>keyword</code> 类型的子字段 <code>title.raw</code>)</li></ul></li><li><strong>为什么要有 <code>fields.raw</code>?</strong> 这是一个<strong>极其重要</strong>的最佳实践。我们经常有这样的需求：对标题进行精确的<strong>聚合 (Aggregation)</strong> 或<strong>排序 (Sorting)</strong>。但 <code>text</code> 字段分词后无法完成这些操作。因此，我们通过 <code>fields</code> 为 <code>title</code> 额外创建了一个名为 <code>raw</code> 的 <code>keyword</code> 类型的子字段。这样：<ul><li>查 <code>title</code> 字段：进行全文搜索。</li><li>查 <code>title.raw</code> 字段：进行精确聚合或排序。</li></ul></li></ul><p><strong>3. <code>description</code>: <code>text</code></strong></p><ul><li><strong>为什么是 <code>text</code>?</strong> 与 <code>title</code> 类似，用于全文搜索。</li><li><strong>为什么要有 <code>&quot;index&quot;: true</code>?</strong> <code>index</code> 的默认值就是 <code>true</code>，这里显式写出来是为了强调。如果某个字段你永远不会用作查询条件，可以设置为 <code>false</code> 来节省空间和提高写入速度。但 <code>description</code> 通常需要被搜索，所以保持 <code>true</code>。</li></ul><p><strong>4. <code>price</code>: <code>scaled_float</code></strong></p><ul><li><strong>为什么是 <code>scaled_float</code>?</strong> 对于价格、汇率等需要精确计算的浮点数，标准的 <code>float</code> 或 <code>double</code> 类型可能会有精度问题。<code>scaled_float</code> 是处理货币的最佳选择。它通过一个 <code>scaling_factor</code> (缩放因子)，将浮点数乘以这个因子后，作为整数存储。这里 <code>100</code> 表示保留两位小数，<code>6999.00</code> 会被存储为 <code>699900</code>，完全避免了浮点数精度陷阱。</li></ul><p><strong>5. <code>tags</code>: <code>keyword</code></strong></p><ul><li><strong>为什么是 <code>keyword</code>?</strong> 标签是原子的，不可分的。我们希望对 “新款”、”旗舰” 这样的标签进行精确过滤和聚合，<code>keyword</code> 是不二之选。ES 会自动处理 <code>keyword</code> 类型的数组。</li></ul><p><strong>6. <code>on_sale</code>: <code>boolean</code></strong></p><ul><li><strong>为什么是 <code>boolean</code>?</strong> 用于表示 <code>true</code> 或 <code>false</code>，最直接，存储和过滤效率也最高。</li></ul><p><strong>7. <code>created_at</code>: <code>date</code></strong></p><ul><li><strong>为什么是 <code>date</code>?</strong> 专门用于处理日期时间，支持丰富的日期格式和强大的日期范围查询。</li><li><strong>为什么要有 <code>format</code>?</strong> 显式指定日期格式，可以加速日期的解析。<code>||</code> 表示“或”，意味着它既能解析 <code>&quot;yyyy-MM-dd HH:mm:ss&quot;</code> 格式的字符串，也能解析毫秒级时间戳，增强了兼容性。</li></ul><p><strong>8. <code>stock_info</code>: <code>object</code> with <code>&quot;enabled&quot;: false</code></strong></p><ul><li><strong>为什么 <code>&quot;enabled&quot;: false</code>?</strong> 这是一个<strong>关键的性能优化</strong>。假设 <code>stock_info</code> 这个对象我们<strong>只用来展示，从不按 <code>warehouse_id</code> 或 <code>quantity</code> 进行搜索</strong>。那么设置 <code>&quot;enabled&quot;: false</code> 就会告诉 ES：“不要为这个对象里的任何字段创建倒排索引”。这会节省大量的存储空间和索引开销。</li></ul><p><strong>9. <code>reviews</code>: <code>nested</code></strong></p><ul><li><p><strong>为什么是 <code>nested</code> 而不是 <code>object</code>?</strong> 这是另一个<strong>极其重要</strong>的概念。默认的 <code>object</code> 类型会“压平”数组对象，导致内部字段的关联性丢失。例如，<code>reviews</code> 数组会被压平成：</p><p>  <code>&quot;reviews.username&quot;: [&quot;Alice&quot;, &quot;Bob&quot;],   &quot;reviews.rating&quot;: [5, 4]</code></p><p>  这时如果你查询“用户是 Alice 并且评分是 4 的评论”，ES 会错误地返回 true，因为它只知道 <code>Alice</code> 和 <code>4</code> 这两个值同时存在于数组中，但不知道它们是否属于同一条评论。</p></li><li><p><strong><code>nested</code> 类型</strong>通过为每个数组元素创建独立的隐藏文档来解决这个问题，<strong>完美地保留了对象内部字段之间的关联性</strong>，让你可以进行精确的内部查询。</p></li></ul><h2 id="写入（索引）性能优化"><a href="#写入（索引）性能优化" class="headerlink" title="写入（索引）性能优化"></a><strong>写入（索引）性能优化</strong></h2><p>想象一个场景：您需要将一个包含 1 亿条商品数据的数据库，首次同步到 Elasticsearch 中。如果一条一条地写入，可能需要几天几夜。但通过优化，我们可能将这个时间缩短到几小时甚至几十分钟。</p><p>以下是提升写入性能的几个关键手段，按照重要性排序：</p><hr><p><strong>1. 使用 Bulk API (批量写入)</strong></p><ul><li><strong>这是最最最重要的一条，没有之一。</strong></li><li><strong>原理：</strong> 每次 HTTP 请求都有网络开销和处理开销。Bulk API 允许你在一次请求中，打包成百上千个独立的 <code>index</code>, <code>create</code>, <code>update</code>, <code>delete</code> 操作。这极大地减少了网络往返次数和请求开销，性能提升是数量级的。</li><li><strong>最佳实践：</strong><ul><li><strong>合理的批次大小：</strong> 批次不是越大越好。太大会消耗大量内存，给集群带来压力。一个好的起点是每次批量处理 <strong>1000 到 5000 个文档</strong>，或者批次的总大小在 <strong>5MB 到 15MB</strong> 之间。你需要通过实验找到最适合你数据和集群配置的“黄金尺寸”。</li><li><strong>并发请求：</strong> 如果机器资源充足，可以开启多个线程或进程，同时向 ES 发送 Bulk 请求。但要注意，并发数过多可能会导致 ES 拒绝请求（HTTP 429 错误），需要有相应的重试机制。</li></ul></li></ul><p><strong>2. 优化索引刷新策略 (Refresh Interval)</strong></p><ul><li><strong>原理：</strong> 之前我们提到，新写入的数据需要经过一次 <code>refresh</code> 操作才能被搜索到。默认情况下，这个操作是每秒执行一次 (<code>&quot;index.refresh_interval&quot;: &quot;1s&quot;</code>)。在海量数据写入时，每秒一次的 <code>refresh</code> 会产生大量的 I&#x2F;O 操作和新的分段（Segment），严重拖慢写入速度。</li><li><strong>最佳实践：</strong><ul><li><p>在开始批量导入数据之前，临时将刷新间隔调大，甚至禁用它。</p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /my_index/_settings&#123;  "index": &#123;    "refresh_interval": "60s"   &#125;&#125;// 或者直接禁用PUT /my_index/_settings&#123;  "index": &#123;    "refresh_interval": "-1"  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>数据全部写入完成后，再将刷新间隔调回默认值 <code>1s</code> 或 <code>null</code>。</strong></p></li></ul></li></ul><p><strong>3. 临时禁用或减少副本 (Number of Replicas)</strong></p><ul><li><strong>原理：</strong> 每写入一个文档到主分片，这个文档也必须被复制到所有的副本分片上。如果有 2 个副本，那么一次写入操作实际上会触发 3 次（1 主 + 2 副）真实的物理写入。这个复制过程是同步的，会阻塞写入请求的返回。</li><li><strong>最佳实践：</strong><ul><li><p>对于<strong>全新的索引</strong>进行首次数据导入时，可以先将副本数设置为 0。</p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /my_index/_settings&#123;  "index": &#123;    "number_of_replicas": 0  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>数据全部写入完成后，再将副本数调回到你期望的值</strong>（比如 1 或 2）。</p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /my_index/_settings&#123;  "index": &#123;    "number_of_replicas": 1  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  ES 会在后台自动开始复制数据，这个过程不会影响你后续的操作。</p></li></ul></li></ul><p><strong>4. 优化磁盘 I&#x2F;O 和分段合并 (Translog &amp; Merging)</strong></p><ul><li><strong>原理：</strong> ES 为了保证数据不丢失，会先将数据写入一个叫 <code>translog</code> 的事务日志中。默认情况下，每次请求都会触发一次 <code>fsync</code>，确保 <code>translog</code> 的数据被刷到磁盘上，这个操作比较耗时。</li><li><strong>最佳实践（谨慎使用）：</strong><ul><li><p>可以将 <code>translog</code> 的刷盘策略从 <code>request</code>（每次请求）改为 <code>async</code>（异步），并增加刷盘间隔。</p><pre class="line-numbers language-language-json"><code class="language-language-json">PUT /my_index/_settings&#123;  "index": &#123;    "translog.sync_interval": "30s",    "translog.durability": "async"  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>警告：</strong> 这个设置<strong>牺牲了数据的安全性</strong>。如果在异步刷盘的间隔内节点宕机，可能会丢失一部分数据。所以，它只适用于可以接受少量数据丢失的场景，比如重新导入日志数据。<strong>数据导入完成后，务必改回默认值。</strong></p></li></ul></li></ul><p><strong>总结：批量导入数据的黄金流程</strong></p><ol><li><strong>创建索引：</strong> 设计好 Mapping，将 <code>number_of_replicas</code> 设置为 <code>0</code>。</li><li><strong>修改设置：</strong> 将 <code>refresh_interval</code> 设置为 <code>1</code>。如果能接受风险，可以调整 <code>translog</code> 设置。</li><li><strong>执行写入：</strong> 使用多线程和 <strong>Bulk API</strong> 并发地写入数据，并监控集群状态。</li><li><strong>恢复设置：</strong> 数据全部写入完成后，将 <code>number_of_replicas</code> 和 <code>refresh_interval</code> 恢复到它们的正常值。</li></ol><h2 id="查询性能优化"><a href="#查询性能优化" class="headerlink" title="查询性能优化"></a>查询性能优化</h2><p><strong>1. 避免深度分页 (Deep Pagination)</strong></p><ul><li><strong>问题是什么？</strong> 在关系型数据库中，<code>LIMIT 10000, 10</code> 这样的查询可能依然很快。但在分布式系统中，这会成为一个灾难。ES 中使用 <code>from</code> 和 <code>size</code> 的分页方式，当你请求 <code>from: 10000, size: 10</code> 时，ES 必须：<ol><li>在<strong>每个分片</strong>上都找出前 <code>10010</code> (from + size) 个文档。</li><li>将所有分片的结果（比如 5 个分片 * 10010 条 &#x3D; 50050 条数据）汇集到协调节点。</li><li>协调节点对这 50050 条数据进行<strong>重新排序</strong>。</li><li>最后，从排序后的结果中，丢弃前 10000 条，返回第 10001 到 10010 条。<br>  这个过程随着 <code>from</code> 值的增大，消耗的内存和 CPU 会急剧增加，非常低效。</li></ol></li><li><strong>解决方案：</strong><ul><li><strong><code>search_after</code></strong>: 这是官方推荐的、用于深度分页或实时滚动加载的解决方案。它的工作方式是，利用上一页最后一条数据的信息来“定位”下一页的开始位置。它不关心你要跳过多少页，只关心“从哪里开始”。这使得它非常高效，因为每个分片只需要返回 <code>size</code> 数量的文档。</li><li><strong><code>scroll</code> API</strong>: 适用于需要处理<strong>全部</strong>查询结果的场景，比如数据导出或数据迁移。它会创建一个查询上下文的“快照”，然后你可以像迭代器一样逐批拉取数据。它不适合用于实时的用户界面分页。</li></ul></li></ul><p><strong>2. 只请求必要的字段 (<code>_source</code> Filtering)</strong></p><ul><li><p><strong>原理：</strong> 默认情况下，ES 会返回完整的 <code>_source</code> 字段，也就是你存入的整个 JSON 文档。如果你的文档很大，而前端展示只需要其中几个字段，那么传输这些不必要的数据会浪费网络带宽，并增加 ES 节点的序列化开销。</p></li><li><p><strong>最佳实践：</strong></p><ul><li><p>在查询时，明确指定你需要的字段。</p><pre class="line-numbers language-language-json"><code class="language-language-json">GET /products/_search&#123;  "_source": ["product_id", "title", "price"],  "query": &#123; "match": &#123; "title": "手机" &#125; &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>如果你完全不需要 <code>_source</code>（比如只关心聚合结果），可以将其禁用：<code>&quot;_source&quot;: false</code>。</p></li></ul><p>  <strong>3. 警惕脚本查询 (Script Queries)</strong></p><ul><li><strong>问题是什么？</strong> ES 允许你使用脚本（通常是 Painless 语言）来执行非常灵活的查询和聚合。但脚本是在查询时动态执行的，无法利用索引，并且通常比原生查询慢得多。</li><li><strong>最佳实践：</strong><ul><li><strong>能不用就不用。</strong> 优先考虑通过改进 Mapping 或在索引时预处理数据来满足需求。</li><li>如果必须用，确保脚本尽可能简单高效，并测试其性能影响。</li></ul></li></ul></li></ul><p><strong>总结一下查询优化的核心思想：</strong></p><ol><li><strong>数据说话：</strong> 优先处理数据量大的部分。先用 <code>filter</code> 快速过滤掉绝大部分无关数据。</li><li><strong>避免浪费：</strong> 不做多余的工作。避免深度分页的巨大开销，只请求必要的字段。</li><li><strong>善用索引：</strong> 确保你的查询能最大化地利用倒排索引的优势，避免全索引扫描。</li></ol><h2 id="脑裂-Split-Brain"><a href="#脑裂-Split-Brain" class="headerlink" title="脑裂 (Split-Brain)"></a>脑裂 (Split-Brain)</h2><p>我们先用一个简单的比喻来理解。</p><p>想象一个公司只有一个 CEO，所有决策都由他做出，公司井井有条。</p><p>突然有一天，公司总部大楼中间的电话线和网络断了，把公司分成了两半（比如A座和B座）。</p><ul><li>A座的员工联系不上CEO，等了一会儿，他们以为CEO出事了，于是他们从A座的管理层里<strong>选举出了一个新的CEO</strong>。</li><li>与此同时，B座的员工和原来的CEO在一起，他们继续正常工作。</li></ul><p>现在，这个公司就出现了<strong>两个CEO</strong>。A座的CEO开始下达指令，B座的CEO也在下达指令，这两个指令可能是完全冲突的。整个公司陷入了混乱，决策不一，数据错乱。</p><p>这就是“脑裂”。</p><p>在 Elasticsearch 集群中，“CEO” 就是 <strong>主节点 (Master Node)</strong>。它的职责是<strong>管理集群状态，比如创建索引、分配分片、维护节点列表等</strong>。</p><p><strong>脑裂 (Split-Brain)</strong> 指的是，由于网络故障（比如交换机故障、网络分区），一个集群被分割成了两个或多个互不通信的“孤岛”。这时，每个“孤岛”都以为对方下线了，于是各自<strong>选举出了自己的主节点</strong>。</p><p>最终的结果就是，一个集群内同时存在了多个主节点。这会导致灾难性的后果：</p><ul><li><strong>数据不一致：</strong> 不同的“孤岛”接收了不同的写请求，数据开始走向不同的“分支”，造成数据丢失或冲突。</li><li><strong>集群状态错乱：</strong> 每个主节点都在管理分片，可能会做出完全相反的决策。</li><li><strong>无法恢复：</strong> 一旦网络恢复，这些拥有不同数据和状态的“孤岛”无法自动合并，需要人工干预，过程极其复杂且可能导致数据丢失。</li></ul><p><strong>如何防止脑裂？(Quorum&#x2F;法定人数)</strong></p><p>防止脑裂的核心思想很简单：<strong>一个决策（比如选举CEO），必须得到“大多数人”的同意才能生效。</strong></p><p>这个“大多数人”，在分布式系统中被称为 <strong>Quorum (法定人数)</strong>。</p><p>在 Elasticsearch 中，这个机制通过一个关键配置来实现。</p><ul><li><strong>ES 7.0 之前:</strong> 需要手动设置一个参数 <code>discovery.zen.minimum_master_nodes</code>。</li><li><strong>ES 7.0 及之后:</strong> 这个过程被极大地简化和自动化了，不再需要手动设置上述参数，但其背后的原理依然是 Quorum。</li></ul><p><strong>Quorum 的工作原理：</strong></p><p>选举主节点或任何一个主节点要做的决策，都必须获得超过半数的主节点候选人（master-eligible nodes）的投票。</p><p>这个“法定人数”的计算公式是：</p><p>Quorum &#x3D; (master_eligible_nodes&#x2F;2) + 1</p><p>假设我们有一个由 <strong>3个</strong> 有主节点资格的节点（Node-1, Node-2, Node-3）组成的集群。</p><p>根据公式，法定人数 Quorum &#x3D; (3 &#x2F; 2) + 1 &#x3D; 1 + 1 &#x3D; <strong>2</strong>。</p><p>现在，网络发生故障，Node-1 单独成为了一个“孤岛”，Node-2 和 Node-3 在另一个“孤岛”。</p><ul><li><strong>孤岛1 (Node-1):</strong> 它只有一个节点，数量 <code>1</code> 小于法定人数 <code>2</code>。因此，它<strong>无权</strong>选举新的主节点，它只能下线自己，静静地等待网络恢复。</li><li><strong>孤岛2 (Node-2, Node-3):</strong> 它们有两个节点，数量 <code>2</code> 等于法定人数 <code>2</code>。因此，它们<strong>有权</strong>在它们之间选举出一个新的主节点（比如 Node-2）。</li></ul><p><strong>结果：</strong> 整个集群在任何时候都只有一个合法的主节点（Node-2）。当网络恢复后，Node-1 会自动发现已经存在主节点，并作为从节点加入回去。<strong>脑裂被成功避免了！</strong></p><h2 id="集群引导-Cluster-Bootstrapping"><a href="#集群引导-Cluster-Bootstrapping" class="headerlink" title="集群引导 (Cluster Bootstrapping)"></a><strong>集群引导 (Cluster Bootstrapping)</strong></h2><p>“集群引导”是一个<strong>仅在集群生命周期中发生一次</strong>的初始化过程。</p><p><strong>这个过程是如何工作的？</strong></p><p>当你准备启动一个全新的、由多个节点（比如 Node-A, Node-B, Node-C）组成的 ES 集群时，你需要在<strong>每一个主节点候选人</strong>的配置文件 (<code>elasticsearch.yml</code>) 中，明确地列出这个初始“三人小组”的成员。</p><p>这个配置项就是：<code>cluster.initial_master_nodes</code></p><ol><li><p><strong>在 Node-A 的 <code>elasticsearch.yml</code> 中：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json"># ======================== Cluster ========================# 为你的集群设置一个唯一的名字cluster.name: my-first-cluster# ======================== Node =========================# 为你的节点设置一个唯一的名字node.name: Node-A# ======================== Network ======================# 绑定节点的IP地址network.host: 192.168.1.10# ====================== Discovery ======================# 启动新集群时的初始主节点候选人列表 (只在第一次启动时使用)cluster.initial_master_nodes: ["Node-A", "Node-B", "Node-C"]# 用于节点发现的种子主机列表 (集群运行起来后使用)discovery.seed_hosts: ["192.168.1.10", "192.168.1.11", "192.168.1.12"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>在 Node-B 的 <code>elasticsearch.yml</code> 中：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json"># ======================== Cluster ========================# 为你的集群设置一个唯一的名字cluster.name: my-first-cluster# ======================== Node =========================# 为你的节点设置一个唯一的名字node.name: Node-B# ======================== Network ======================# 绑定节点的IP地址network.host: 192.168.1.11# ====================== Discovery ======================# 启动新集群时的初始主节点候选人列表 (只在第一次启动时使用)cluster.initial_master_nodes: ["Node-A", "Node-B", "Node-C"]# 用于节点发现的种子主机列表 (集群运行起来后使用)discovery.seed_hosts: ["192.168.1.10", "192.168.1.11", "192.168.1.12"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>在 Node-C 的 <code>elasticsearch.yml</code> 中：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json"># ======================== Cluster ========================# 为你的集群设置一个唯一的名字cluster.name: my-first-cluster# ======================== Node =========================# 为你的节点设置一个唯一的名字node.name: Node-C# ======================== Network ======================# 绑定节点的IP地址network.host: 192.168.1.12# ====================== Discovery ======================# 启动新集群时的初始主节点候选人列表 (只在第一次启动时使用)cluster.initial_master_nodes: ["Node-A", "Node-B", "Node-C"]# 用于节点发现的种子主机列表 (集群运行起来后使用)discovery.seed_hosts: ["192.168.1.10", "192.168.1.11", "192.168.1.12"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>启动时会发生什么？</strong></p><ol><li>当你启动这三个节点时，它们会读取这个 <code>cluster.initial_master_nodes</code> 列表。</li><li>它们会尝试与列表中的其他成员建立联系。</li><li>一旦它们发现彼此，并且确认当前在线的节点数量达到了<strong>这个初始列表的法定人数 (Quorum)</strong>（在这个例子里是 <code>(3/2)+1=2</code>），它们就会开始进行第一次主节点选举。</li><li>选举成功后（比如 Node-A 当选为 Master），第一个健康的集群就正式形成了。</li></ol><p><strong>最关键的一步：“记住”与“抛弃”</strong></p><p>一旦集群成功形成，这个 <code>cluster.initial_master_nodes</code> 配置的<strong>历史使命就完成了</strong>。</p><ul><li><strong>“记住”：</strong> 新当选的主节点会把当前集群的成员列表（包含所有节点的详细信息）写入到<strong>集群状态 (Cluster State)</strong> 中。这个状态会被同步到所有节点上并持久化。从此以后，集群就拥有了自己“记忆”，它知道自己有哪些成员。</li><li><strong>“抛弃”：</strong> 从此刻起，<strong>所有节点都会忽略 <code>elasticsearch.yml</code> 文件中的 <code>cluster.initial_master_nodes</code> 配置</strong>。即使你重启节点，它也不会再去看这个配置了。它会信任持久化在自己磁盘上的那个“记忆”（集群状态），并通过 <code>discovery.seed_hosts</code> 配置去寻找已知的其他成员。</li></ul><p><strong>为什么这个机制如此重要？</strong></p><ol><li><strong>安全性：</strong> 因为这个配置只在第一次启动空集群时生效，所以它能有效地防止你意外地用它来初始化一个已经有数据的、正在运行的集群，从而避免灾难。如果你在一个已经有数据的节点上设置了这个参数并启动，ES 会报错并拒绝启动，保护你的数据。</li><li><strong>自动化：</strong> 它为集群提供了一个安全的“起点”。一旦集群启动并运行起来，后续的成员管理（比如节点的加入、离开、主节点重选）就都进入了我们之前讨论的全自动 Quorum 模式，不再需要人工干预。</li></ol><p><strong>后续进来新的节点会发生什么？</strong></p><p>后续添加的所有节点已经不属于<strong>集群引导</strong>了，而是属于<strong>节点发现</strong>。现在在已经存在的由 A、B、C 组成的集群中添加 Node-D，正确的操作如下：</p><ol><li><p><strong>准备 Node-D 的配置文件 (<code>elasticsearch.yml</code>)</strong></p><ul><li><strong>不要设置 <code>cluster.initial_master_nodes</code>！</strong> 这个参数是新集群的“出生证明”，而 Node-D 是要加入一个已经“成年”的集群，所以用不上。</li><li><strong>设置 <code>discovery.seed_hosts</code></strong>：这个配置项告诉 Node-D 去哪里“敲门”。你需要在这里填上现有集群中部分或全部节点的地址。</li><li>确保 <code>cluster.name</code> 与现有集群完全一致。</li></ul><p> <strong>Node-D 的 <code>elasticsearch.yml</code> 应该长这样：</strong></p><pre class="line-numbers language-language-json"><code class="language-language-json"># ======================== Cluster ========================# 为你的集群设置一个唯一的名字cluster.name: my-first-cluster# ======================== Node =========================# 为你的节点设置一个唯一的名字node.name: Node-D# ======================== Network ======================# 绑定节点的IP地址network.host: 192.168.1.13# ====================== Discovery ======================# 用于节点发现的种子主机列表 (集群运行起来后使用)discovery.seed_hosts: ["192.168.1.10", "192.168.1.11", "192.168.1.12"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>启动 Node-D</strong></p><p> 直接启动 Node-D 的 Elasticsearch 服务即可。</p></li></ol><p><strong>启动后发生了什么？(自动的“发现”过程)</strong></p><ol><li>Node-D 启动，读取自己的配置，发现 <code>discovery.seed_hosts</code> 中有三个地址。</li><li>它会尝试联系这几个地址上的节点。</li><li>比如，它成功联系上了 Node-A（当前的主节点）。</li><li>Node-D 会向 Node-A 发送“加入请求”，并报上自己的集群名 (<code>cluster.name</code>)。</li><li>主节点 Node-A 验证通过后，会欢迎 Node-D 加入集群，并更新<strong>集群状态 (Cluster State)</strong>，将 Node-D 的信息添加进去。</li><li>最后，主节点 Node-A 会将这个<strong>包含了新成员的、最新的集群状态</strong>广播给集群里的所有节点（包括刚加入的 Node-D）。</li></ol><p>至此，Node-D 就成功地、平滑地成为了集群的一员。整个过程<strong>完全不需要</strong>重启或修改现有的 Node-A, B, C。</p><table><thead><tr><th>过程</th><th><strong>集群引导 (Bootstrapping)</strong></th><th><strong>节点发现 (Discovery)</strong></th></tr></thead><tbody><tr><td><strong>目的</strong></td><td>从零开始，选举<strong>第一个</strong> Master</td><td>向<strong>已存在的</strong>集群添加新节点</td></tr><tr><td><strong>时机</strong></td><td>集群生命周期中<strong>仅一次</strong></td><td><strong>随时</strong>发生</td></tr><tr><td><strong>关键配置</strong></td><td><code>cluster.initial_master_nodes</code></td><td><code>discovery.seed_hosts</code></td></tr></tbody></table><p><strong>主节点挂掉重启的过程中，会发生什么？</strong></p><p><strong>会的。在 Node-A（主节点）从关闭到重启完成的这段时间里，只要剩余节点满足法定人数 (Quorum)，集群就会发起一次新的主节点选举。</strong></p><p>整个过程是全自动的，目的就是为了保证集群的服务尽可能不中断。我们来分解一下这个过程的时间线：</p><hr><p><strong>主节点重启期间的事件时间线</strong></p><p>假设我们有一个由 Node-A (Master), Node-B, Node-C 组成的健康集群。</p><p><strong>第 1 步：Node-A (主节点) 开始重启</strong></p><ul><li>您在 Node-A 的服务器上执行了重启命令。Node-A 进程关闭，与集群中的其他节点断开连接。</li></ul><p><strong>第 2 步：主节点故障被检测</strong></p><ul><li>集群中的其他节点（Node-B 和 Node-C）会定期地对主节点进行“心跳检测”（Health Check）。</li><li>当它们连续几次都无法收到来自 Node-A 的心跳时，它们就会判定：“我们的主节点失联了！”</li></ul><p><strong>第 3 步：触发新一轮选举</strong></p><ul><li>此时，Node-B 和 Node-C 发现集群中没有主节点了。</li><li>它们会立即检查当前还存活的、有主节点资格的节点数量是否满足<strong>法定人数 (Quorum)</strong>。<ul><li>在我们的例子中，总共有 3 个主节点候选人，法定人数是 <code>(3/2) + 1 = 2</code>。</li><li>现在还剩下 Node-B 和 Node-C，数量是 2，<strong>满足法定人数</strong>。</li></ul></li><li>因为满足法定人数，它们就立即开始一轮新的选举。</li></ul><p><strong>第 4 步：新的主节点诞生</strong></p><ul><li>在 Node-B 和 Node-C 之间，通过选举算法，会有一个胜出者。我们假设 <strong>Node-B 被选举为新的主节点</strong>。</li><li>一旦选举成功，Node-B 就会接管主节点的所有职责，开始管理整个集群。</li><li><strong>关键点：</strong> 从 Node-A 宕机到 Node-B 上位，这个选举过程通常非常快（秒级）。在这段短暂的“权力真空期”，集群可能无法处理需要主节点协调的写操作（比如创建索引），但对于数据的<strong>读写操作基本不受影响</strong>，因为数据分片（Data Shards）仍然在各自的数据节点上正常工作。</li></ul><p><strong>第 5 步：旧主节点 (Node-A) 重启完成并归队</strong></p><ul><li>几分钟后，Node-A 的服务器重启完毕，Elasticsearch 服务也启动了。</li><li>Node-A 启动后，会读取自己的 <code>discovery.seed_hosts</code> 配置，去联系它记忆中的其他集群成员。</li><li>它联系上了 Node-B（或者 Node-C），然后它会发现——<strong>集群已经有了一个新的主节点 (Node-B)！</strong></li><li>此时，Node-A <strong>不会</strong>去尝试“抢回”主节点的位置。它会很“谦虚”地认识到自己已经不是 Master 了。</li><li>它会向新的主节点 Node-B 发送加入请求，并作为一个<strong>普通的从节点 (Follower Node)</strong> 重新加入集群。</li></ul><p><strong>最终结果</strong></p><p>集群恢复到拥有 3 个健康节点的状态，只不过此时的<strong>主节点变成了 Node-B</strong>，而原来的主节点 Node-A 则变成了一个从节点。整个集群的可用性得到了保障。</p><p><strong>总结一下：</strong></p><p>这个自动的故障转移和选举机制，正是 Elasticsearch 高可用性的核心体现。只要你保证<strong>主节点候选人的数量 &gt;&#x3D; 3 （并且是奇数）</strong>，那么任何一个主节点的重启或宕机，都不会导致整个集群的瘫痪，系统具备了自我修复的能力。</p><hr><p><strong>心跳检测是谁和谁检测？</strong></p><p>心跳检测<strong>不是</strong>所有节点相互检测（这在大型集群中会产生巨大的网络风暴），而是一个以<strong>主节点为中心的、双向的星型检测模式</strong>。</p><p>我们可以把这个机制分为两种官方的叫法：</p><ol><li><strong>Follower Checks (主查从)</strong><ul><li><strong>谁做：</strong> 由<strong>主节点 (Master)</strong> 发起。</li><li><strong>做什么：</strong> 主节点会定期地、主动地去 <code>ping</code> 集群中的<strong>每一个其他节点</strong>（无论是数据节点、协调节点还是其他主节点候选人），问一声“你还在吗？”。</li><li><strong>目的：</strong> 确认集群的所有成员都还健康地活着并且保持在线。</li></ul></li><li><strong>Leader Checks (从查主)</strong><ul><li><strong>谁做：</strong> 由集群中<strong>所有非主节点 (Non-master Nodes)</strong> 发起。</li><li><strong>做什么：</strong> 每一个从节点会定期地、主动地去 <code>ping</code> <strong>当前的主节点</strong>，问一声“老板，你还在吗？”。</li><li><strong>目的：</strong> 确认集群的“大脑”——主节点，依然在线且能够正常响应。如果从节点发现主节点失联，它们就会发起新一轮的选举（如我们之前讨论的）。</li></ul></li></ol><p><strong>一句话总结：主节点负责“点名”检查每个手下，而每个手下也负责时刻关注“老板”的健康状况。</strong></p><hr><p><strong>问题2：如果挂掉的是普通节点（比如数据节点）呢？</strong></p><p>如果挂掉的是一个普通的数据节点（我们称之为 Node-X），整个过程<strong>不会发生主节点选举</strong>，而是会触发一套不同的、以<strong>数据恢复</strong>为核心的流程。</p><p>以下是事件的完整时间线：</p><p><strong>第 1 步：故障检测 (Fault Detection)</strong></p><ul><li>主节点在执行它的“Follower Check”时，发现联系不上 Node-X。在经过几次（可配置的）重试后，主节点会正式将 Node-X 标记为“已掉线 (failed)”。</li></ul><p><strong>第 2 步：更新集群状态 (Updating Cluster State)</strong></p><ul><li>主节点的第一项行动，就是立即从<strong>集群状态</strong>中移除 Node-X。</li><li>然后，它会把这个<strong>不包含 Node-X 的、最新的集群状态</strong>广播给所有还存活的节点。至此，整个集群达成共识：Node-X 已经不在了。</li></ul><p><strong>第 3 步：数据自动恢复 (Shard Recovery)</strong></p><ul><li>这个新的集群状态会触发自动恢复流程。主节点现在需要处理那些“无家可归”的分片（Shards），也就是之前存放在 Node-X 上的分片。</li><li><strong>情况A：如果 Node-X 上有主分片 (Primary Shards)</strong><ul><li>主节点会立即在其他存活的节点上，找到这些主分片对应的<strong>副本分片 (Replica Shards)</strong>。</li><li>它会从这些副本中选择一个，并将其<strong>提升 (Promote) 为新的主分片</strong>。这是一个非常快速的元数据操作，能确保索引几乎立刻恢复可写状态。</li></ul></li><li><strong>情况B：恢复数据冗余度</strong><ul><li>现在，由于 Node-X 的离开，集群中很多分片（包括被提升为新主分片的那些）的副本数量都少了一个，整个集群的数据冗余度下降了（集群健康状态会变为 <code>yellow</code>）。</li><li>主节点会开始在其他健康的节点上，<strong>创建新的副本分片</strong>，并从对应的主分片那里复制数据。</li><li>这个数据复制的过程可能会消耗一定的网络和磁盘 I&#x2F;O，但它是在后台自动进行的。</li></ul></li></ul><p><strong>第 4 步：恢复完成</strong></p><ul><li>一旦所有缺失的副本都被重新创建并同步好数据，集群的健康状态就会恢复到 <code>green</code>。</li></ul><p><strong>总结：两种故障的核心区别</strong></p><ul><li><strong>主节点故障：</strong> 核心是**“权力真空”<strong>。解决方法是</strong>重新选举 (Re-election)<strong>，恢复</strong>集群的控制能力**。</li><li><strong>普通节点故障：</strong> 核心是**“数据丢失风险”<strong>。解决方法是</strong>分片恢复 (Shard Recovery)<strong>，包括</strong>副本提主 (Replica Promotion)** 和 <strong>重新分配副本 (Replica Allocation)</strong>，恢复<strong>数据的完整性和冗余度</strong>。这个过程由现有的主节点全权负责，<strong>不需要重新选举</strong>。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2024/08/10/linux-bi-ji/"/>
      <url>/2024/08/10/linux-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="一、文件与目录操作"><a href="#一、文件与目录操作" class="headerlink" title="一、文件与目录操作"></a>一、文件与目录操作</h2><ol><li><p><strong>列出目录内容</strong></p><ul><li><code>ls</code>：列出当前目录</li><li><code>ls -l</code>：显示详细信息（权限、拥有者、大小、修改时间）</li><li><code>ls -a</code>：包括隐藏文件（<code>.</code> 开头）</li><li><code>ls -lh</code>：人性化显示文件大小（K, M, G）</li><li><code>ls -lt</code>：按修改时间排序，最新在前</li><li><code>ls -lS</code>：按文件大小排序，最大在前</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ls -la /etc          # 列出 /etc 目录的所有文件（包括隐藏文件）ls -lh ~/Downloads   # 显示下载目录文件，以人性化方式显示大小ls -lt *.log         # 按时间排序显示所有日志文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>目录切换与查看</strong></p><ul><li><code>pwd</code>：显示当前所在目录的完整路径</li><li><code>cd 目录</code>：切换目录</li><li><code>cd ..</code>：返回上级目录</li><li><code>cd -</code>：返回上次所在目录</li><li><code>cd ~</code>：切换到用户主目录</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">pwd                  # 显示当前工作目录cd /var/log          # 切换到系统日志目录cd ../..             # 返回上两级目录cd -                 # 快速返回上一个访问的目录cd ~/Documents       # 切换到用户文档目录<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>复制&#x2F;移动&#x2F;删除</strong></p><ul><li><code>cp 源 目标</code>：复制文件或目录（加 <code>-r</code> 递归）</li><li><code>cp -i</code>：覆盖前询问</li><li><code>cp -p</code>：保留源文件属性（时间戳、权限等）</li><li><code>mv 源 目标</code>：移动或重命名</li><li><code>mv -i</code>：覆盖前询问</li><li><code>rm 文件</code>：删除文件；<code>rm -r 目录</code>：递归删除目录</li><li><code>rm -i</code>：删除前询问确认</li><li><code>rm -f</code>：强制删除，不询问</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">cp file1.txt file2.txt              # 复制文件cp -r dir1/ dir2/                   # 递归复制目录cp -ip old.conf new.conf            # 复制文件并保留属性，覆盖前询问mv oldname.txt newname.txt          # 重命名文件mv *.jpg ~/Pictures/                # 移动所有图片到目录rm -rf temp/                        # 强制删除目录及内容（慎用）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>创建文件与目录</strong></p><ul><li><code>mkdir 目录名</code>：创建目录</li><li><code>mkdir -p 路径/目录</code>：创建多级目录</li><li><code>touch 文件名</code>：创建空文件或更新时间戳</li><li><code>rmdir 目录名</code>：删除空目录</li><li><code>tree</code>：以树状图显示目录结构（若未安装可 <code>sudo apt install tree</code>）</li></ul></li><li><p><strong>链接操作</strong></p><ul><li><code>ln -s 源文件 软链接</code>：创建软链接（符号链接）</li><li><code>ln 源文件 硬链接</code>：创建硬链接</li><li><code>readlink 链接</code>：查看链接指向的实际路径</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ln -s /usr/bin/python3 ~/bin/python           # 创建 Python 软链接ln important.txt backup_hard_link.txt         # 创建硬链接readlink /bin/sh                              # 查看链接指向的实际文件ls -l /bin | grep '^l'                        # 查看所有符号链接<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看文件内容</strong></p><ul><li><code>cat 文件</code>：一次性显示全文</li><li><code>cat -n</code>：显示行号</li><li><code>more/less 文件</code>：分页查看，<code>/ + 关键字</code> 搜索，<code>n</code> 跳到下一个匹配</li><li><code>head -n 10 文件</code>：查看前 10 行；<code>tail -n 10 文件</code>：后 10 行</li><li><code>tail -f 文件</code>：实时追踪文件新增内容，常用于日志监控</li><li><code>file 文件</code>：查看文件类型</li></ul></li></ol><hr><h2 id="二、文本处理与过滤"><a href="#二、文本处理与过滤" class="headerlink" title="二、文本处理与过滤"></a>二、文本处理与过滤</h2><ol><li><p><strong>过滤与查找</strong></p><ul><li><code>grep 模式 文件</code>：查找匹配行；<code>-i</code> 忽略大小写；<code>-n</code> 显示行号；<code>-v</code> 反向查找</li><li><code>grep -r 模式 目录</code>：递归搜索目录</li><li><code>grep -E &#39;正则表达式&#39;</code>：使用扩展正则表达式</li><li><code>egrep</code>：等同于 <code>grep -E</code></li><li><code>awk &#39;&#123;print $1,$3&#125;&#39; 文件</code>：按空格分列，打印第 1、3 列</li><li><code>cut -d&#39;:&#39; -f1 /etc/passwd</code>：以 <code>:</code> 分割，取第一列</li><li><code>sed &#39;s/old/new/g&#39; 文件</code>：替换（默认只输出到标准输出）；加 <code>-i</code> 直接修改文件</li><li><code>sed -n &#39;1,10p&#39; 文件</code>：只显示第 1-10 行</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">grep -n "ERROR" server.log                    # 查找错误日志，显示行号grep -r "TODO" ./src                          # 递归搜索源代码中的 TODOgrep -E '^[0-9]&#123;3&#125;-[0-9]&#123;4&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>排序与去重</strong></p><ul><li><code>sort 文件</code>：排序；<code>sort -n</code> 数值排序；<code>-r</code> 逆序；<code>-k n</code> 指定第 n 列排序</li><li><code>uniq 文件</code>：去除相邻重复；<code>uniq -c</code> 统计次数；常与 <code>sort</code> 管道配合：<pre class="line-numbers language-language-bash"><code class="language-language-bash">sort abc.txt | uniq -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><code>comm 文件1 文件2</code>：比较两个已排序文件的异同</li></ul></li><li><p><strong>统计与比较</strong></p><ul><li><code>wc -l 文件</code>：统计行数；<code>-m</code> 字符数；<code>-c</code> 字节数；<code>-L</code> 最长行长度；<code>-w</code> 单词数</li><li><code>diff 文件1 文件2</code>：比较文件差异</li><li><code>diff -u</code>：以统一格式显示差异</li><li><code>cmp 文件1 文件2</code>：二进制比较文件</li></ul></li><li><p><strong>字符转换</strong></p><ul><li><code>tr &#39;a-z&#39; &#39;A-Z&#39; &lt; 文件</code>：小写转大写</li><li><code>tr -d &#39;0-9&#39;</code>：删除所有数字</li><li><code>tr -s &#39; &#39;</code>：压缩连续空格为单个空格</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">echo "hello world" | tr 'a-z' 'A-Z'          # 转换为大写: HELLO WORLDecho "user123pass456" | tr -d '0-9'          # 删除数字: userpassecho "too    many    spaces" | tr -s ' '      # 压缩空格: too many spacescat file.txt | tr '\n' ' '                    # 将换行符替换为空格<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="三、权限与拥有者"><a href="#三、权限与拥有者" class="headerlink" title="三、权限与拥有者"></a>三、权限与拥有者</h2><ol><li><p><strong>查看权限</strong></p><ul><li><code>ls -l</code> 第一列如 <code>-rwxr-xr--</code>：分别是文件类型、用户&#x2F;组&#x2F;其他用户的读(r)&#x2F;写(w)&#x2F;执行(x) 权限</li><li><code>stat 文件</code>：查看文件详细状态信息</li></ul></li><li><p><strong>修改权限</strong></p><ul><li><code>chmod u+rw 文件</code>：用户添加读写权限；也可用数字模式，如 <code>chmod 755 文件</code></li><li><code>chmod g-w 文件</code>：组移除写权限</li><li><code>chmod o+x 文件</code>：其他用户添加执行权限</li><li><code>chmod -R 755 目录</code>：递归修改目录及子目录权限</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">chmod 644 file.txt                   # 设置 rw-r--r-- 权限chmod u+x script.sh                  # 给所有者添加执行权限chmod -R 755 /var/www/html          # 递归设置 web 目录权限chmod a-x sensitive.dat             # 移除所有用户的执行权限<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>修改拥有者&#x2F;所属组</strong></p><ul><li><code>chown user:group 文件</code>：同时修改用户与组</li><li><code>chown user 文件</code> 或 <code>chown :group 文件</code></li><li><code>chown -R user:group 目录</code>：递归修改目录权限</li></ul></li><li><p><strong>特殊权限</strong></p><ul><li><code>chmod u+s 文件</code>：设置 SUID 权限</li><li><code>chmod g+s 目录</code>：设置 SGID 权限</li><li><code>chmod +t 目录</code>：设置粘滞位</li><li><code>umask</code>：查看默认权限掩码</li><li><code>umask 022</code>：设置默认权限掩码</li></ul></li><li><p><strong>文件属性</strong></p><ul><li><code>lsattr 文件</code>：查看文件特殊属性</li><li><code>chattr +i 文件</code>：设置不可修改属性</li><li><code>chattr +a 文件</code>：设置只能追加属性</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">chattr +i /etc/resolv.conf           # 防止文件被修改chattr +a /var/log/secure           # 只允许追加内容lsattr /etc/resolv.conf             # 查看文件属性chattr -i /etc/resolv.conf          # 移除不可修改属性<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="四、进程与作业管理"><a href="#四、进程与作业管理" class="headerlink" title="四、进程与作业管理"></a>四、进程与作业管理</h2><ol><li><p><strong>查看进程</strong></p><ul><li><code>ps -ef</code>：列出所有进程；常与 <code>grep</code> 结合过滤</li><li><code>ps aux</code>：显示详细进程信息</li><li><code>pstree</code>：以树状图显示进程关系</li><li><code>top</code> &#x2F; <code>htop</code>：动态监控 CPU&#x2F;内存&#x2F;进程（按 q 退出）</li><li><code>pgrep 进程名</code>：按名称查找进程 ID</li></ul></li><li><p><strong>杀死进程</strong></p><ul><li><code>kill PID</code>：温和终止（发送 SIGTERM 信号）</li><li><code>kill -9 PID</code>：强制终止（发送 SIGKILL 信号）</li><li><code>kill -l</code>：列出所有信号</li><li><code>pkill 进程名</code> &#x2F; <code>killall 进程名</code>：按名杀死</li><li><code>killall -u 用户名</code>：杀死指定用户的所有进程</li></ul></li><li><p><strong>进程优先级</strong></p><ul><li><code>nice -n 10 命令</code>：以较低优先级运行命令</li><li><code>renice +10 -p PID</code>：调整运行中进程的优先级</li></ul></li><li><p><strong>后台与作业控制</strong></p><ul><li><code>command &amp;</code>：后台运行</li><li><code>jobs</code>：查看后台作业，<code>fg %1</code> 将第 1 个作业拉到前台，<code>bg %1</code> 让其后台运行</li><li><code>nohup command &amp;</code>：后台运行且忽略挂断信号（退出终端后继续运行）</li><li><code>Ctrl+Z</code>：挂起当前进程</li><li><code>Ctrl+C</code>：终止当前进程</li></ul></li></ol><hr><h2 id="五、网络与远程操作"><a href="#五、网络与远程操作" class="headerlink" title="五、网络与远程操作"></a>五、网络与远程操作</h2><ol><li><p><strong>网络诊断</strong></p><ul><li><code>ping 主机</code>：连通性测试</li><li><code>ping -c 4</code>：只发送 4 个数据包</li><li><code>traceroute 主机</code>：路由追踪</li><li><code>mtr 主机</code>：结合 ping 和 traceroute 功能</li><li><code>netstat -anp</code>：查看端口、连接；常与 <code>grep</code> 结合</li><li><code>ss -tulnp</code>：更现代的端口查看工具</li><li><code>nmap 主机</code>：端口扫描（需要安装）</li></ul></li><li><p><strong>DNS 查询</strong></p><ul><li><code>nslookup 域名</code>：查询 DNS 记录</li><li><code>dig 域名</code>：详细 DNS 查询</li><li><code>host 域名</code>：简单 DNS 查询</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">nslookup google.com                  # 查询域名的 IP 地址dig @8.8.8.8 example.com            # 使用指定 DNS 服务器查询dig example.com MX                   # 查询邮件服务器记录host -t AAAA ipv6.google.com        # 查询 IPv6 地址<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>网络配置</strong></p><ul><li><code>ifconfig</code>：查看网络接口（较旧）</li><li><code>ip addr show</code>：查看 IP 地址（现代替代 ifconfig）</li><li><code>ip route show</code>：查看路由表</li><li><code>hostname</code>：显示主机名</li><li><code>hostname -I</code>：显示所有 IP 地址</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ip addr show eth0                    # 显示特定网卡信息ip route add 192.168.1.0/24 via 192.168.1.1  # 添加静态路由ip link set eth0 up                  # 启用网卡ifconfig eth0 192.168.1.100         # 设置 IP 地址（旧方式）sudo hostnamectl set-hostname newname  # 修改主机名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>文件传输与远程登录</strong></p><ul><li><code>ssh user@host</code>：远程登录</li><li><code>ssh -p 端口 user@host</code>：指定端口登录</li><li><code>scp 本地 远端</code> &#x2F; <code>scp user@host:远端 本地</code>：安全复制</li><li><code>sftp user@host</code>：安全文件传输</li><li><code>rsync -avz 源 目标</code>：增量同步</li><li><code>rsync -avz --delete 源 目标</code>：同步并删除目标中源没有的文件</li></ul></li><li><p><strong>下载工具</strong></p><ul><li><code>wget URL</code>：下载文件</li><li><code>wget -c URL</code>：断点续传</li><li><code>curl URL</code>：发送网络请求</li><li><code>curl -O URL</code>：下载文件</li><li><code>curl -H &quot;Header: Value&quot; URL</code>：添加请求头</li></ul></li><li><p><strong>查看公网 IP</strong></p><ul><li><code>curl -4 ifconfig.co</code> 或 <code>curl ipinfo.io/ip</code></li><li><code>curl icanhazip.com</code></li></ul></li></ol><hr><h2 id="六、磁盘与存储"><a href="#六、磁盘与存储" class="headerlink" title="六、磁盘与存储"></a>六、磁盘与存储</h2><ol><li><p><strong>磁盘使用情况</strong></p><ul><li><code>df -h</code>：查看各分区总量与剩余空间</li><li><code>df -i</code>：查看 inode 使用情况</li><li><code>du -sh 目录</code>：查看目录总占用；<code>du -h --max-depth=1</code> 查看子目录分布</li><li><code>du -h | sort -h</code>：按大小排序显示</li><li><code>ncdu</code>：交互式磁盘使用分析（需要安装）</li></ul></li><li><p><strong>块设备管理</strong></p><ul><li><code>lsblk</code>：列出块设备信息</li><li><code>blkid</code>：显示块设备属性</li><li><code>fdisk -l</code>：列出磁盘分区信息</li><li><code>parted -l</code>：列出分区信息（支持 GPT）</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">lsblk -f                             # 显示文件系统类型blkid /dev/sda1                      # 显示分区 UUID 和类型sudo fdisk -l /dev/sda               # 查看磁盘分区表sudo parted /dev/sda print           # 显示分区信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>文件系统操作</strong></p><ul><li><code>mount 设备 挂载点</code>：挂载文件系统</li><li><code>umount 设备/挂载点</code>：卸载文件系统</li><li><code>mount -o remount,rw /</code>：重新挂载为读写模式</li><li><code>fsck 设备</code>：文件系统检查和修复</li><li><code>mkfs.ext4 设备</code>：创建 ext4 文件系统</li></ul></li><li><p><strong>打包压缩</strong></p><ul><li><code>tar -czvf archive.tar.gz 目录/文件</code>：打包并 gzip 压缩</li><li><code>tar -xzvf archive.tar.gz</code>：解压</li><li><code>tar -tf archive.tar</code>：查看压缩包内容而不解压</li><li><code>zip -r archive.zip 目录</code> &#x2F; <code>unzip archive.zip</code></li><li><code>gzip 文件</code> &#x2F; <code>gunzip 文件.gz</code></li><li><code>bzip2 文件</code> &#x2F; <code>bunzip2 文件.bz2</code></li><li><code>xz 文件</code> &#x2F; <code>unxz 文件.xz</code></li></ul></li><li><p><strong>数据复制与备份</strong></p><ul><li><code>dd if=源 of=目标 bs=块大小</code>：数据复制</li><li><code>dd if=/dev/zero of=file bs=1M count=100</code>：创建100M空文件</li><li><code>cp --sparse=always 文件 目标</code>：复制稀疏文件</li></ul></li></ol><hr><h2 id="七、查找与批处理"><a href="#七、查找与批处理" class="headerlink" title="七、查找与批处理"></a>七、查找与批处理</h2><ol><li><p><strong>查找文件</strong></p><ul><li><code>find . -name &#39;*.log&#39;</code>：当前目录及子目录查找</li><li><code>find /path -type f -mtime -7</code>：查找 7 天内修改的文件</li><li><code>find . -type f -name &#39;xiaohub.log.2024-06*&#39; -delete</code>：批量删除</li><li><code>find . -size +100M</code>：查找大于 100M 的文件</li><li><code>find . -empty</code>：查找空文件或空目录</li><li><code>find . -perm 777</code>：查找权限为 777 的文件</li></ul></li><li><p><strong>快速定位</strong></p><ul><li><code>locate 文件名</code>：快速查找文件（基于数据库）</li><li><code>updatedb</code>：更新 locate 数据库</li><li><code>which 命令</code>：查找命令的完整路径</li><li><code>whereis 命令</code>：查找二进制、源代码和手册页位置</li><li><code>type 命令</code>：显示命令类型（别名、内置、外部命令）</li></ul></li><li><p><strong>批量执行</strong></p><ul><li>与 <code>find</code> + <code>-exec</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.sh' -exec chmod +x &#123;&#125; \;find /var/log -name '*.log' -exec cp &#123;&#125; &#123;&#125;.bak \;find . -type f -exec md5sum &#123;&#125; \; > checksums.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><code>xargs</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.txt' | xargs grep '关键字'find . -name '*.tmp' | xargs rm -fls *.jpg | xargs -I &#123;&#125; convert &#123;&#125; &#123;&#125;.png<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><code>parallel</code>：并行执行命令（需要安装）<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.jpg' | parallel convert &#123;&#125; &#123;.&#125;.pngcat urls.txt | parallel wget &#123;&#125;seq 1 10 | parallel -j4 'echo "Process &#123;&#125;"'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li></ol><hr><h2 id="八、Shell-环境与脚本"><a href="#八、Shell-环境与脚本" class="headerlink" title="八、Shell 环境与脚本"></a>八、Shell 环境与脚本</h2><ol><li><p><strong>环境变量</strong></p><ul><li><code>export VAR=value</code>：临时设置；写入 <code>~/.bashrc</code> 或 <code>~/.profile</code> 实现持久</li><li><code>echo $VAR</code>：显示变量值</li><li><code>env</code>：显示所有环境变量</li><li><code>set</code>：显示所有变量（包括局部变量）</li><li><code>unset VAR</code>：删除变量</li><li><code>source 文件</code> 或 <code>.</code>：重新加载配置</li></ul></li><li><p><strong>Shell 配置</strong></p><ul><li><code>~/.bashrc</code>：bash shell 配置文件</li><li><code>~/.bash_profile</code>：登录 shell 配置文件</li><li><code>~/.bash_history</code>：命令历史记录</li></ul></li><li><p><strong>常见提示</strong></p><ul><li><code>history</code>：查看历史命令；<code>!n</code> 重复第 n 条，<code>!!</code> 重复上一条</li><li><code>history -c</code>：清除历史记录</li><li><code>Ctrl+R</code>：反向搜索历史命令</li><li>Tab 自动补全</li><li><code>alias ll=&#39;ls -l&#39;</code>：自定义快捷命令，写入 <code>~/.bashrc</code> 生效</li><li><code>unalias 别名</code>：删除别名</li></ul></li><li><p><strong>目录操作</strong></p><ul><li><code>pushd 目录</code>：将目录压入堆栈并切换</li><li><code>popd</code>：从堆栈弹出目录并切换</li><li><code>dirs</code>：显示目录堆栈</li></ul></li><li><p><strong>脚本规范</strong></p><ul><li>首行 <code>#!/bin/bash</code> 或 <code>#!/usr/bin/env bash</code></li><li>脚本执行前加执行权限：<code>chmod +x script.sh</code></li><li>参数获取：<code>$1,$2,…</code>；循环 <code>for arg in &quot;$@&quot;; do …; done</code></li><li><code>$0</code>：脚本名称</li><li><code>$#</code>：参数个数</li><li><code>$?</code>：上个命令的退出状态</li><li><code>test</code> 或 <code>[ ]</code>：条件测试</li><li><code>[[ ]]</code>：扩展的条件测试（支持正则）</li></ul></li></ol><hr><h2 id="九、系统管理"><a href="#九、系统管理" class="headerlink" title="九、系统管理"></a>九、系统管理</h2><ol><li><p><strong>服务与日志</strong></p><ul><li><code>systemctl status 服务名</code>：查看服务状态</li><li><code>systemctl start/stop/restart 服务名</code>：启停服务</li><li><code>systemctl enable/disable 服务名</code>：开机自启设置</li><li><code>journalctl -u 服务名</code>：查看 systemd 日志</li><li><code>journalctl -f</code>：实时查看系统日志</li></ul></li><li><p><strong>定时任务</strong></p><ul><li><code>crontab -e</code>：编辑当前用户 crontab</li><li><code>crontab -l</code>：列出当前用户的定时任务</li><li>格式：<code>* * * * * command</code>（分 时 日 月 周）</li><li><code>/etc/crontab</code>：系统级定时任务</li></ul></li><li><p><strong>用户与组管理</strong></p><ul><li><code>useradd 用户名</code> &#x2F; <code>usermod -aG group 用户名</code> &#x2F; <code>userdel 用户名</code></li><li><code>passwd 用户名</code>：设置用户密码</li><li><code>groups 用户名</code>：查看用户所属组</li><li><code>id 用户名</code>：显示用户和组 ID</li><li><code>su - 用户名</code>：切换用户</li><li><code>sudo 命令</code>：以 root 权限执行命令</li></ul></li><li><p><strong>系统监控</strong></p><ul><li><code>free -h</code>：查看内存使用情况</li><li><code>uptime</code>：系统运行时间和负载</li><li><code>dmesg</code>：查看内核消息</li><li><code>lsof</code>：列出打开的文件</li><li><code>lsof -i :端口</code>：查看端口占用情况</li><li><code>vmstat</code>：虚拟内存统计</li><li><code>iostat</code>：IO 统计</li><li><code>sar</code>：系统活动报告（需要安装 sysstat）</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">free -h                              # 显示人性化的内存使用信息uptime                               # 显示运行时间和负载dmesg | tail -20                     # 查看最近的内核消息lsof -i :80                          # 查看 80 端口占用lsof -p 1234                         # 查看进程 1234 打开的文件vmstat 2 5                           # 每 2 秒更新一次，共 5 次iostat -x 1                          # 每秒显示详细 IO 统计sar -u 1 5                           # CPU 使用率，每秒更新，共 5 次<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="十、软件包管理"><a href="#十、软件包管理" class="headerlink" title="十、软件包管理"></a>十、软件包管理</h2><ol><li><p><strong>Debian&#x2F;Ubuntu（APT）</strong></p><ul><li><code>apt update</code>：更新软件包列表</li><li><code>apt upgrade</code>：升级已安装软件包</li><li><code>apt install 软件包</code>：安装软件</li><li><code>apt remove 软件包</code>：删除软件</li><li><code>apt search 关键字</code>：搜索软件包</li><li><code>apt show 软件包</code>：显示软件包信息</li><li><code>apt autoremove</code>：删除不需要的依赖包</li></ul></li><li><p><strong>RHEL&#x2F;CentOS（YUM&#x2F;DNF）</strong></p><ul><li><code>yum update</code>：更新软件包</li><li><code>yum install 软件包</code>：安装软件</li><li><code>yum remove 软件包</code>：删除软件</li><li><code>yum search 关键字</code>：搜索软件包</li><li><code>yum info 软件包</code>：显示软件包信息</li><li><code>dnf</code>：新版本的 Fedora&#x2F;RHEL 使用 dnf 替代 yum</li></ul></li><li><p><strong>常用工具安装</strong></p><ul><li><code>apt install net-tools</code>：安装传统网络工具（ifconfig 等）</li><li><code>apt install vim</code>：安装 vim 编辑器</li><li><code>apt install htop</code>：安装 htop 进程监控工具</li><li><code>apt install ncdu</code>：安装磁盘使用分析工具</li></ul></li></ol><hr><h2 id="十一、快捷键与技巧"><a href="#十一、快捷键与技巧" class="headerlink" title="十一、快捷键与技巧"></a>十一、快捷键与技巧</h2><ol><li><p><strong>终端快捷键</strong></p><ul><li><code>Ctrl+A</code>：移到行首</li><li><code>Ctrl+E</code>：移到行尾</li><li><code>Ctrl+U</code>：删除光标前的内容</li><li><code>Ctrl+K</code>：删除光标后的内容</li><li><code>Ctrl+W</code>：删除光标前的单词</li><li><code>Ctrl+L</code>：清屏（等同于 clear）</li><li><code>Ctrl+D</code>：退出当前 shell</li></ul></li><li><p><strong>命令行技巧</strong></p><ul><li><code>!!</code>：重复执行上一条命令</li><li>&#96;!## 一、文件与目录操作</li></ul></li><li><p><strong>列出目录内容</strong></p><ul><li><code>ls</code>：列出当前目录</li><li><code>ls -l</code>：显示详细信息（权限、拥有者、大小、修改时间）</li><li><code>ls -a</code>：包括隐藏文件（<code>.</code> 开头）</li><li><code>ls -lh</code>：人性化显示文件大小（K, M, G）</li><li><code>ls -lt</code>：按修改时间排序，最新在前</li><li><code>ls -lS</code>：按文件大小排序，最大在前</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ls -la /etc          # 列出 /etc 目录的所有文件（包括隐藏文件）ls -lh ~/Downloads   # 显示下载目录文件，以人性化方式显示大小ls -lt *.log         # 按时间排序显示所有日志文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>目录切换与查看</strong></p><ul><li><code>pwd</code>：显示当前所在目录的完整路径</li><li><code>cd 目录</code>：切换目录</li><li><code>cd ..</code>：返回上级目录</li><li><code>cd -</code>：返回上次所在目录</li><li><code>cd ~</code>：切换到用户主目录</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">pwd                  # 显示当前工作目录cd /var/log          # 切换到系统日志目录cd ../..             # 返回上两级目录cd -                 # 快速返回上一个访问的目录cd ~/Documents       # 切换到用户文档目录<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>复制&#x2F;移动&#x2F;删除</strong></p><ul><li><code>cp 源 目标</code>：复制文件或目录（加 <code>-r</code> 递归）</li><li><code>cp -i</code>：覆盖前询问</li><li><code>cp -p</code>：保留源文件属性（时间戳、权限等）</li><li><code>mv 源 目标</code>：移动或重命名</li><li><code>mv -i</code>：覆盖前询问</li><li><code>rm 文件</code>：删除文件；<code>rm -r 目录</code>：递归删除目录</li><li><code>rm -i</code>：删除前询问确认</li><li><code>rm -f</code>：强制删除，不询问</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">cp file1.txt file2.txt              # 复制文件cp -r dir1/ dir2/                   # 递归复制目录cp -ip old.conf new.conf            # 复制文件并保留属性，覆盖前询问mv oldname.txt newname.txt          # 重命名文件mv *.jpg ~/Pictures/                # 移动所有图片到目录rm -rf temp/                        # 强制删除目录及内容（慎用）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>创建文件与目录</strong></p><ul><li><code>mkdir 目录名</code>：创建目录</li><li><code>mkdir -p 路径/目录</code>：创建多级目录</li><li><code>touch 文件名</code>：创建空文件或更新时间戳</li><li><code>rmdir 目录名</code>：删除空目录</li><li><code>tree</code>：以树状图显示目录结构（若未安装可 <code>sudo apt install tree</code>）</li></ul></li><li><p><strong>链接操作</strong></p><ul><li><code>ln -s 源文件 软链接</code>：创建软链接（符号链接）</li><li><code>ln 源文件 硬链接</code>：创建硬链接</li><li><code>readlink 链接</code>：查看链接指向的实际路径</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ln -s /usr/bin/python3 ~/bin/python           # 创建 Python 软链接ln important.txt backup_hard_link.txt         # 创建硬链接readlink /bin/sh                              # 查看链接指向的实际文件ls -l /bin | grep '^l'                        # 查看所有符号链接<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看文件内容</strong></p><ul><li><code>cat 文件</code>：一次性显示全文</li><li><code>cat -n</code>：显示行号</li><li><code>more/less 文件</code>：分页查看，<code>/ + 关键字</code> 搜索，<code>n</code> 跳到下一个匹配</li><li><code>head -n 10 文件</code>：查看前 10 行；<code>tail -n 10 文件</code>：后 10 行</li><li><code>tail -f 文件</code>：实时追踪文件新增内容，常用于日志监控</li><li><code>file 文件</code>：查看文件类型</li></ul></li></ol><hr><h2 id="二、文本处理与过滤-1"><a href="#二、文本处理与过滤-1" class="headerlink" title="二、文本处理与过滤"></a>二、文本处理与过滤</h2><ol><li><p><strong>过滤与查找</strong></p><ul><li><code>grep 模式 文件</code>：查找匹配行；<code>-i</code> 忽略大小写；<code>-n</code> 显示行号；<code>-v</code> 反向查找</li><li><code>grep -r 模式 目录</code>：递归搜索目录</li><li><code>grep -E &#39;正则表达式&#39;</code>：使用扩展正则表达式</li><li><code>egrep</code>：等同于 <code>grep -E</code></li><li><code>awk &#39;&#123;print $1,$3&#125;&#39; 文件</code>：按空格分列，打印第 1、3 列</li><li><code>cut -d&#39;:&#39; -f1 /etc/passwd</code>：以 <code>:</code> 分割，取第一列</li><li><code>sed &#39;s/old/new/g&#39; 文件</code>：替换（默认只输出到标准输出）；加 <code>-i</code> 直接修改文件</li><li><code>sed -n &#39;1,10p&#39; 文件</code>：只显示第 1-10 行</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">grep -n "ERROR" server.log                    # 查找错误日志，显示行号grep -r "TODO" ./src                          # 递归搜索源代码中的 TODOgrep -E '^[0-9]&#123;3&#125;-[0-9]&#123;4&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>排序与去重</strong></p><ul><li><code>sort 文件</code>：排序；<code>sort -n</code> 数值排序；<code>-r</code> 逆序；<code>-k n</code> 指定第 n 列排序</li><li><code>uniq 文件</code>：去除相邻重复；<code>uniq -c</code> 统计次数；常与 <code>sort</code> 管道配合：<pre class="line-numbers language-language-bash"><code class="language-language-bash">sort abc.txt | uniq -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><code>comm 文件1 文件2</code>：比较两个已排序文件的异同</li></ul></li><li><p><strong>统计与比较</strong></p><ul><li><code>wc -l 文件</code>：统计行数；<code>-m</code> 字符数；<code>-c</code> 字节数；<code>-L</code> 最长行长度；<code>-w</code> 单词数</li><li><code>diff 文件1 文件2</code>：比较文件差异</li><li><code>diff -u</code>：以统一格式显示差异</li><li><code>cmp 文件1 文件2</code>：二进制比较文件</li></ul></li><li><p><strong>字符转换</strong></p><ul><li><code>tr &#39;a-z&#39; &#39;A-Z&#39; &lt; 文件</code>：小写转大写</li><li><code>tr -d &#39;0-9&#39;</code>：删除所有数字</li><li><code>tr -s &#39; &#39;</code>：压缩连续空格为单个空格</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">echo "hello world" | tr 'a-z' 'A-Z'          # 转换为大写: HELLO WORLDecho "user123pass456" | tr -d '0-9'          # 删除数字: userpassecho "too    many    spaces" | tr -s ' '      # 压缩空格: too many spacescat file.txt | tr '\n' ' '                    # 将换行符替换为空格<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="三、权限与拥有者-1"><a href="#三、权限与拥有者-1" class="headerlink" title="三、权限与拥有者"></a>三、权限与拥有者</h2><ol><li><p><strong>查看权限</strong></p><ul><li><code>ls -l</code> 第一列如 <code>-rwxr-xr--</code>：分别是文件类型、用户&#x2F;组&#x2F;其他用户的读(r)&#x2F;写(w)&#x2F;执行(x) 权限</li><li><code>stat 文件</code>：查看文件详细状态信息</li></ul></li><li><p><strong>修改权限</strong></p><ul><li><code>chmod u+rw 文件</code>：用户添加读写权限；也可用数字模式，如 <code>chmod 755 文件</code></li><li><code>chmod g-w 文件</code>：组移除写权限</li><li><code>chmod o+x 文件</code>：其他用户添加执行权限</li><li><code>chmod -R 755 目录</code>：递归修改目录及子目录权限</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">chmod 644 file.txt                   # 设置 rw-r--r-- 权限chmod u+x script.sh                  # 给所有者添加执行权限chmod -R 755 /var/www/html          # 递归设置 web 目录权限chmod a-x sensitive.dat             # 移除所有用户的执行权限<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>修改拥有者&#x2F;所属组</strong></p><ul><li><code>chown user:group 文件</code>：同时修改用户与组</li><li><code>chown user 文件</code> 或 <code>chown :group 文件</code></li><li><code>chown -R user:group 目录</code>：递归修改目录权限</li></ul></li><li><p><strong>特殊权限</strong></p><ul><li><code>chmod u+s 文件</code>：设置 SUID 权限</li><li><code>chmod g+s 目录</code>：设置 SGID 权限</li><li><code>chmod +t 目录</code>：设置粘滞位</li><li><code>umask</code>：查看默认权限掩码</li><li><code>umask 022</code>：设置默认权限掩码</li></ul></li><li><p><strong>文件属性</strong></p><ul><li><code>lsattr 文件</code>：查看文件特殊属性</li><li><code>chattr +i 文件</code>：设置不可修改属性</li><li><code>chattr +a 文件</code>：设置只能追加属性</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">chattr +i /etc/resolv.conf           # 防止文件被修改chattr +a /var/log/secure           # 只允许追加内容lsattr /etc/resolv.conf             # 查看文件属性chattr -i /etc/resolv.conf          # 移除不可修改属性<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="四、进程与作业管理-1"><a href="#四、进程与作业管理-1" class="headerlink" title="四、进程与作业管理"></a>四、进程与作业管理</h2><ol><li><p><strong>查看进程</strong></p><ul><li><code>ps -ef</code>：列出所有进程；常与 <code>grep</code> 结合过滤</li><li><code>ps aux</code>：显示详细进程信息</li><li><code>pstree</code>：以树状图显示进程关系</li><li><code>top</code> &#x2F; <code>htop</code>：动态监控 CPU&#x2F;内存&#x2F;进程（按 q 退出）</li><li><code>pgrep 进程名</code>：按名称查找进程 ID</li></ul></li><li><p><strong>杀死进程</strong></p><ul><li><code>kill PID</code>：温和终止（发送 SIGTERM 信号）</li><li><code>kill -9 PID</code>：强制终止（发送 SIGKILL 信号）</li><li><code>kill -l</code>：列出所有信号</li><li><code>pkill 进程名</code> &#x2F; <code>killall 进程名</code>：按名杀死</li><li><code>killall -u 用户名</code>：杀死指定用户的所有进程</li></ul></li><li><p><strong>进程优先级</strong></p><ul><li><code>nice -n 10 命令</code>：以较低优先级运行命令</li><li><code>renice +10 -p PID</code>：调整运行中进程的优先级</li></ul></li><li><p><strong>后台与作业控制</strong></p><ul><li><code>command &amp;</code>：后台运行</li><li><code>jobs</code>：查看后台作业，<code>fg %1</code> 将第 1 个作业拉到前台，<code>bg %1</code> 让其后台运行</li><li><code>nohup command &amp;</code>：后台运行且忽略挂断信号（退出终端后继续运行）</li><li><code>Ctrl+Z</code>：挂起当前进程</li><li><code>Ctrl+C</code>：终止当前进程</li></ul></li></ol><hr><h2 id="五、网络与远程操作-1"><a href="#五、网络与远程操作-1" class="headerlink" title="五、网络与远程操作"></a>五、网络与远程操作</h2><ol><li><p><strong>网络诊断</strong></p><ul><li><code>ping 主机</code>：连通性测试</li><li><code>ping -c 4</code>：只发送 4 个数据包</li><li><code>traceroute 主机</code>：路由追踪</li><li><code>mtr 主机</code>：结合 ping 和 traceroute 功能</li><li><code>netstat -anp</code>：查看端口、连接；常与 <code>grep</code> 结合</li><li><code>ss -tulnp</code>：更现代的端口查看工具</li><li><code>nmap 主机</code>：端口扫描（需要安装）</li></ul></li><li><p><strong>DNS 查询</strong></p><ul><li><code>nslookup 域名</code>：查询 DNS 记录</li><li><code>dig 域名</code>：详细 DNS 查询</li><li><code>host 域名</code>：简单 DNS 查询</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">nslookup google.com                  # 查询域名的 IP 地址dig @8.8.8.8 example.com            # 使用指定 DNS 服务器查询dig example.com MX                   # 查询邮件服务器记录host -t AAAA ipv6.google.com        # 查询 IPv6 地址<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>网络配置</strong></p><ul><li><code>ifconfig</code>：查看网络接口（较旧）</li><li><code>ip addr show</code>：查看 IP 地址（现代替代 ifconfig）</li><li><code>ip route show</code>：查看路由表</li><li><code>hostname</code>：显示主机名</li><li><code>hostname -I</code>：显示所有 IP 地址</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">ip addr show eth0                    # 显示特定网卡信息ip route add 192.168.1.0/24 via 192.168.1.1  # 添加静态路由ip link set eth0 up                  # 启用网卡ifconfig eth0 192.168.1.100         # 设置 IP 地址（旧方式）sudo hostnamectl set-hostname newname  # 修改主机名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>文件传输与远程登录</strong></p><ul><li><code>ssh user@host</code>：远程登录</li><li><code>ssh -p 端口 user@host</code>：指定端口登录</li><li><code>scp 本地 远端</code> &#x2F; <code>scp user@host:远端 本地</code>：安全复制</li><li><code>sftp user@host</code>：安全文件传输</li><li><code>rsync -avz 源 目标</code>：增量同步</li><li><code>rsync -avz --delete 源 目标</code>：同步并删除目标中源没有的文件</li></ul></li><li><p><strong>下载工具</strong></p><ul><li><code>wget URL</code>：下载文件</li><li><code>wget -c URL</code>：断点续传</li><li><code>curl URL</code>：发送网络请求</li><li><code>curl -O URL</code>：下载文件</li><li><code>curl -H &quot;Header: Value&quot; URL</code>：添加请求头</li></ul></li><li><p><strong>查看公网 IP</strong></p><ul><li><code>curl -4 ifconfig.co</code> 或 <code>curl ipinfo.io/ip</code></li><li><code>curl icanhazip.com</code></li></ul></li></ol><hr><h2 id="六、磁盘与存储-1"><a href="#六、磁盘与存储-1" class="headerlink" title="六、磁盘与存储"></a>六、磁盘与存储</h2><ol><li><p><strong>磁盘使用情况</strong></p><ul><li><code>df -h</code>：查看各分区总量与剩余空间</li><li><code>df -i</code>：查看 inode 使用情况</li><li><code>du -sh 目录</code>：查看目录总占用；<code>du -h --max-depth=1</code> 查看子目录分布</li><li><code>du -h | sort -h</code>：按大小排序显示</li><li><code>ncdu</code>：交互式磁盘使用分析（需要安装）</li></ul></li><li><p><strong>块设备管理</strong></p><ul><li><code>lsblk</code>：列出块设备信息</li><li><code>blkid</code>：显示块设备属性</li><li><code>fdisk -l</code>：列出磁盘分区信息</li><li><code>parted -l</code>：列出分区信息（支持 GPT）</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">lsblk -f                             # 显示文件系统类型blkid /dev/sda1                      # 显示分区 UUID 和类型sudo fdisk -l /dev/sda               # 查看磁盘分区表sudo parted /dev/sda print           # 显示分区信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>文件系统操作</strong></p><ul><li><code>mount 设备 挂载点</code>：挂载文件系统</li><li><code>umount 设备/挂载点</code>：卸载文件系统</li><li><code>mount -o remount,rw /</code>：重新挂载为读写模式</li><li><code>fsck 设备</code>：文件系统检查和修复</li><li><code>mkfs.ext4 设备</code>：创建 ext4 文件系统</li></ul></li><li><p><strong>打包压缩</strong></p><ul><li><code>tar -czvf archive.tar.gz 目录/文件</code>：打包并 gzip 压缩</li><li><code>tar -xzvf archive.tar.gz</code>：解压</li><li><code>tar -tf archive.tar</code>：查看压缩包内容而不解压</li><li><code>zip -r archive.zip 目录</code> &#x2F; <code>unzip archive.zip</code></li><li><code>gzip 文件</code> &#x2F; <code>gunzip 文件.gz</code></li><li><code>bzip2 文件</code> &#x2F; <code>bunzip2 文件.bz2</code></li><li><code>xz 文件</code> &#x2F; <code>unxz 文件.xz</code></li></ul></li><li><p><strong>数据复制与备份</strong></p><ul><li><code>dd if=源 of=目标 bs=块大小</code>：数据复制</li><li><code>dd if=/dev/zero of=file bs=1M count=100</code>：创建100M空文件</li><li><code>cp --sparse=always 文件 目标</code>：复制稀疏文件</li></ul></li></ol><hr><h2 id="七、查找与批处理-1"><a href="#七、查找与批处理-1" class="headerlink" title="七、查找与批处理"></a>七、查找与批处理</h2><ol><li><p><strong>查找文件</strong></p><ul><li><code>find . -name &#39;*.log&#39;</code>：当前目录及子目录查找</li><li><code>find /path -type f -mtime -7</code>：查找 7 天内修改的文件</li><li><code>find . -type f -name &#39;xiaohub.log.2024-06*&#39; -delete</code>：批量删除</li><li><code>find . -size +100M</code>：查找大于 100M 的文件</li><li><code>find . -empty</code>：查找空文件或空目录</li><li><code>find . -perm 777</code>：查找权限为 777 的文件</li></ul></li><li><p><strong>快速定位</strong></p><ul><li><code>locate 文件名</code>：快速查找文件（基于数据库）</li><li><code>updatedb</code>：更新 locate 数据库</li><li><code>which 命令</code>：查找命令的完整路径</li><li><code>whereis 命令</code>：查找二进制、源代码和手册页位置</li><li><code>type 命令</code>：显示命令类型（别名、内置、外部命令）</li></ul></li><li><p><strong>批量执行</strong></p><ul><li>与 <code>find</code> + <code>-exec</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.sh' -exec chmod +x &#123;&#125; \;find /var/log -name '*.log' -exec cp &#123;&#125; &#123;&#125;.bak \;find . -type f -exec md5sum &#123;&#125; \; > checksums.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><code>xargs</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.txt' | xargs grep '关键字'find . -name '*.tmp' | xargs rm -fls *.jpg | xargs -I &#123;&#125; convert &#123;&#125; &#123;&#125;.png<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><code>parallel</code>：并行执行命令（需要安装）<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.jpg' | parallel convert &#123;&#125; &#123;.&#125;.pngcat urls.txt | parallel wget &#123;&#125;seq 1 10 | parallel -j4 'echo "Process &#123;&#125;"'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li></ol><hr><h2 id="八、Shell-环境与脚本-1"><a href="#八、Shell-环境与脚本-1" class="headerlink" title="八、Shell 环境与脚本"></a>八、Shell 环境与脚本</h2><ol><li><p><strong>环境变量</strong></p><ul><li><code>export VAR=value</code>：临时设置；写入 <code>~/.bashrc</code> 或 <code>~/.profile</code> 实现持久</li><li><code>echo $VAR</code>：显示变量值</li><li><code>env</code>：显示所有环境变量</li><li><code>set</code>：显示所有变量（包括局部变量）</li><li><code>unset VAR</code>：删除变量</li><li><code>source 文件</code> 或 <code>.</code>：重新加载配置</li></ul></li><li><p><strong>Shell 配置</strong></p><ul><li><code>~/.bashrc</code>：bash shell 配置文件</li><li><code>~/.bash_profile</code>：登录 shell 配置文件</li><li><code>~/.bash_history</code>：命令历史记录</li></ul></li><li><p><strong>常见提示</strong></p><ul><li><code>history</code>：查看历史命令；<code>!n</code> 重复第 n 条，<code>!!</code> 重复上一条</li><li><code>history -c</code>：清除历史记录</li><li><code>Ctrl+R</code>：反向搜索历史命令</li><li>Tab 自动补全</li><li><code>alias ll=&#39;ls -l&#39;</code>：自定义快捷命令，写入 <code>~/.bashrc</code> 生效</li><li><code>unalias 别名</code>：删除别名</li></ul></li><li><p><strong>目录操作</strong></p><ul><li><code>pushd 目录</code>：将目录压入堆栈并切换</li><li><code>popd</code>：从堆栈弹出目录并切换</li><li><code>dirs</code>：显示目录堆栈</li></ul></li><li><p><strong>脚本规范</strong></p><ul><li>首行 <code>#!/bin/bash</code> 或 <code>#!/usr/bin/env bash</code></li><li>脚本执行前加执行权限：<code>chmod +x script.sh</code></li><li>参数获取：<code>$1,$2,…</code>；循环 <code>for arg in &quot;$@&quot;; do …; done</code></li><li><code>$0</code>：脚本名称</li><li><code>$#</code>：参数个数</li><li><code>$?</code>：上个命令的退出状态</li><li><code>test</code> 或 <code>[ ]</code>：条件测试</li><li><code>[[ ]]</code>：扩展的条件测试（支持正则）</li></ul></li></ol><hr><h2 id="九、系统管理-1"><a href="#九、系统管理-1" class="headerlink" title="九、系统管理"></a>九、系统管理</h2><ol><li><p><strong>服务与日志</strong></p><ul><li><code>systemctl status 服务名</code>：查看服务状态</li><li><code>systemctl start/stop/restart 服务名</code>：启停服务</li><li><code>systemctl enable/disable 服务名</code>：开机自启设置</li><li><code>journalctl -u 服务名</code>：查看 systemd 日志</li><li><code>journalctl -f</code>：实时查看系统日志</li></ul></li><li><p><strong>定时任务</strong></p><ul><li><code>crontab -e</code>：编辑当前用户 crontab</li><li><code>crontab -l</code>：列出当前用户的定时任务</li><li>格式：<code>* * * * * command</code>（分 时 日 月 周）</li><li><code>/etc/crontab</code>：系统级定时任务</li></ul></li><li><p><strong>用户与组管理</strong></p><ul><li><code>useradd 用户名</code> &#x2F; <code>usermod -aG group 用户名</code> &#x2F; <code>userdel 用户名</code></li><li><code>passwd 用户名</code>：设置用户密码</li><li><code>groups 用户名</code>：查看用户所属组</li><li><code>id 用户名</code>：显示用户和组 ID</li><li><code>su - 用户名</code>：切换用户</li><li><code>sudo 命令</code>：以 root 权限执行命令</li></ul></li><li><p><strong>系统监控</strong></p><ul><li><code>free -h</code>：查看内存使用情况</li><li><code>uptime</code>：系统运行时间和负载</li><li><code>dmesg</code>：查看内核消息</li><li><code>lsof</code>：列出打开的文件</li><li><code>lsof -i :端口</code>：查看端口占用情况</li><li><code>vmstat</code>：虚拟内存统计</li><li><code>iostat</code>：IO 统计</li><li><code>sar</code>：系统活动报告（需要安装 sysstat）</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">free -h                              # 显示人性化的内存使用信息uptime                               # 显示运行时间和负载dmesg | tail -20                     # 查看最近的内核消息lsof -i :80                          # 查看 80 端口占用lsof -p 1234                         # 查看进程 1234 打开的文件vmstat 2 5                           # 每 2 秒更新一次，共 5 次iostat -x 1                          # 每秒显示详细 IO 统计sar -u 1 5                           # CPU 使用率，每秒更新，共 5 次<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><hr><h2 id="十、软件包管理-1"><a href="#十、软件包管理-1" class="headerlink" title="十、软件包管理"></a>十、软件包管理</h2><ol><li><p><strong>Debian&#x2F;Ubuntu（APT）</strong></p><ul><li><code>apt update</code>：更新软件包列表</li><li><code>apt upgrade</code>：升级已安装软件包</li><li><code>apt install 软件包</code>：安装软件</li><li><code>apt remove 软件包</code>：删除软件</li><li><code>apt search 关键字</code>：搜索软件包</li><li><code>apt show 软件包</code>：显示软件包信息</li><li><code>apt autoremove</code>：删除不需要的依赖包</li></ul></li><li><p><strong>RHEL&#x2F;CentOS（YUM&#x2F;DNF）</strong></p><ul><li><code>yum update</code>：更新软件包</li><li><code>yum install 软件包</code>：安装软件</li><li><code>yum remove 软件包</code>：删除软件</li><li><code>yum search 关键字</code>：搜索软件包</li><li><code>yum info 软件包</code>：显示软件包信息</li><li><code>dnf</code>：新版本的 Fedora&#x2F;RHEL 使用 dnf 替代 yum</li></ul></li><li><p><strong>常用工具安装</strong></p><ul><li><code>apt install net-tools</code>：安装传统网络工具（ifconfig 等）</li><li><code>apt install vim</code>：安装 vim 编辑器</li><li><code>apt install htop</code>：安装 htop 进程监控工具</li><li><code>apt install ncdu</code>：安装磁盘使用分析工具</li></ul></li></ol><hr><h2 id="十一、快捷键与技巧-1"><a href="#十一、快捷键与技巧-1" class="headerlink" title="十一、快捷键与技巧"></a>十一、快捷键与技巧</h2><ol><li><strong>终端快捷键</strong><ul><li><code>Ctrl+A</code>：移到行首</li><li><code>Ctrl+E</code>：移到行尾</li><li><code>Ctrl+U</code>：删除光标前的内容</li><li><code>Ctrl+K</code>：删除光标后的内容</li><li><code>Ctrl+W</code>：删除光标前的单词</li><li><code>Ctrl+L</code>：清屏（等同于 clear）</li><li><code>Ctrl+D</code>：退出当前 shell</li></ul></li></ol><p>：引用上条命令的最后一个参数</p><ul><li><code>!*</code>：引用上条命令的所有参数</li><li><code>command1 &amp;&amp; command2</code>：前一个命令成功后才执行后一个</li><li><code>command1 || command2</code>：前一个命令失败后才执行后一个</li><li><code>command1; command2</code>：顺序执行，不管是否成功</li><li><code>$(command)</code>：命令替换</li><li><code>`command`</code>：命令替换（旧式）</li></ul><pre class="line-numbers language-language-bash"><code class="language-language-bash">sudo !!                              # 以 root 权限重新执行上条命令cd !$                                # 切换到上条命令的最后一个参数echo !*                              # 显示上条命令的所有参数mkdir test && cd test                # 创建目录并进入ping -c1 google.com || echo "网络不通"  # 网络不通时提示echo "今天是 $(date +%Y-%m-%d)"      # 命令替换显示日期for file in *.txt; do echo $file; done  # 遍历文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li><p><strong>通配符与正则</strong></p><ul><li><code>*</code>：匹配任意多个字符</li><li><code>?</code>：匹配单个字符</li><li><code>[abc]</code>：匹配中括号内任一字符</li><li><code>[a-z]</code>：匹配范围内任一字符</li><li><code>&#123;a,b,c&#125;</code>：匹配大括号内任一字符串</li></ul></li><li><p><strong>重定向与管道</strong></p><ul><li><code>&gt;</code>：输出重定向（覆盖）</li><li><code>&gt;&gt;</code>：输出重定向（追加）</li><li><code>2&gt;</code>：错误输出重定向</li><li><code>&amp;&gt;</code>：标准输出和错误都重定向</li><li><code>|</code>：管道，将前一个命令的输出作为后一个命令的输入</li><li><code>tee</code>：将输出同时写入文件和标准输出 phone.txt      # 正则匹配电话号码格式<br>ps aux | grep nginx | grep -v grep            # 查找 nginx 进程（排除 grep 自身）<br>awk -F: ‘{print $1,$7}’ &#x2F;etc&#x2F;passwd           # 显示用户名和 shell<br>df -h | awk ‘NR&gt;1{print $5,$6}’              # 显示磁盘使用率和挂载点<br>sed -i ‘s&#x2F;localhost&#x2F;127.0.0.1&#x2F;g’ config.ini   # 直接替换文件内容<br>sed -n ‘&#x2F;start&#x2F;,&#x2F;end&#x2F;p’ log.txt               # 显示两个标记之间的行</li></ul><pre><code></code></pre></li><li><p><strong>排序与去重</strong></p><ul><li><code>sort 文件</code>：排序；<code>sort -n</code> 数值排序；<code>-r</code> 逆序；<code>-k n</code> 指定第 n 列排序</li><li><code>uniq 文件</code>：去除相邻重复；<code>uniq -c</code> 统计次数；常与 <code>sort</code> 管道配合：<pre class="line-numbers language-language-bash"><code class="language-language-bash">sort abc.txt | uniq -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><code>comm 文件1 文件2</code>：比较两个已排序文件的异同</li></ul></li><li><p><strong>统计与比较</strong></p><ul><li><code>wc -l 文件</code>：统计行数；<code>-m</code> 字符数；<code>-c</code> 字节数；<code>-L</code> 最长行长度；<code>-w</code> 单词数</li><li><code>diff 文件1 文件2</code>：比较文件差异</li><li><code>diff -u</code>：以统一格式显示差异</li><li><code>cmp 文件1 文件2</code>：二进制比较文件</li></ul></li><li><p><strong>字符转换</strong></p><ul><li><code>tr &#39;a-z&#39; &#39;A-Z&#39; &lt; 文件</code>：小写转大写</li><li><code>tr -d &#39;0-9&#39;</code>：删除所有数字</li><li><code>tr -s &#39; &#39;</code>：压缩连续空格为单个空格</li></ul></li></ol><hr><h2 id="三、权限与拥有者-2"><a href="#三、权限与拥有者-2" class="headerlink" title="三、权限与拥有者"></a>三、权限与拥有者</h2><ol><li><p><strong>查看权限</strong></p><ul><li><code>ls -l</code> 第一列如 <code>-rwxr-xr--</code>：分别是文件类型、用户&#x2F;组&#x2F;其他用户的读(r)&#x2F;写(w)&#x2F;执行(x) 权限</li><li><code>stat 文件</code>：查看文件详细状态信息</li></ul></li><li><p><strong>修改权限</strong></p><ul><li><code>chmod u+rw 文件</code>：用户添加读写权限；也可用数字模式，如 <code>chmod 755 文件</code></li><li><code>chmod g-w 文件</code>：组移除写权限</li><li><code>chmod o+x 文件</code>：其他用户添加执行权限</li><li><code>chmod -R 755 目录</code>：递归修改目录及子目录权限</li></ul></li><li><p><strong>修改拥有者&#x2F;所属组</strong></p><ul><li><code>chown user:group 文件</code>：同时修改用户与组</li><li><code>chown user 文件</code> 或 <code>chown :group 文件</code></li><li><code>chown -R user:group 目录</code>：递归修改目录权限</li></ul></li><li><p><strong>特殊权限</strong></p><ul><li><code>chmod u+s 文件</code>：设置 SUID 权限</li><li><code>chmod g+s 目录</code>：设置 SGID 权限</li><li><code>chmod +t 目录</code>：设置粘滞位</li><li><code>umask</code>：查看默认权限掩码</li><li><code>umask 022</code>：设置默认权限掩码</li></ul></li><li><p><strong>文件属性</strong></p><ul><li><code>lsattr 文件</code>：查看文件特殊属性</li><li><code>chattr +i 文件</code>：设置不可修改属性</li><li><code>chattr +a 文件</code>：设置只能追加属性</li></ul></li></ol><hr><h2 id="四、进程与作业管理-2"><a href="#四、进程与作业管理-2" class="headerlink" title="四、进程与作业管理"></a>四、进程与作业管理</h2><ol><li><p><strong>查看进程</strong></p><ul><li><code>ps -ef</code>：列出所有进程；常与 <code>grep</code> 结合过滤</li><li><code>ps aux</code>：显示详细进程信息</li><li><code>pstree</code>：以树状图显示进程关系</li><li><code>top</code> &#x2F; <code>htop</code>：动态监控 CPU&#x2F;内存&#x2F;进程（按 q 退出）</li><li><code>pgrep 进程名</code>：按名称查找进程 ID</li></ul></li><li><p><strong>杀死进程</strong></p><ul><li><code>kill PID</code>：温和终止（发送 SIGTERM 信号）</li><li><code>kill -9 PID</code>：强制终止（发送 SIGKILL 信号）</li><li><code>kill -l</code>：列出所有信号</li><li><code>pkill 进程名</code> &#x2F; <code>killall 进程名</code>：按名杀死</li><li><code>killall -u 用户名</code>：杀死指定用户的所有进程</li></ul></li><li><p><strong>进程优先级</strong></p><ul><li><code>nice -n 10 命令</code>：以较低优先级运行命令</li><li><code>renice +10 -p PID</code>：调整运行中进程的优先级</li></ul></li><li><p><strong>后台与作业控制</strong></p><ul><li><code>command &amp;</code>：后台运行</li><li><code>jobs</code>：查看后台作业，<code>fg %1</code> 将第 1 个作业拉到前台，<code>bg %1</code> 让其后台运行</li><li><code>nohup command &amp;</code>：后台运行且忽略挂断信号（退出终端后继续运行）</li><li><code>Ctrl+Z</code>：挂起当前进程</li><li><code>Ctrl+C</code>：终止当前进程</li></ul></li></ol><hr><h2 id="五、网络与远程操作-2"><a href="#五、网络与远程操作-2" class="headerlink" title="五、网络与远程操作"></a>五、网络与远程操作</h2><ol><li><p><strong>网络诊断</strong></p><ul><li><code>ping 主机</code>：连通性测试</li><li><code>ping -c 4</code>：只发送 4 个数据包</li><li><code>traceroute 主机</code>：路由追踪</li><li><code>mtr 主机</code>：结合 ping 和 traceroute 功能</li><li><code>netstat -anp</code>：查看端口、连接；常与 <code>grep</code> 结合</li><li><code>ss -tulnp</code>：更现代的端口查看工具</li><li><code>nmap 主机</code>：端口扫描（需要安装）</li></ul></li><li><p><strong>DNS 查询</strong></p><ul><li><code>nslookup 域名</code>：查询 DNS 记录</li><li><code>dig 域名</code>：详细 DNS 查询</li><li><code>host 域名</code>：简单 DNS 查询</li></ul></li><li><p><strong>网络配置</strong></p><ul><li><code>ifconfig</code>：查看网络接口（较旧）</li><li><code>ip addr show</code>：查看 IP 地址（现代替代 ifconfig）</li><li><code>ip route show</code>：查看路由表</li><li><code>hostname</code>：显示主机名</li><li><code>hostname -I</code>：显示所有 IP 地址</li></ul></li><li><p><strong>文件传输与远程登录</strong></p><ul><li><code>ssh user@host</code>：远程登录</li><li><code>ssh -p 端口 user@host</code>：指定端口登录</li><li><code>scp 本地 远端</code> &#x2F; <code>scp user@host:远端 本地</code>：安全复制</li><li><code>sftp user@host</code>：安全文件传输</li><li><code>rsync -avz 源 目标</code>：增量同步</li><li><code>rsync -avz --delete 源 目标</code>：同步并删除目标中源没有的文件</li></ul></li><li><p><strong>下载工具</strong></p><ul><li><code>wget URL</code>：下载文件</li><li><code>wget -c URL</code>：断点续传</li><li><code>curl URL</code>：发送网络请求</li><li><code>curl -O URL</code>：下载文件</li><li><code>curl -H &quot;Header: Value&quot; URL</code>：添加请求头</li></ul></li><li><p><strong>查看公网 IP</strong></p><ul><li><code>curl -4 ifconfig.co</code> 或 <code>curl ipinfo.io/ip</code></li><li><code>curl icanhazip.com</code></li></ul></li></ol><hr><h2 id="六、磁盘与存储-2"><a href="#六、磁盘与存储-2" class="headerlink" title="六、磁盘与存储"></a>六、磁盘与存储</h2><ol><li><p><strong>磁盘使用情况</strong></p><ul><li><code>df -h</code>：查看各分区总量与剩余空间</li><li><code>df -i</code>：查看 inode 使用情况</li><li><code>du -sh 目录</code>：查看目录总占用；<code>du -h --max-depth=1</code> 查看子目录分布</li><li><code>du -h | sort -h</code>：按大小排序显示</li><li><code>ncdu</code>：交互式磁盘使用分析（需要安装）</li></ul></li><li><p><strong>块设备管理</strong></p><ul><li><code>lsblk</code>：列出块设备信息</li><li><code>blkid</code>：显示块设备属性</li><li><code>fdisk -l</code>：列出磁盘分区信息</li><li><code>parted -l</code>：列出分区信息（支持 GPT）</li></ul></li><li><p><strong>文件系统操作</strong></p><ul><li><code>mount 设备 挂载点</code>：挂载文件系统</li><li><code>umount 设备/挂载点</code>：卸载文件系统</li><li><code>mount -o remount,rw /</code>：重新挂载为读写模式</li><li><code>fsck 设备</code>：文件系统检查和修复</li><li><code>mkfs.ext4 设备</code>：创建 ext4 文件系统</li></ul></li><li><p><strong>打包压缩</strong></p><ul><li><code>tar -czvf archive.tar.gz 目录/文件</code>：打包并 gzip 压缩</li><li><code>tar -xzvf archive.tar.gz</code>：解压</li><li><code>tar -tf archive.tar</code>：查看压缩包内容而不解压</li><li><code>zip -r archive.zip 目录</code> &#x2F; <code>unzip archive.zip</code></li><li><code>gzip 文件</code> &#x2F; <code>gunzip 文件.gz</code></li><li><code>bzip2 文件</code> &#x2F; <code>bunzip2 文件.bz2</code></li><li><code>xz 文件</code> &#x2F; <code>unxz 文件.xz</code></li></ul></li><li><p><strong>数据复制与备份</strong></p><ul><li><code>dd if=源 of=目标 bs=块大小</code>：数据复制</li><li><code>dd if=/dev/zero of=file bs=1M count=100</code>：创建100M空文件</li><li><code>cp --sparse=always 文件 目标</code>：复制稀疏文件</li></ul></li></ol><hr><h2 id="七、查找与批处理-2"><a href="#七、查找与批处理-2" class="headerlink" title="七、查找与批处理"></a>七、查找与批处理</h2><ol><li><p><strong>查找文件</strong></p><ul><li><code>find . -name &#39;*.log&#39;</code>：当前目录及子目录查找</li><li><code>find /path -type f -mtime -7</code>：查找 7 天内修改的文件</li><li><code>find . -type f -name &#39;xiaohub.log.2024-06*&#39; -delete</code>：批量删除</li><li><code>find . -size +100M</code>：查找大于 100M 的文件</li><li><code>find . -empty</code>：查找空文件或空目录</li><li><code>find . -perm 777</code>：查找权限为 777 的文件</li></ul></li><li><p><strong>快速定位</strong></p><ul><li><code>locate 文件名</code>：快速查找文件（基于数据库）</li><li><code>updatedb</code>：更新 locate 数据库</li><li><code>which 命令</code>：查找命令的完整路径</li><li><code>whereis 命令</code>：查找二进制、源代码和手册页位置</li><li><code>type 命令</code>：显示命令类型（别名、内置、外部命令）</li></ul></li><li><p><strong>批量执行</strong></p><ul><li>与 <code>find</code> + <code>-exec</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.sh' -exec chmod +x &#123;&#125; \;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><code>xargs</code>：<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.txt' | xargs grep '关键字'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><code>parallel</code>：并行执行命令（需要安装）<pre class="line-numbers language-language-bash"><code class="language-language-bash">find . -name '*.jpg' | parallel convert &#123;&#125; &#123;.&#125;.png<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ol><hr><h2 id="八、Shell-环境与脚本-2"><a href="#八、Shell-环境与脚本-2" class="headerlink" title="八、Shell 环境与脚本"></a>八、Shell 环境与脚本</h2><ol><li><p><strong>环境变量</strong></p><ul><li><code>export VAR=value</code>：临时设置；写入 <code>~/.bashrc</code> 或 <code>~/.profile</code> 实现持久</li><li><code>echo $VAR</code>：显示变量值</li><li><code>env</code>：显示所有环境变量</li><li><code>set</code>：显示所有变量（包括局部变量）</li><li><code>unset VAR</code>：删除变量</li><li><code>source 文件</code> 或 <code>.</code>：重新加载配置</li></ul></li><li><p><strong>Shell 配置</strong></p><ul><li><code>~/.bashrc</code>：bash shell 配置文件</li><li><code>~/.bash_profile</code>：登录 shell 配置文件</li><li><code>~/.bash_history</code>：命令历史记录</li></ul></li><li><p><strong>常见提示</strong></p><ul><li><code>history</code>：查看历史命令；<code>!n</code> 重复第 n 条，<code>!!</code> 重复上一条</li><li><code>history -c</code>：清除历史记录</li><li><code>Ctrl+R</code>：反向搜索历史命令</li><li>Tab 自动补全</li><li><code>alias ll=&#39;ls -l&#39;</code>：自定义快捷命令，写入 <code>~/.bashrc</code> 生效</li><li><code>unalias 别名</code>：删除别名</li></ul></li><li><p><strong>目录操作</strong></p><ul><li><code>pushd 目录</code>：将目录压入堆栈并切换</li><li><code>popd</code>：从堆栈弹出目录并切换</li><li><code>dirs</code>：显示目录堆栈</li></ul></li><li><p><strong>脚本规范</strong></p><ul><li>首行 <code>#!/bin/bash</code> 或 <code>#!/usr/bin/env bash</code></li><li>脚本执行前加执行权限：<code>chmod +x script.sh</code></li><li>参数获取：<code>$1,$2,…</code>；循环 <code>for arg in &quot;$@&quot;; do …; done</code></li><li><code>$0</code>：脚本名称</li><li><code>$#</code>：参数个数</li><li><code>$?</code>：上个命令的退出状态</li><li><code>test</code> 或 <code>[ ]</code>：条件测试</li><li><code>[[ ]]</code>：扩展的条件测试（支持正则）</li></ul></li></ol><hr><h2 id="九、系统管理-2"><a href="#九、系统管理-2" class="headerlink" title="九、系统管理"></a>九、系统管理</h2><ol><li><p><strong>服务与日志</strong></p><ul><li><code>systemctl status 服务名</code>：查看服务状态</li><li><code>systemctl start/stop/restart 服务名</code>：启停服务</li><li><code>systemctl enable/disable 服务名</code>：开机自启设置</li><li><code>journalctl -u 服务名</code>：查看 systemd 日志</li><li><code>journalctl -f</code>：实时查看系统日志</li></ul></li><li><p><strong>定时任务</strong></p><ul><li><code>crontab -e</code>：编辑当前用户 crontab</li><li><code>crontab -l</code>：列出当前用户的定时任务</li><li>格式：<code>* * * * * command</code>（分 时 日 月 周）</li><li><code>/etc/crontab</code>：系统级定时任务</li></ul></li><li><p><strong>用户与组管理</strong></p><ul><li><code>useradd 用户名</code> &#x2F; <code>usermod -aG group 用户名</code> &#x2F; <code>userdel 用户名</code></li><li><code>passwd 用户名</code>：设置用户密码</li><li><code>groups 用户名</code>：查看用户所属组</li><li><code>id 用户名</code>：显示用户和组 ID</li><li><code>su - 用户名</code>：切换用户</li><li><code>sudo 命令</code>：以 root 权限执行命令</li></ul></li><li><p><strong>系统监控</strong></p><ul><li><code>free -h</code>：查看内存使用情况</li><li><code>uptime</code>：系统运行时间和负载</li><li><code>dmesg</code>：查看内核消息</li><li><code>lsof</code>：列出打开的文件</li><li><code>lsof -i :端口</code>：查看端口占用情况</li><li><code>vmstat</code>：虚拟内存统计</li><li><code>iostat</code>：IO 统计</li><li><code>sar</code>：系统活动报告（需要安装 sysstat）</li></ul></li></ol><hr><h2 id="十、软件包管理-2"><a href="#十、软件包管理-2" class="headerlink" title="十、软件包管理"></a>十、软件包管理</h2><ol><li><p><strong>Debian&#x2F;Ubuntu（APT）</strong></p><ul><li><code>apt update</code>：更新软件包列表</li><li><code>apt upgrade</code>：升级已安装软件包</li><li><code>apt install 软件包</code>：安装软件</li><li><code>apt remove 软件包</code>：删除软件</li><li><code>apt search 关键字</code>：搜索软件包</li><li><code>apt show 软件包</code>：显示软件包信息</li><li><code>apt autoremove</code>：删除不需要的依赖包</li></ul></li><li><p><strong>RHEL&#x2F;CentOS（YUM&#x2F;DNF）</strong></p><ul><li><code>yum update</code>：更新软件包</li><li><code>yum install 软件包</code>：安装软件</li><li><code>yum remove 软件包</code>：删除软件</li><li><code>yum search 关键字</code>：搜索软件包</li><li><code>yum info 软件包</code>：显示软件包信息</li><li><code>dnf</code>：新版本的 Fedora&#x2F;RHEL 使用 dnf 替代 yum</li></ul></li><li><p><strong>常用工具安装</strong></p><ul><li><code>apt install net-tools</code>：安装传统网络工具（ifconfig 等）</li><li><code>apt install vim</code>：安装 vim 编辑器</li><li><code>apt install htop</code>：安装 htop 进程监控工具</li><li><code>apt install ncdu</code>：安装磁盘使用分析工具</li></ul></li></ol><hr><h2 id="十一、快捷键与技巧-2"><a href="#十一、快捷键与技巧-2" class="headerlink" title="十一、快捷键与技巧"></a>十一、快捷键与技巧</h2><ol><li><p><strong>终端快捷键</strong></p><ul><li><code>Ctrl+A</code>：移到行首</li><li><code>Ctrl+E</code>：移到行尾</li><li><code>Ctrl+U</code>：删除光标前的内容</li><li><code>Ctrl+K</code>：删除光标后的内容</li><li><code>Ctrl+W</code>：删除光标前的单词</li><li><code>Ctrl+L</code>：清屏（等同于 clear）</li><li><code>Ctrl+D</code>：退出当前 shell</li></ul></li><li><p><strong>命令行技巧</strong></p><ul><li><code>!!</code>：重复执行上一条命令</li><li><code>!$</code>：引用上条命令的最后一个参数</li><li><code>!*</code>：引用上条命令的所有参数</li><li><code>command1 &amp;&amp; command2</code>：前一个命令成功后才执行后一个</li><li><code>command1 || command2</code>：前一个命令失败后才执行后一个</li><li><code>command1; command2</code>：顺序执行，不管是否成功</li><li><code>$(command)</code>：命令替换</li><li><code>`command`</code>：命令替换（旧式）</li></ul></li><li><p><strong>通配符与正则</strong></p><ul><li><code>*</code>：匹配任意多个字符</li><li><code>?</code>：匹配单个字符</li><li><code>[abc]</code>：匹配中括号内任一字符</li><li><code>[a-z]</code>：匹配范围内任一字符</li><li><code>&#123;a,b,c&#125;</code>：匹配大括号内任一字符串</li></ul></li><li><p><strong>重定向与管道</strong></p><ul><li><code>&gt;</code>：输出重定向（覆盖）</li><li><code>&gt;&gt;</code>：输出重定向（追加）</li><li><code>2&gt;</code>：错误输出重定向</li><li><code>&amp;&gt;</code>：标准输出和错误都重定向</li><li><code>|</code>：管道，将前一个命令的输出作为后一个命令的输入</li><li><code>tee</code>：将输出同时写入文件和标准输出</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 笔记</title>
      <link href="/2024/07/15/docker-bi-ji/"/>
      <url>/2024/07/15/docker-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-笔记"><a href="#Docker-笔记" class="headerlink" title="Docker 笔记"></a>Docker 笔记</h1><hr><h2 id="容器是什么？镜像是什么？"><a href="#容器是什么？镜像是什么？" class="headerlink" title="容器是什么？镜像是什么？"></a>容器是什么？镜像是什么？</h2><p><strong>容器</strong>：是一个轻量级、可独立运行的软件包。它是<strong>动态的</strong>、<strong>可运行的</strong>。它是通过用镜像创建出来的一个<strong>具体的、正在运行的进程实例</strong>。</p><p>镜像：是用来创建和运行 Docker 容器的。它包含了运行某个应用所需的一切：代码、运行时环境（如 Node.js 或 Java）、系统工具、系统库和设置。</p><h2 id="分层-Layer-是什么？"><a href="#分层-Layer-是什么？" class="headerlink" title="分层 (Layer) 是什么？"></a>分层 (Layer) 是什么？</h2><p>一个 Docker 镜像并非一个单一的文件，而是由一组<strong>文件系统层 (Layers)</strong> 堆叠而成的。你可以把它想象成一个千层蛋糕。</p><ul><li><strong>只读性</strong>: 镜像的每一层都是<strong>只读的 (read-only)</strong>。</li><li><strong>来源</strong>: <code>Dockerfile</code> 中的每一条指令，通常都会创建一个新的镜像层。比如 <code>FROM</code>, <code>RUN</code>, <code>COPY</code>, <code>ADD</code> 等。</li><li><strong>共享与复用</strong>: 不同的镜像可以<strong>共享</strong>相同的层。如果它们都基于同一个基础镜像，那么这个基础镜像的层在主机上只需要存储一份。</li></ul><h3 id="工作原理：只读的镜像层-可写的容器层"><a href="#工作原理：只读的镜像层-可写的容器层" class="headerlink" title="工作原理：只读的镜像层 + 可写的容器层"></a><strong>工作原理：只读的镜像层 + 可写的容器层</strong></h3><p>当 Docker 使用一个镜像来启动一个容器时，它并不会改变这个镜像。而是在镜像的顶层添加了一个新的、<strong>可写的层</strong>，我们称之为 <strong>“容器层” (Container Layer)</strong>。</p><ul><li><strong>容器的生命周期内</strong>：你对容器做的所有更改，比如新建文件、修改文件、删除文件，都发生在这个<strong>可写的容器层</strong>里。</li><li><strong>写时复制 (Copy-on-Write)</strong>: 如果容器需要修改一个底层镜像中已存在的文件，Docker 会先把这个文件从下面的只读层复制到最上层的可写容器层，然后再进行修改。原来的只读层中的文件保持不变。这就是所谓的“写时复制”机制。</li></ul><h3 id="分层架构的好处-面试重点"><a href="#分层架构的好处-面试重点" class="headerlink" title="分层架构的好处 (面试重点)"></a><strong>分层架构的好处 (面试重点)</strong></h3><p>面试官通常会问，为什么 Docker 要采用这种分层设计？</p><ol><li><strong>高效的存储 (Efficient Storage)</strong>:<ul><li><strong>共享基础层</strong>: 假设你的主机上有三个Python服务，它们都基于<code>python:3.12-slim</code>。那么 <code>python:3.12-slim</code> 对应的所有层都只会在磁盘上存储<strong>一份</strong>。这极大地节省了磁盘空间。<ul><li><strong>共享的基础镜像 (<code>python:3.12-slim</code>)</strong>: 由 <code>[基础层]</code> 构成，大小为 <strong>100MB</strong>。</li><li><strong>镜像 a (<code>service-a</code>)</strong>:<ul><li>构成: <code>[基础层]</code> + <code>[a层(5MB)]</code></li><li><code>docker images</code> 显示的大小 ≈ <strong>105MB</strong></li></ul></li><li><strong>镜像 b (<code>service-b</code>)</strong>:<ul><li>构成: <code>[基础层]</code> + <code>[b层(8MB)]</code></li><li><code>docker images</code> 显示的大小 ≈ <strong>108MB</strong></li></ul></li><li><strong>镜像 c (<code>service-c</code>)</strong>:<ul><li>构成: <code>[基础层]</code> + <code>[c层(2MB)]</code></li><li><code>docker images</code> 显示的大小 ≈ <strong>102MB</strong></li></ul></li><li>总占用 &#x3D; 100MB (基础) + 5MB (a) + 8MB (b) + 2MB (c) &#x3D; 115MB</li></ul></li></ul></li><li><strong>更快的构建 (Faster Builds)</strong>:<ul><li><strong>利用缓存</strong>: Docker 会缓存每一层的构建结果。当你修改 <code>Dockerfile</code> 并重新构建时，Docker 只会重新构建你修改的那一行指令以及它之后的所有层，之前的层会直接使用缓存。这极大地加快了开发和构建的速度。<ul><li><p><strong>场景：开发一个简单的 Python Web 应用</strong></p><p>  假设你的项目文件夹里有这两个文件：</p><ol><li><code>requirements.txt</code> (记录 Python 依赖，比如 <code>flask</code>, <code>requests</code>)</li><li><code>app.py</code> (你的主程序代码)</li></ol><p>  你的 <code>Dockerfile</code> 是这样精心设计的（<strong>这个顺序很关键</strong>）：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># --- 关卡 1: 选择基础镜像 ---FROM python:3.9-slim# --- 关卡 2: 设置工作目录 ---WORKDIR /app# --- 关卡 3: 复制依赖文件 ---COPY requirements.txt .# --- 关卡 4: 安装依赖 (这步通常很慢) ---RUN pip install -r requirements.txt# --- 关卡 5: 复制所有项目代码 ---COPY . .# --- 关卡 6: 设置启动命令 ---CMD ["python", "app.py"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="第一次构建：从零开始"><a href="#第一次构建：从零开始" class="headerlink" title="第一次构建：从零开始"></a><strong>第一次构建：从零开始</strong></h3><p>  你第一次运行 <code>docker build -t my-app .</code>。</p><ul><li><strong>Docker 的动作</strong>：它会老老实实地从头到尾执行每一条指令。<ul><li><code>关卡1 (FROM)</code>: 执行… 完成！<strong>(创建存档1)</strong></li><li><code>关卡2 (WORKDIR)</code>: 执行… 完成！<strong>(创建存档2)</strong></li><li><code>关卡3 (COPY req...)</code>: 执行… 完成！<strong>(创建存档3)</strong></li><li><code>关卡4 (RUN pip...)</code>: 开始下载和安装库，可能需要几十秒甚至几分钟… 完成！<strong>(创建存档4)</strong></li><li><code>关卡5 (COPY . .)</code>: 执行… 完成！<strong>(创建存档5)</strong></li><li><code>关卡6 (CMD)</code>: 执行… 完成！<strong>(创建存档6)</strong></li></ul></li><li><strong>你的感受</strong>：有点慢，需要耐心等待 <code>pip install</code> 完成。</li></ul><hr><h3 id="第二次构建：只改了-Python-代码"><a href="#第二次构建：只改了-Python-代码" class="headerlink" title="第二次构建：只改了 Python 代码"></a><strong>第二次构建：只改了 Python 代码</strong></h3><p>  这是最常见的开发场景。你修改了 <code>app.py</code> 里面的逻辑，但没有动 <code>requirements.txt</code>。现在你再次运行 <code>docker build -t my-app .</code>。</p><ul><li><strong>Docker 的动作</strong>：<ul><li><code>关卡1 (FROM)</code>: 指令没变 -&gt; <strong>使用存档1 (USING CACHE)</strong></li><li><code>关卡2 (WORKDIR)</code>: 指令没变 -&gt; <strong>使用存档2 (USING CACHE)</strong></li><li><code>关卡3 (COPY req...)</code>: Docker 会检查 <code>requirements.txt</code> 文件的内容。发现文件没变 -&gt; <strong>使用存档3 (USING CACHE)</strong></li><li><code>关卡4 (RUN pip...)</code>: 因为上一层（关卡3）使用了缓存，并且本行指令也没变 -&gt; <strong>使用存档4 (USING CACHE)</strong>。<strong>最耗时的安装步骤被瞬间跳过！</strong></li><li><code>关卡5 (COPY . .)</code>: Docker 检查要复制的所有文件，发现 <code>app.py</code> 变了！ -&gt; <strong>存档5失效，重新执行！</strong></li><li><code>关卡6 (CMD)</code>: <strong>重要规则：一旦中间某一层缓存失效，它之后的所有层都会重新执行</strong> -&gt; <strong>重新执行！</strong></li></ul></li><li><strong>你的感受</strong>：构建速度极快！几乎是秒级完成，因为最慢的 <code>pip install</code> 被跳过了。</li></ul><hr><h3 id="第三次构建：修改了依赖文件"><a href="#第三次构建：修改了依赖文件" class="headerlink" title="第三次构建：修改了依赖文件"></a><strong>第三次构建：修改了依赖文件</strong></h3><p>  这次，你往 <code>requirements.txt</code> 里增加了一个新的库，比如 <code>numpy</code>。然后再次运行 <code>docker build -t my-app .</code>。</p><ul><li><strong>Docker 的动作</strong>：<ul><li><code>关卡1 (FROM)</code>: 使用缓存。</li><li><code>关卡2 (WORKDIR)</code>: 使用缓存。</li><li><code>关卡3 (COPY req...)</code>: Docker 检查 <code>requirements.txt</code>，发现文件内容变了！-&gt; <strong>存档3失效，重新执行！</strong></li><li><code>关卡4 (RUN pip...)</code>: 因为上一层（关卡3）是重新执行的 -&gt; <strong>存档4失效，必须重新执行！</strong> Docker 会再次运行 <code>pip install</code>，这次会把 <code>numpy</code> 也装上。</li><li><code>关-关6</code>: 同样全部重新执行。</li></ul></li><li><strong>你的感受</strong>：构建又变慢了。这是符合预期的，因为依赖确实发生了变化，需要重新安装。</li></ul></li></ul></li></ul></li><li><strong>高效的分发 (Efficient Distribution)</strong>:<ul><li><strong>按需传输</strong>: 当你从 Docker Hub 拉取 (<code>docker pull</code>) 一个镜像时，Docker 客户端会检查本地已经存在的层，只下载那些本地没有的层。同理，推送 (<code>docker push</code>) 镜像时也一样。这使得镜像的拉取和推送非常快。</li></ul></li></ol><h2 id="缓存失效了，之前的缓存会自动删除吗"><a href="#缓存失效了，之前的缓存会自动删除吗" class="headerlink" title="缓存失效了，之前的缓存会自动删除吗"></a>缓存失效了，之前的缓存会自动删除吗</h2><p>回答：<strong>不会，缓存失效后，旧的缓存层不会在构建时自动删除。</strong></p><p>它们会变成一种特殊的状态，我们称之为 <strong>“悬空镜像” (dangling image)</strong> 或更广义的 “悬空层” (dangling layers)。</p><h3 id="这是什么意思？我们用一个比喻来理解"><a href="#这是什么意思？我们用一个比喻来理解" class="headerlink" title="这是什么意思？我们用一个比喻来理解"></a><strong>这是什么意思？我们用一个比喻来理解</strong></h3><p>把镜像的标签（比如 <code>my-app:latest</code>）想象成一个<strong>便利贴</strong>。</p><ol><li><strong>第一次构建</strong>:<ul><li>你构建了一个镜像，我们叫它 <code>镜像A</code>。</li><li>Docker 把 <code>my-gzh:v1</code> 这张便利贴粘在了 <code>镜像A</code> 上。</li><li>此时，<code>镜像A</code> 是有主（有便利贴）的。</li></ul></li><li><strong>你修改了代码，再次构建</strong>:<ul><li>因为缓存失效，Docker 创建了一个全新的镜像，我们叫它 <code>镜像B</code>。</li><li>Docker 会把 <code>my-app:latest</code> 这张便利贴从 <code>镜像A</code> 身上<strong>撕下来</strong>，然后<strong>粘到新的 <code>镜像B</code></strong> 身上。</li></ul></li><li><strong>现在发生了什么？</strong><ul><li><code>镜像B</code> 成为了新的 <code>my-app:latest</code>。</li><li>而 <code>镜像A</code> 呢？它身上的便利贴被撕走了，它变成了一个<strong>没有名字、没有标签的“无主孤魂”</strong>。</li></ul></li></ol><p>这个被撕掉标签的 <code>镜像A</code>，就是我们所说的 <strong>“悬空镜像” (dangling image)</strong>。它和它所包含的那些旧的、失效的缓存层，依然<strong>静静地躺在你的硬盘上</strong>，并不会自动消失。</p><h3 id="为什么要这么设计？"><a href="#为什么要这么设计？" class="headerlink" title="为什么要这么设计？"></a><strong>为什么要这么设计？</strong></h3><p>Docker 这么做有它的道理，比如：</p><ul><li><strong>安全回滚</strong>：万一你发现新的构建有问题，那个旧的、没有标签的镜像还在，理论上你还可以通过它的 ID 找回它，重新给它贴上标签来回滚。</li><li><strong>性能考虑</strong>：在构建过程中频繁地进行磁盘删除操作，可能会影响构建性能。所以 Docker 选择把“构建”和“清理”这两个动作分开。</li></ul><h3 id="那如何清理这些“无主孤魂”呢？"><a href="#那如何清理这些“无主孤魂”呢？" class="headerlink" title="那如何清理这些“无主孤魂”呢？"></a><strong>那如何清理这些“无主孤魂”呢？</strong></h3><p>既然 Docker 不会自动清理，就需要我们手动当一下“清洁工”。Docker 提供了非常方便的命令。</p><ol><li><p><strong>查看有哪些悬空镜像</strong><br>你可以用这个命令，专门列出所有被撕掉标签的悬空镜像：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">docker images -f dangling=true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> 你可能会看到一堆 <code>&lt;none&gt;</code>:<code>&lt;none&gt;</code> 的镜像，这些就是它们。</p></li><li><p><strong>清理悬空镜像</strong></p><p> 使用 prune (修剪) 命令可以一键清理。</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">docker image prune<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> 执行后，Docker 会找到所有悬空镜像并询问你是否要删除它们，你输入 <code>y</code> 即可。这些镜像占用的磁盘空间就会被释放。</p></li></ol><h3 id="如何查看镜像的分层？"><a href="#如何查看镜像的分层？" class="headerlink" title="如何查看镜像的分层？"></a><strong>如何查看镜像的分层？</strong></h3><p>你可以使用 <code>docker history</code> 命令来查看一个镜像的构建历史和分层信息。</p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 拉取一个镜像用于演示docker pull python:3.9-slim# 查看它的分层历史docker history python:3.9-slim<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>你会看到类似下面的输出，每一行都代表一个层，以及创建该层的命令和它的大小：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">IMAGE          CREATED        CREATED BY                                      SIZE      COMMENTa1b2c3d4e5f6   2 weeks ago    /bin/sh -c #(nop)  CMD ["python3"]              0B<missing>      2 weeks ago    /bin/sh -c #(nop)  ENTRYPOINT ["docker-entry…   0B<missing>      2 weeks ago    /bin/sh -c #(nop) COPY file:238737301c4475…     341B<missing>      2 weeks ago    /bin/sh -c set -ex;   savedAptMark="$(apt-ma…   4.86MB<missing>      2 weeks ago    /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_URL=h…   0B... (更多层) ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>总结一下：</strong></p><p>镜像是静态的、只读的模板。它由多个只读层构成，每一层对应<code>Dockerfile</code>的一条指令。启动容器时，会在镜像之上增加一个可写层。这种分层结构带来了存储、构建和分发上的巨大效率优势。</p><h2 id="Docker-的整体架构"><a href="#Docker-的整体架构" class="headerlink" title="Docker 的整体架构"></a>Docker 的整体架构</h2><p>Docker 采用的是经典的 <strong>C&#x2F;S 架构 (Client&#x2F;Server, 客户端&#x2F;服务器架构)</strong>。它主要由三大部分组成：</p><ol><li>客户端 (Docker Client)</li><li>主机 (Docker Host) &#x2F; 守护进程 (Docker Daemon)</li><li>镜像仓库 (Docker Registry)</li></ol><h3 id="客户端-Docker-Client"><a href="#客户端-Docker-Client" class="headerlink" title="客户端 (Docker Client)"></a><strong>客户端 (Docker Client)</strong></h3><ul><li><strong>是什么</strong>：就是你主要打交道的对象，最常见的就是 <code>docker</code> 命令行工具。</li><li><strong>做什么</strong>：当你输入一个命令，比如 <code>docker run ...</code> 或 <code>docker build ...</code>，Client 会将这个命令通过 <strong>REST API</strong> 发送给 Docker Daemon。</li><li><strong>在哪</strong>：Client 可以和 Docker Daemon 运行在<strong>同一台机器</strong>上，也可以通过网络连接到<strong>远程</strong>的 Docker Daemon 上进行管理。</li></ul><p>简单来说，客户端就是个“传话的”，负责把你的指令传递给真正的“干活的”。</p><h3 id="主机-Docker-Host-守护进程-Docker-Daemon"><a href="#主机-Docker-Host-守护进程-Docker-Daemon" class="headerlink" title="主机 (Docker Host) &amp; 守护进程 (Docker Daemon)"></a><strong>主机 (Docker Host) &amp; 守护进程 (Docker Daemon)</strong></h3><p>这是 Docker 架构的核心和大脑，真正的“实干家”。</p><ul><li><strong>守护进程 (Docker Daemon, <code>dockerd</code>)</strong>:<ul><li>它是一个常驻在后台的系统进程。</li><li>它负责监听来自 Docker Client 的 API 请求。</li><li>它拥有管理一切 Docker 对象的全部权力，包括：<ul><li><strong>镜像 (Images)</strong>: 管理本地镜像，如果本地没有，会从 Registry 拉取。</li><li><strong>容器 (Containers)</strong>: 创建、运行、停止、删除容器。</li><li><strong>网络 (Networks)</strong>: 创建和配置容器网络。</li><li><strong>数据卷 (Volumes)</strong>: 管理数据的持久化。</li></ul></li></ul></li><li><strong>Docker Host</strong>: 指安装了 Docker Daemon 的物理机或虚拟机。Daemon 就是运行在这个 Host 上的。</li></ul><h3 id="镜像仓库-Docker-Registry"><a href="#镜像仓库-Docker-Registry" class="headerlink" title="镜像仓库 (Docker Registry)"></a><strong>镜像仓库 (Docker Registry)</strong></h3><ul><li><strong>是什么</strong>：一个集中存放和分发 Docker 镜像的服务。你可以把它理解为 Docker 世界的 “GitHub” 或 “Maven 中央仓库”。</li><li><strong>做什么</strong>：<ul><li><code>docker pull</code>: Daemon 从 Registry 拉取镜像到本地。</li><li><code>docker push</code>: Daemon 将本地构建的镜像推送到 Registry。</li></ul></li><li><strong>分类</strong>：<ul><li><strong>公共仓库 (Public Registry)</strong>: 比如官方的 <strong>Docker Hub</strong>，是默认的 Registry。</li><li><strong>私有仓库 (Private Registry)</strong>: 出于安全或网络原因，公司通常会搭建自己的私有仓库，比如 Harbor，或者使用云厂商提供的私有仓库服务 (如阿里云 ACR)。</li></ul></li></ul><h2 id="Dockerfile-最佳实践与深度理解"><a href="#Dockerfile-最佳实践与深度理解" class="headerlink" title="Dockerfile 最佳实践与深度理解"></a><code>Dockerfile</code> 最佳实践与深度理解</h2><h3 id="1-保持镜像轻量-Keep-Images-Small"><a href="#1-保持镜像轻量-Keep-Images-Small" class="headerlink" title="1. 保持镜像轻量 (Keep Images Small)"></a><strong>1. 保持镜像轻量 (Keep Images Small)</strong></h3><p><strong>为什么重要？</strong></p><ul><li><strong>存储成本</strong>：更小的镜像占用更少的磁盘和仓库空间。</li><li><strong>分发速度</strong>：更小的镜像 <code>push</code> 和 <code>pull</code> 都更快，能极大地提升 CI&#x2F;CD 效率和应用部署速度。</li></ul><p><strong>实践方法一：选择一个精简的基础镜像</strong></p><p><code>FROM</code> 是 <code>Dockerfile</code> 的第一行，也是决定镜像大小的起点。</p><ul><li><strong><code>ubuntu</code> (或 <code>centos</code>)</strong>: 包含完整的操作系统工具，体积最大，通常几百MB。适合需要大量系统工具的场景。</li><li><strong><code>image:tag-slim</code> (如 <code>python:3.9-slim</code>)</strong>: 官方提供的“瘦身版”，移除了很多非必需的工具包，体积适中。<strong>通常是生产环境的首选。</strong></li><li><strong><code>alpine</code></strong>: 基于 Alpine Linux，一个极度轻量化的发行版。体积最小，通常只有几MB。但它使用 <code>musl</code> 代替 <code>glibc</code> 作为 C 标准库，偶尔可能遇到兼容性问题。</li></ul><p><strong>建议</strong>：<strong>没有特殊理由，就从 <code>slim</code> 版本开始。如果对体积有极致要求，再考虑 <code>alpine</code>。</strong></p><p><strong>实践方法二：使用多阶段构建 (Multi-Stage Builds)</strong></p><p>这是减小镜像体积<strong>最有效</strong>的手段，尤其适用于需要编译的语言（如 Go, Java, Rust）或需要构建步骤的前端应用（如 Node.js, React）。</p><p><strong>核心思想</strong>：用一个包含完整构建工具的“构建镜像”来完成编译&#x2F;打包，然后把最终的产物（可执行文件、静态资源等）复制到一个干净的、极小的“生产镜像”中。</p><p><strong>糟糕的例子 (单阶段)</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># 把所有构建工具和中间产物都打进了最终镜像FROM node:16WORKDIR /appCOPY package*.json ./RUN npm installCOPY . .RUN npm run build # 生成了 dist 目录# 问题：最终镜像里包含了完整的Node.js、npm、devDependencies等，非常臃肿CMD ["npm", "start"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个镜像可能有 1GB 大小。</p><p><strong>优秀的例子 (多阶段)</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># --- 构建阶段 (Builder Stage) ---# 使用一个完整的 Node.js 镜像，并给它起个别名 `builder`FROM node:16 AS builderWORKDIR /appCOPY package*.json ./RUN npm installCOPY . .RUN npm run build# --- 生产阶段 (Production Stage) ---# 使用一个极度轻量化的 Nginx 服务器作为最终镜像FROM nginx:1.21-alpine# 只从 `builder` 阶段复制出最终的静态文件产物COPY --from=builder /app/dist /usr/share/nginx/html# EXPOSE 80 (Nginx 默认)# CMD ["nginx", "-g", "daemon off;"] (Nginx 默认)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个最终镜像可能只有 20MB 大小。对比天差地别！</p><p><strong>实践方法三：清理不必要的中间文件</strong></p><p>在 <code>RUN</code> 指令中，应该把多个命令用 <code>&amp;&amp;</code> 串联起来，并在结尾清理掉缓存和临时文件。因为每一条 <code>RUN</code> 都会创建一个新的层，如果你不一次性清理，垃圾就会被永久封存在上一层里。</p><p><strong>糟糕的例子</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># 垃圾被留在了第一层RUN apt-get update# 安装了软件，但缓存还在RUN apt-get install -y curl<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>优秀的例子</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">RUN apt-get update && \    apt-get install -y curl && \    rm -rf /var/lib/apt/lists/*<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>把所有操作合并到一条 <code>RUN</code> 指令中，并在结尾删除了 apt 缓存，确保这一层是干净的。</p><h3 id="2-高效利用构建缓存"><a href="#2-高效利用构建缓存" class="headerlink" title="2. 高效利用构建缓存"></a><strong>2. 高效利用构建缓存</strong></h3><p>我们之前讨论过缓存的原理。这里的核心就是<strong>优化 <code>Dockerfile</code> 的指令顺序</strong>。</p><p><strong>核心原则：把变化最少、最稳定的层放在前面；把最频繁变化的层放在后面。</strong></p><p><strong>糟糕的例子</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">FROM python:3.9-slimWORKDIR /app# 只要任何一个代码文件变动，下面整块都要重新执行COPY . .# 导致每次改代码都要重新安装所有依赖，非常慢RUN pip install -r requirements.txtCMD ["python", "app.py"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>优秀的例子</strong>:</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">FROM python:3.9-slimWORKDIR /app# 1. 先只复制依赖文件。这个文件通常不怎么变。COPY requirements.txt .# 2. 安装依赖。只要 requirements.txt 不变，这一层就会被完美缓存。RUN pip install -r requirements.txt# 3. 最后再复制你的代码。这样你频繁修改代码时，上面耗时的步骤都能利用缓存。COPY . .CMD ["python", "app.py"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Docker网络操作"><a href="#Docker网络操作" class="headerlink" title="Docker网络操作"></a>Docker网络操作</h2><p>Docker 主要有三种常见的网络模式：</p><ol><li><strong><code>bridge</code> (桥接模式)</strong>：默认模式，单机环境下的“局域网”。Docker 会为这个网络分配一个私有的 IP 地址段（比如 <code>172.17.0.0/16</code>），每个接入的容器都会获得一个自己的内部 IP 地址。</li><li><strong><code>host</code> (主机模式)</strong>：直接使用主机的网络，性能最好但隔离性差。容器直接共享使用宿主机的网络协议栈。在容器内部看到的网卡、IP 地址、端口等，都和宿主机上完全一样。</li><li><strong><code>overlay</code> (覆盖网络)</strong>：用于多台主机之间的容器通信，是集群（Swarm）的基石。<br>它可以在多台不同的物理或虚拟主机之上，创建一个统一的、虚拟的二层网络。这使得连接到同一个 <code>overlay</code> 网络的容器，即使它们分布在不同的主机上，也感觉像是在同一个局域网内，可以互相直接通信。</li></ol><h2 id="数据持久化：Volume-vs-Bind-Mount"><a href="#数据持久化：Volume-vs-Bind-Mount" class="headerlink" title="数据持久化：Volume vs. Bind Mount"></a>数据持久化：<code>Volume</code> vs. <code>Bind Mount</code></h2><h3 id="1-数据卷-Volumes-：Docker-的“专业行李箱”"><a href="#1-数据卷-Volumes-：Docker-的“专业行李箱”" class="headerlink" title="1. 数据卷 (Volumes)：Docker 的“专业行李箱”"></a><strong>1. 数据卷 (Volumes)：Docker 的“专业行李箱”</strong></h3><p>这是 Docker官方<strong>最推荐</strong>的数据持久化方式。</p><ul><li><strong>是什么</strong>：Volume 是一个由 <strong>Docker 自己创建和管理</strong>的特殊目录。它独立于容器的生命周期，专门用来存放持久化数据。在 Linux 系统上，它通常位于 <code>/var/lib/docker/volumes/</code> 目录下，但我们<strong>不应该</strong>直接去操作这个目录。</li><li><strong>工作原理</strong>：你可以把它想象成一个 Docker 帮你保管的、可插拔的“U盘”。你创建一个 Volume (U盘)，然后把它“插”到容器的指定目录上。即使容器这个“电脑”坏了、被扔掉了，这个“U盘”和里面的数据都还在，可以再插到新的容器上继续使用。</li><li><strong>特点</strong>：<ul><li><strong>Docker 管理</strong>：创建、删除、查看等都通过 Docker 命令进行，与宿主机的具体目录结构解耦。</li><li><strong>独立生命周期</strong>：删除容器时，Volume <strong>不会</strong>被删除（除非你显式删除它）。</li><li><strong>高性能</strong>：在 Linux 上，Volume 的读写性能接近原生磁盘。</li><li><strong>跨平台兼容性好</strong>：无论你在 Windows, macOS 还是 Linux 上，Volume 的行为都是一致的。</li><li><strong>更安全</strong>：容器只能访问到挂载的 Volume，无法随意访问宿主机的其他文件。</li></ul></li><li><strong>适用场景</strong>：<ul><li><strong>数据库数据</strong>：比如 MySQL, PostgreSQL 的数据目录。</li><li><strong>应用生成的内容</strong>：比如用户上传的图片、附件等。</li><li>需要在多个容器之间<strong>共享数据</strong>的场景。</li><li><strong>所有需要持久化的生产环境数据</strong>。</li></ul></li></ul><p><strong>示例</strong>：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># 1. 创建一个 Volumedocker volume create my-db-data# 2. 运行 MySQL 容器，并将这个 Volume 挂载到容器内的数据目录# -v my-db-data:/var/lib/mysql#  (Volume名称):(容器内路径)docker run -d --name mysql-db \  -v my-db-data:/var/lib/mysql \  -e MYSQL_ROOT_PASSWORD=secret \  mysql:5.7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-绑定挂载-Bind-Mounts-：宿主机的“任意门”"><a href="#2-绑定挂载-Bind-Mounts-：宿主机的“任意门”" class="headerlink" title="2. 绑定挂载 (Bind Mounts)：宿主机的“任意门”"></a><strong>2. 绑定挂载 (Bind Mounts)：宿主机的“任意门”</strong></h3><p>这种方式是直接将宿主机上的<strong>一个已存在的目录或文件</strong>挂载到容器中。</p><ul><li><strong>是什么</strong>：Bind Mount 就是在容器和宿主机之间开了一扇“任意门”。门的一边是宿主机的某个指定路径，另一边是容器内的某个路径。两边的内容是实时同步、完全一致的。</li><li><strong>工作原理</strong>：容器内的进程对挂载点的读写，实际上就是直接对宿主机文件系统的读写。容器内的 <code>uid/gid</code> 权限也可能会影响到宿主机上的文件权限。</li><li><strong>特点</strong>：<ul><li><strong>直接映射</strong>：非常直观，所见即所得。</li><li><strong>依赖宿主机</strong>：强依赖于宿主机的特定文件路径。如果换一台机器，这个路径可能不存在，导致无法启动。</li><li><strong>潜在安全风险</strong>：如果挂载了像 <code>/</code> 或 <code>/etc</code> 这样的系统目录，容器内的进程（尤其是 root 用户）就可能修改甚至破坏宿主机系统。</li><li><strong>性能</strong>：在 Linux 上性能也很好，但在 macOS 和 Windows 上，因为有虚拟化文件系统的介入，性能可能会比 Volume 差一些。</li></ul></li><li><strong>适用场景</strong>：<ul><li><strong>开发环境</strong>：这是 Bind Mount 最核心、最完美的应用场景。你可以把本地的<strong>源代码目录</strong>挂载到容器里，这样你在本地 IDE 里修改代码，容器内的应用可以立刻感知到变化（比如用 <code>nodemon</code> 实现热更新），无需重新构建镜像。</li><li>需要将主机的某些<strong>配置文件</strong>（如 <code>nginx.conf</code>）共享给容器使用的场景。</li></ul></li></ul><p><strong>示例</strong>：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker"># 把宿主机当前目录下的 `my-app` 文件夹，挂载到容器内的 `/app` 目录# -v $(pwd)/my-app:/app#  (宿主机绝对路径):(容器内路径)docker run -d --name dev-container \  -v $(pwd)/my-app:/app \  my-dev-image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="如何分析Docker镜像的大小组成"><a href="#如何分析Docker镜像的大小组成" class="headerlink" title="如何分析Docker镜像的大小组成"></a>如何分析Docker镜像的大小组成</h2><h3 id="方法一：基础体检-docker-history"><a href="#方法一：基础体检-docker-history" class="headerlink" title="方法一：基础体检 - docker history"></a><strong>方法一：基础体检 - <code>docker history</code></strong></h3><p>这是 Docker 自带的最基础的工具，可以快速查看镜像的“履历”，也就是它的分层历史和每一层的大小。</p><ul><li><p><strong>如何使用</strong>：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">docker history your-image:tag<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>能看到什么</strong>：<br>它会列出构成这个镜像的每一层，以及创建该层的 <code>Dockerfile</code> 指令（<code>CREATED BY</code> 列）和该层自身的大小（<code>SIZE</code> 列）。</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">IMAGE          CREATED         CREATED BY                                      SIZEf1a3a4b5c6d7   5 minutes ago   /bin/sh -c #(nop) CMD ["./my-app"]              0Ba2b3c4d5e6f7   5 minutes ago   /bin/sh -c #(nop) COPY file:123... in /app/     15MBc3d4e5f6a7b8   10 minutes ago  /bin/sh -c pip install -r requirements.txt      120MB  <-- 尺寸可疑！d4e5f6a7b8c9   2 hours ago     /bin/sh -c #(nop) COPY file:456... in /app/     1KB...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>优点</strong>：</p><ul><li>无需安装任何额外工具，是 Docker 内置功能。</li><li>可以快速定位到是<strong>哪一条 <code>RUN</code> 或 <code>COPY</code> 命令</strong>导致了镜像体积的剧增。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>只能看到层的大小，看不到层里面具体是哪个文件大</strong>。比如上面那个 120MB 的层，我们只知道是 <code>pip install</code> 产生的，但不知道是 <code>numpy</code> 占了 80MB 还是 <code>pandas</code> 占了 100MB。</li></ul></li></ul><h3 id="方法三：专业分析工具-dive-强烈推荐"><a href="#方法三：专业分析工具-dive-强烈推荐" class="headerlink" title="方法三：专业分析工具 - dive (强烈推荐)"></a><strong>方法三：专业分析工具 - <code>dive</code> (强烈推荐)</strong></h3><p><code>dive</code> 是一个开源的第三方工具，是专门为分析 Docker 镜像而生的“神器”。它可以让你像透视一样看清镜像的每一层。</p><ul><li><p><strong>如何使用</strong>：<br>你需要先安装 <code>dive</code> (在 macOS 上可以用 <code>brew install dive</code>，其他系统请参考其 GitHub 页面)。然后执行：</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">dive your-image:tag<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>能看到什么</strong>：<br><code>dive</code> 会打开一个交互式的界面，主要分为两个窗格：</p><ul><li><strong>左侧窗格</strong>：显示了镜像的所有<strong>分层</strong>，和 <code>docker history</code> 的结果类似。你可以用上下键在不同层之间切换。</li><li><strong>右侧窗格</strong>：实时显示<strong>当前选中层的文件系统树</strong>。你可以像在文件管理器里一样，展开目录，查看文件。</li></ul></li></ul><h2 id="Docker-Compose-单机环境下的“轻量级”编排工具。"><a href="#Docker-Compose-单机环境下的“轻量级”编排工具。" class="headerlink" title="Docker Compose -  单机环境下的“轻量级”编排工具。"></a>Docker Compose -  <strong>单机环境下的“轻量级”编排工具</strong>。</h2><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">version: '3' # 文件格式版本services: # “食材”列表，定义了应用包含的所有服务    web: # 第一个服务，名为 "web"    image: nginx:latest # 使用哪个镜像    ports: # 端口映射      - "80:80"    volumes: # 数据挂载      - ./html:/usr/share/nginx/html    networks: # 连接到哪个网络      - webnet       db: # 第二个服务，名为 "db"    image: mysql:5.7    environment: # 环境变量      MYSQL_ROOT_PASSWORD: password    volumes:      - db_data:/var/lib/mysql    networks:      - webnetvolumes: # 定义这个应用会用到的数据卷  db_data:networks: # 定义这个应用会用到的网络  webnet:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="核心优势"><a href="#核心优势" class="headerlink" title="核心优势"></a><strong>核心优势</strong></h3><ol><li><strong>一键管理，简单高效</strong><ul><li><code>docker-compose up -d</code>：一键启动并后台运行整个应用。</li><li><code>docker-compose down</code>：一键停止并删除所有相关的容器、网络。</li><li><code>docker-compose logs -f</code>：一键查看所有服务的日志。<br> 你不再需要关心单个容器，而是从<strong>整个应用</strong>的层面来管理服务。</li></ul></li><li><strong>自动化的服务发现与网络</strong><br>当你运行 <code>docker-compose up</code> 时，Compose 会<strong>自动创建一个专用的桥接网络</strong>（在上面的例子里就是 <code>webnet</code>）。<ul><li>处在这个网络里的所有服务（<code>web</code> 和 <code>db</code>），可以<strong>直接通过服务名作为主机名进行通信</strong>。</li><li>例如，<code>web</code> 服务可以直接连接 <code>mysql://db:3306</code> 来访问数据库，而不需要关心 <code>db</code> 容器的 IP 地址是什么。这个过程是全自动的。</li></ul></li><li><strong>配置即代码，易于分享和版本控制</strong><code>docker-compose.yml</code> 是一个纯文本文件，你可以像对待代码一样，把它提交到 Git 仓库里。<ul><li><strong>团队协作</strong>：新同事加入项目时，他只需要 <code>git pull</code> 拉取代码，然后运行 <code>docker-compose up</code>，一个完全一致的开发环境瞬间就搭建好了。</li><li><strong>环境一致性</strong>：确保了开发、测试、甚至简单生产环境的配置都是完全一致的，避免了“在我电脑上是好的”这种经典问题。</li></ul></li></ol><h2 id="Docker-Swarm"><a href="#Docker-Swarm" class="headerlink" title="Docker Swarm"></a>Docker Swarm</h2><p><strong>定义：Docker 官方内置的、原生的集群管理和编排工具</strong>。<br>Docker Swarm 则是把<strong>多台安装了 Docker 的主机（物理机或虚拟机）聚合在一起，组成一个“虚拟的单一主机”</strong>，也就是一个 Docker 集群。</p><h3 id="核心概念与角色"><a href="#核心概念与角色" class="headerlink" title="核心概念与角色"></a>核心概念与角色</h3><ul><li><strong>Swarm (集群)</strong>: 指的是由多台 Docker 主机组成的整个集群。</li><li><strong>Node (节点)</strong>: 集群中的<strong>每一台</strong> Docker 主机就是一个节点。节点分为两种角色：<ul><li><strong>Manager Node (管理节点)</strong>: <strong>集群的“大脑”和“指挥官”</strong>。<ul><li><strong>职责</strong>：接收用户的指令（比如 <code>docker service create</code>）、维护集群的期望状态、决定将容器（任务）调度到哪个节点上运行、管理整个集群的状态。</li><li><strong>高可用</strong>：为了防止“指挥官”单点故障，生产环境通常会设置 3 个或 5 个 Manager 节点。它们之间通过 Raft 一致性算法同步数据，即使一个 Manager 宕机，其他 Manager 也能接管工作。</li></ul></li><li><strong>Worker Node (工作节点)</strong>: <strong>集群的“工兵”和“劳动力”</strong>。<ul><li><strong>职责</strong>：不参与决策，唯一的任务就是接收来自 Manager 节点的指令，然后<strong>运行或停止容器（任务）</strong>。它们是真正干活的。</li></ul></li></ul></li><li><strong>Service (服务)</strong>: 这是在 Swarm 模式下部署应用的<strong>核心抽象</strong>。<ul><li>它不再是单个的容器，而是一个<strong>服务定义</strong>。你向 Manager 声明：“我想要一个名为 <code>webapp</code> 的服务，它使用 <code>nginx</code> 镜像，需要一直保持 <strong>3 个副本</strong>在运行，并对外暴露 80 端口”。</li><li>Swarm 会持续监控，确保任何时候都有 3 个符合你定义的容器在运行。</li></ul></li><li><strong>Task (任务)</strong>: 一个服务运行的<strong>具体实例</strong>，也就是一个<strong>容器</strong>。如果一个服务有 3 个副本（replicas），那么它就对应着 3 个正在运行的任务（容器）。<strong>任务是 Swarm 中最小的调度单位。</strong></li></ul><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p><strong>第一步：初始化集群</strong></p><p>在一台准备当 Manager 的机器上执行：</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker swarm init --advertise-addr <MANAGER-IP><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会把这台机器变成 Swarm 集群的第一个（也是目前唯一一个）Manager 节点。并且它会生成一个“入群口令”(Token)。</p><p><strong>第二步：节点加入集群</strong><br>在其他机器上，使用第一步生成的口令，让它们作为 Worker 节点加入集群：</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker swarm join --token <YOUR-TOKEN> <MANAGER-IP>:2377<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>第三步：部署服务 (下发乐谱)</strong></p><p>现在，在<strong>Manager 节点</strong>上告诉“Manager ”要部署什么服务：</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker service create --name web --replicas 3 -p 8080:80 nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>docker swarm启动服务的命令是：<code>docker service create</code></p><ul><li>我想要一个名为 <code>web</code> 的<strong>服务</strong>。</li><li>它的期望状态是永远保持 <strong>3 个副本</strong>（<code>-replicas 3</code>）。</li><li>它使用 <code>nginx</code> 镜像。</li><li>将集群的 <code>8080</code> 端口映射到这个服务所有副本的 <code>80</code> 端口。</li></ul><p><strong>第四步：Swarm 的魔法：声明式服务与自愈</strong></p><ol><li><strong>调度</strong>：Manager 节点收到命令后，会查看所有 Worker 节点的负载情况，然后决定在哪几个 Worker 上启动这 3 个 Nginx 容器（任务）。</li><li><strong>路由网格 (Routing Mesh)</strong>： <code>-p 8080:80</code> 这个端口映射非常强大。无论你访问<strong>集群中任何一个节点</strong>的 <code>8080</code> 端口，Swarm 都会自动把请求负载均衡到这 3 个 Nginx 容器中的一个，不管那个容器具体在哪台机器上。</li><li><strong>自愈 (Self-healing)</strong>：这是编排平台的核心魅力。假设一个运行着 Nginx 容器的 Worker 节点突然宕机了。<ul><li>Manager 会立刻发现：“警告！<code>web</code> 服务的当前状态（2个副本）不等于期望状态（3个副本）！”</li><li>为了恢复到期望状态，Manager 会马上在另一个健康的 Worker 节点上，<strong>自动启动一个新的 Nginx 容器</strong>。</li><li>整个过程无需人工干预，服务会自动恢复。你只需要声明你的“期望”，Swarm 会搞定一切。</li></ul></li></ol><p><strong>第五步：扩缩容与更新</strong></p><ul><li><p><strong>扩容</strong>：业务高峰来了，你想把 3 个 Nginx 增加到 10 个。</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker service scale web=10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  Manager 会立刻调度，启动 7 个新的 Nginx 容器。</p></li><li><p><strong>更新</strong>：你想把 Nginx 的版本升级到 <code>1.21</code>。</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker service update --image nginx:1.21 web<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  Swarm 会自动进行“滚动更新”，一个一个地用新版本容器替换旧版本容器，保证服务在更新过程中不中断。</p></li></ul><h3 id="路由网格-Routing-Mesh"><a href="#路由网格-Routing-Mesh" class="headerlink" title="路由网格 (Routing Mesh)"></a>路由网格 (Routing Mesh)</h3><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">docker service create --name my-nginx-service --replicas 3 -p 8080:80 nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里的 <code>-p 8080:80</code> 已经不是“把主机端口 8080 映射到单个容器的 80 端口”这个简单的意思了。它的含义是：</p><p><strong>“在整个集群的所有节点上，都发布 8080 端口，并让它成为 <code>my-nginx-service</code> 这个服务的统一入口。”</strong></p><ul><li><strong>发布端口</strong>：当你创建服务并发布端口后，Swarm 会在<strong>集群中的每一个节点</strong>上都监听 8080 端口，准备接收外部流量。</li><li><strong>智能路由</strong>：无论你的请求到达<strong>哪一个节点</strong>的 8080 端口（可以是 Manager，也可以是 Worker，即使这个节点上并没有运行 Nginx 容器），路由网格都会接收这个请求。</li><li><strong>负载均衡</strong>：路由网格知道 <code>my-nginx-service</code> 背后有 3 个正在运行的容器（任务），也知道它们各自的内部 IP 地址。它会自动将接收到的请求，以轮询的方式<strong>负载均衡</strong>到这 3 个容器中的一个去处理。</li></ul><h2 id="一、Docker-基础配置"><a href="#一、Docker-基础配置" class="headerlink" title="一、Docker 基础配置"></a>一、Docker 基础配置</h2><ol><li><p><strong>安装后配置</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 将当前用户添加到docker组（避免每次使用sudo）sudo usermod -aG docker $USER# 配置Docker开机自启sudo systemctl enable dockersudo systemctl start docker# 查看Docker版本docker versiondocker info<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>配置镜像加速器</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 编辑Docker配置文件sudo vim /etc/docker/daemon.json# 添加镜像加速器配置&#123;  "registry-mirrors": [    "<https://docker.m.daocloud.io>",    "<https://dockerproxy.com>",    "<https://docker.mirrors.ustc.edu.cn>",    "<https://docker.nju.edu.cn>",    "<https://iju9kaj2.mirror.aliyuncs.com>",    "<http://hub-mirror.c.163.com>",    "<https://cr.console.aliyun.com>",    "<https://hub.docker.com>",    "<http://mirrors.ustc.edu.cn>"  ]&#125;# 重启Docker服务sudo systemctl daemon-reloadsudo systemctl restart docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="二、镜像管理"><a href="#二、镜像管理" class="headerlink" title="二、镜像管理"></a>二、镜像管理</h2><ol><li><p><strong>搜索和拉取镜像</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker search nginx                 # 搜索镜像docker pull nginx                   # 拉取最新版本docker pull nginx:1.21              # 拉取指定版本docker pull ubuntu:20.04            # 拉取Ubuntu 20.04<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看本地镜像</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker images                       # 列出本地镜像docker images -a                    # 显示所有镜像（包括中间层）docker images nginx                 # 查看特定镜像docker images --format "&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;"  # 自定义格式输出<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>镜像详细信息</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker inspect nginx                # 查看镜像详细信息docker history nginx                # 查看镜像历史docker image prune                  # 清理未使用的镜像docker rmi nginx:1.21               # 删除指定镜像docker rmi $(docker images -q)      # 删除所有镜像<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>保存和加载镜像</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker save -o nginx.tar nginx:latest     # 保存镜像到tar文件docker load -i nginx.tar                  # 从tar文件加载镜像docker save nginx:latest | gzip > nginx.tar.gz  # 压缩保存<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>构建镜像</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker build -t myapp:1.0 .              # 从Dockerfile构建docker build -t myapp:1.0 -f Dockerfile.dev .  # 指定Dockerfiledocker build --no-cache -t myapp:1.0 .   # 不使用缓存构建<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="三、容器管理"><a href="#三、容器管理" class="headerlink" title="三、容器管理"></a>三、容器管理</h2><ol><li><p><strong>运行容器</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run nginx                    # 运行nginx容器docker run -d nginx                 # 后台运行docker run -p 8080:80 nginx         # 端口映射docker run --name mynginx nginx     # 指定容器名称docker run -it ubuntu bash          # 交互式运行docker run -e MYSQL_ROOT_PASSWORD=123456 mysql  # 设置环境变量docker run -v /host/data:/container/data nginx  # 挂载数据卷docker run --rm nginx               # 容器退出后自动删除<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看容器</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker ps                           # 查看运行中的容器docker ps -a                        # 查看所有容器docker ps -q                        # 只显示容器IDdocker ps --format "table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;\\t&#123;&#123;.Status&#125;&#125;"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>容器操作</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker start container_id           # 启动容器docker stop container_id            # 停止容器docker restart container_id         # 重启容器docker pause container_id           # 暂停容器docker unpause container_id         # 恢复容器docker rm container_id              # 删除容器docker rm -f container_id           # 强制删除运行中的容器docker container prune              # 清理停止的容器<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>容器交互</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker exec -it container_id bash   # 进入运行中的容器docker exec container_id ls /app    # 在容器中执行命令docker attach container_id          # 附加到容器docker cp file.txt container_id:/app/  # 复制文件到容器docker cp container_id:/app/file.txt ./  # 从容器复制文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看容器信息</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker logs container_id            # 查看容器日志docker logs -f container_id         # 实时查看日志docker logs --tail 100 container_id # 查看最后100行docker top container_id             # 查看容器进程docker stats                        # 查看容器资源使用docker inspect container_id         # 查看容器详细信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="四、网络管理"><a href="#四、网络管理" class="headerlink" title="四、网络管理"></a>四、网络管理</h2><ol><li><p><strong>网络操作</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker network ls                   # 列出网络docker network create mynet         # 创建网络docker network inspect bridge       # 查看网络详情docker network rm mynet             # 删除网络<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>容器网络</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run --network=mynet nginx    # 指定网络运行容器docker network connect mynet container_id    # 连接容器到网络docker network disconnect mynet container_id  # 断开连接<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>端口映射</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run -p 8080:80 nginx         # 映射端口8080到容器80docker run -p 127.0.0.1:8080:80 nginx  # 只绑定本地地址docker run -P nginx                 # 随机映射端口docker port container_id            # 查看端口映射<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="五、数据卷管理"><a href="#五、数据卷管理" class="headerlink" title="五、数据卷管理"></a>五、数据卷管理</h2><ol><li><p><strong>创建和管理数据卷</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker volume create mydata         # 创建数据卷docker volume ls                    # 列出数据卷docker volume inspect mydata        # 查看数据卷详情docker volume rm mydata             # 删除数据卷docker volume prune                 # 清理未使用的数据卷<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>使用数据卷</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run -v mydata:/app nginx     # 挂载命名数据卷docker run -v /host/path:/container/path nginx  # 挂载主机目录docker run --mount source=mydata,target=/app nginx  # 使用--mountdocker run -v $(pwd):/app nginx     # 挂载当前目录<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="六、Docker-Compose"><a href="#六、Docker-Compose" class="headerlink" title="六、Docker Compose"></a>六、Docker Compose</h2><ol><li><p><strong>基本命令</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker-compose up                   # 启动服务docker-compose up -d                # 后台启动docker-compose down                 # 停止并删除容器docker-compose ps                   # 查看服务状态docker-compose logs                 # 查看日志docker-compose logs -f service_name # 查看特定服务日志<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>服务管理</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker-compose start                # 启动服务docker-compose stop                 # 停止服务docker-compose restart              # 重启服务docker-compose build                # 构建服务docker-compose pull                 # 拉取服务镜像<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>docker-compose.yml示例</strong></p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">version: '3'services:  web:    image: nginx:latest    ports:      - "80:80"    volumes:      - ./html:/usr/share/nginx/html    networks:      - webnet  db:    image: mysql:5.7    environment:      MYSQL_ROOT_PASSWORD: password    volumes:      - db_data:/var/lib/mysql    networks:      - webnetvolumes:  db_data:networks:  webnet:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="七、Dockerfile编写"><a href="#七、Dockerfile编写" class="headerlink" title="七、Dockerfile编写"></a>七、Dockerfile编写</h2><ol><li><p><strong>基础指令</strong></p><pre><code># 基础镜像FROM node:14-alpine# 维护者信息LABEL maintainer=&quot;name@example.com&quot;# 设置工作目录WORKDIR /app# 复制文件COPY package*.json ./COPY . .# 运行命令RUN npm install# 环境变量ENV NODE_ENV=productionENV PORT=3000# 暴露端口EXPOSE 3000# 启动命令CMD [&quot;npm&quot;, &quot;start&quot;]</code></pre></li><li><p><strong>多阶段构建</strong></p><pre><code># 构建阶段FROM node:14 AS builderWORKDIR /appCOPY package*.json ./RUN npm installCOPY . .RUN npm run build# 生产阶段FROM nginx:alpineCOPY --from=builder /app/dist /usr/share/nginx/htmlEXPOSE 80CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]</code></pre></li><li><p><strong>最佳实践</strong></p><pre><code># 使用特定版本的基础镜像FROM node:14.17.0-alpine# 使用非root用户RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroupUSER appuser# 利用构建缓存COPY package*.json ./RUN npm installCOPY . .# 减少层数RUN apt-get update &amp;&amp; apt-get install -y \\    package1 \\    package2 \\    &amp;&amp; rm -rf /var/lib/apt/lists/*</code></pre></li></ol><h2 id="八、镜像仓库操作"><a href="#八、镜像仓库操作" class="headerlink" title="八、镜像仓库操作"></a>八、镜像仓库操作</h2><ol><li><p><strong>登录和推送</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker login                        # 登录Docker Hubdocker login registry.example.com   # 登录私有仓库docker tag myapp:1.0 username/myapp:1.0  # 标记镜像docker push username/myapp:1.0      # 推送镜像<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>私有仓库</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 运行私有仓库docker run -d -p 5000:5000 --name registry registry:2# 标记并推送到私有仓库docker tag myapp:1.0 localhost:5000/myapp:1.0docker push localhost:5000/myapp:1.0# 从私有仓库拉取docker pull localhost:5000/myapp:1.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="九、容器资源限制"><a href="#九、容器资源限制" class="headerlink" title="九、容器资源限制"></a>九、容器资源限制</h2><ol><li><p><strong>CPU限制</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run --cpus=2 nginx           # 限制使用2个CPUdocker run --cpu-shares=512 nginx   # CPU共享权重docker run --cpuset-cpus="0,1" nginx  # 指定CPU核心<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>内存限制</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run -m 512m nginx            # 限制内存512MBdocker run --memory-swap=1g nginx   # 限制总内存(含swap)docker run --oom-kill-disable nginx # 禁用OOM killer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>其他资源限制</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run --device-read-bps /dev/sda:1mb nginx  # 限制磁盘读取速度docker run --device-write-bps /dev/sda:1mb nginx # 限制磁盘写入速度docker run --pids-limit 100 nginx   # 限制进程数<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十、Docker健康检查"><a href="#十、Docker健康检查" class="headerlink" title="十、Docker健康检查"></a>十、Docker健康检查</h2><ol><li><p><strong>Dockerfile中定义</strong></p><pre><code>HEALTHCHECK --interval=30s --timeout=3s \\  CMD curl -f &lt;http://localhost/&gt; || exit 1</code></pre></li><li><p><strong>运行时指定</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run -d --health-cmd="curl -f <http://localhost/> || exit 1" \\           --health-interval=30s \\           --health-timeout=3s \\           --health-retries=3 \\           nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看健康状态</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker inspect --format='&#123;&#123;.State.Health.Status&#125;&#125;' container_iddocker ps --filter health=healthy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h2 id="十一、日志管理"><a href="#十一、日志管理" class="headerlink" title="十一、日志管理"></a>十一、日志管理</h2><ol><li><p><strong>日志驱动配置</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 配置JSON文件日志驱动docker run --log-driver=json-file \\           --log-opt max-size=10m \\           --log-opt max-file=3 \\           nginx# 配置syslog日志驱动docker run --log-driver=syslog \\           --log-opt syslog-address=tcp://192.168.0.42:123 \\           nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>查看日志</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker logs container_id            # 查看容器日志docker logs -f --tail 100 container_id  # 实时查看最后100行docker logs --since 30m container_id    # 查看最近30分钟的日志<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十二、清理和维护"><a href="#十二、清理和维护" class="headerlink" title="十二、清理和维护"></a>十二、清理和维护</h2><ol><li><p><strong>系统清理</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker system df                    # 查看Docker磁盘使用docker system prune                 # 清理未使用的数据docker system prune -a              # 清理所有未使用的数据docker system prune --volumes       # 同时清理数据卷<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>资源清理</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker container prune              # 清理停止的容器docker image prune                  # 清理未使用的镜像docker volume prune                 # 清理未使用的数据卷docker network prune                # 清理未使用的网络<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十三、故障排查"><a href="#十三、故障排查" class="headerlink" title="十三、故障排查"></a>十三、故障排查</h2><ol><li><p><strong>容器调试</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker logs container_id            # 查看容器日志docker inspect container_id         # 查看容器详细信息docker exec -it container_id sh     # 进入容器shelldocker diff container_id            # 查看容器文件变化docker events                       # 查看Docker事件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>网络调试</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker network inspect bridge       # 查看网络详情docker exec container_id ping other_container  # 测试容器间连通性docker exec container_id nslookup other_container  # DNS解析测试<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>性能分析</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker stats                        # 实时查看资源使用docker top container_id             # 查看容器进程docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' container_id  # 获取容器PID<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十四、安全最佳实践"><a href="#十四、安全最佳实践" class="headerlink" title="十四、安全最佳实践"></a>十四、安全最佳实践</h2><ol><li><p><strong>运行时安全</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 以只读模式运行容器docker run --read-only nginx# 限制容器capabilitiesdocker run --cap-drop=ALL --cap-add=NET_BIND_SERVICE nginx# 使用非root用户运行docker run --user=1000:1000 nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>镜像安全</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 扫描镜像漏洞docker scan nginx:latest# 使用官方镜像或可信来源docker pull docker.io/library/nginx:latest# 签名验证docker trust inspect nginx:latest<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十五、常用技巧和别名"><a href="#十五、常用技巧和别名" class="headerlink" title="十五、常用技巧和别名"></a>十五、常用技巧和别名</h2><ol><li><p><strong>实用别名</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 添加到~/.bashrc或~/.zshrcalias dps='docker ps'alias dpsa='docker ps -a'alias di='docker images'alias drm='docker rm $(docker ps -aq)'alias drmi='docker rmi $(docker images -q)'alias dex='docker exec -it'alias dlog='docker logs -f'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>常用组合命令</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 停止所有容器docker stop $(docker ps -aq)# 删除所有停止的容器docker rm $(docker ps -aq -f status=exited)# 删除所有未打标签的镜像docker rmi $(docker images -f "dangling=true" -q)# 查看容器IP地址docker inspect -f '&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' container_id# 导出容器文件系统docker export container_id > container.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十六、Docker-Swarm集群管理"><a href="#十六、Docker-Swarm集群管理" class="headerlink" title="十六、Docker Swarm集群管理"></a>十六、Docker Swarm集群管理</h2><ol><li><p><strong>初始化和加入集群</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 初始化Swarm集群docker swarm init --advertise-addr 192.168.1.100# 获取加入tokendocker swarm join-token workerdocker swarm join-token manager# 加入集群docker swarm join --token SWMTKN-1-xxx 192.168.1.100:2377<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>服务管理</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 创建服务docker service create --name web --replicas 3 -p 80:80 nginx# 扩展服务docker service scale web=5# 更新服务docker service update --image nginx:1.21 web# 查看服务docker service lsdocker service ps web<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="十七、实际应用示例"><a href="#十七、实际应用示例" class="headerlink" title="十七、实际应用示例"></a>十七、实际应用示例</h2><ol><li><p><strong>Web应用部署</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># 创建网络docker network create webapp-net# 运行数据库docker run -d \\  --name db \\  --network webapp-net \\  -e MYSQL_ROOT_PASSWORD=secret \\  -e MYSQL_DATABASE=webapp \\  -v mysql-data:/var/lib/mysql \\  mysql:5.7# 运行Web应用docker run -d \\  --name webapp \\  --network webapp-net \\  -p 8080:80 \\  -e DB_HOST=db \\  -e DB_USER=root \\  -e DB_PASSWORD=secret \\  -e DB_NAME=webapp \\  webapp:latest<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>开发环境搭建</strong></p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml"># docker-compose.ymlversion: '3'services:  frontend:    build: ./frontend    volumes:      - ./frontend:/app      - /app/node_modules    ports:      - "3000:3000"    command: npm start  backend:    build: ./backend    volumes:      - ./backend:/app    ports:      - "5000:5000"    environment:      - NODE_ENV=development    depends_on:      - db  db:    image: postgres:13    volumes:      - pgdata:/var/lib/postgresql/data    environment:      - POSTGRES_PASSWORD=secret      - POSTGRES_DB=myappvolumes:  pgdata:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>CI&#x2F;CD集成</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash"># Jenkinsfile示例pipeline &#123;  agent &#123; docker &#123; image 'node:14' &#125; &#125;  stages &#123;    stage('Build') &#123;      steps &#123;        sh 'npm install'        sh 'npm run build'      &#125;    &#125;    stage('Test') &#123;      steps &#123;        sh 'npm test'      &#125;    &#125;    stage('Docker Build') &#123;      steps &#123;        sh 'docker build -t myapp:$&#123;BUILD_NUMBER&#125; .'      &#125;    &#125;    stage('Deploy') &#123;      steps &#123;        sh 'docker push myapp:$&#123;BUILD_NUMBER&#125;'      &#125;    &#125;  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis基础巩固</title>
      <link href="/2024/05/04/redis/"/>
      <url>/2024/05/04/redis/</url>
      
        <content type="html"><![CDATA[<p>Redis 是一种高性能、基于内存的开源键值存储数据库系统，它主要用于<code>缓存</code>、<code>会话管理</code>和<code>实时分析</code>等用途。</p><h2 id="关键特点"><a href="#关键特点" class="headerlink" title="关键特点"></a>关键特点</h2><ol><li><strong>键值存储</strong>：Redis 以键值对的形式存储数据。每个键都是一个唯一的标识符，与一个值相关联。</li><li><strong>基于内存存储</strong>：Redis 所有的数据存储在内存中，这意味着它提供极高的读写速度，使得它非常适合用作缓存层，能够快速响应读取请求。</li><li><strong>持久性</strong>：Redis 支持数据持久性，可以将数据保存到磁盘上，以便在重启后恢复数据。</li><li><strong>多数据结构支持</strong>：Redis 支持各种数据结构，包括如字符串（string）、列表（list）、集合（set）、有序集合（sorted set）、哈希表（hash）、位图（bitmap）、超日志（hyperloglog）和地理空间索引（geospatial index）。等。这些数据结构可以用于不同的应用场景。</li><li><strong>发布-订阅模式</strong>：Redis 支持发布-订阅消息模式，允许客户端订阅特定的频道，从而实现高效的消息通信。这对于实现实时通信和事件驱动应用程序非常有用。</li><li><strong>事务</strong>：Redis 支持事务，可以一次执行多个命令，并保证这些命令在执行期间不会受到其他客户端的干扰。</li><li><strong>分布式和高可用性</strong>：通过使用哨兵（Sentinel）和集群，Redis 可以提供高可用性。哨兵用于监控 Redis 服务器的健康状况并自动执行故障转移。集群则提供数据分片和自动分区，以支持更大规模的数据存储。</li><li><strong>分片</strong>：Redis 支持数据分片，可以水平扩展存储容量和吞吐量。</li><li><strong>Lua脚本</strong>：Redis 允许使用 Lua 脚本执行高级操作，使得在服务器端可以进行复杂的逻辑，减少网络往返次数。</li><li><strong>社区支持</strong>：Redis 社区活跃，提供了许多扩展和插件，使其适用于各种不同的用例。</li></ol><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ol><li>字符串（String）<ul><li><strong>结构</strong>：字符串是Redis最基本的数据类型。Redis中的字符串是二进制安全的，这意味着它们可以包含任何数据，比如JPEG图像或序列化对象。</li><li><strong>实现</strong>：字符串使用简单的<strong>动态字符串</strong>（SDS，Simple Dynamic String）实现，允许快速追加操作，与Java中的<code>StringBuffer</code>或<code>StringBuilder</code>类似，都支持动态修改，如追加、修改等操作，而不需要每次操作都创建一个新的字符串实例。</li></ul></li><li>列表（List）<ul><li><strong>结构</strong>：列表是字符串元素的集合，按插入顺序排序。</li><li><strong>实现</strong>：Redis列表用双向链表或压缩列表（ziplist）实现。较小的列表通常使用压缩列表以节省空间，而较大的列表则使用双向链表。</li></ul></li><li>集合（Set）<ul><li><strong>结构</strong>：集合是无序的字符串集合，每个元素都是唯一的。</li><li><strong>实现</strong>：小的集合使用压缩列表（ziplist）实现，较大的集合使用散列（hash table）实现。</li></ul></li><li>有序集合（Sorted Set）<ul><li><strong>结构</strong>：有序集合类似于集合，但每个元素都关联一个分数。这些元素按分数有序排列。</li><li><strong>实现</strong>：有序集合通过跳跃表（skiplist）和散列结构组合实现。跳跃表用于按分数排序和范围查询，而散列用于快速访问。</li></ul></li><li>哈希（Hash）<ul><li><strong>结构</strong>：哈希是键值对的集合，类似于Java中的HashMap或Python中的字典。</li><li><strong>实现</strong>：小的哈希使用压缩列表实现，大的哈希使用散列结构。</li></ul></li><li>位图（Bitmap）<ul><li><strong>结构</strong>：位图不是独立的数据类型，而是在字符串上操作单个位（binary digit）。</li><li><strong>实现</strong>：通过对字符串类型的特殊操作实现，允许设置和查询字符串值的特定位。</li></ul></li><li>HyperLogLog<ul><li><strong>结构</strong>：用于高效地执行基数计数（比如计算一个集合中不同元素的数量）。</li><li><strong>实现</strong>：使用近似算法，牺牲了精度以换取极高的空间效率。</li></ul></li><li>地理空间索引（Geospatial）<ul><li><strong>结构</strong>：用于存储地理位置信息，并进行各种地理相关的计算，如两点之间的距离。</li><li><strong>实现</strong>：基于有序集合，利用Z-order曲线在一维值中编码二维经纬度。</li></ul></li></ol><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ol><li>缓存系统：<ul><li><strong>场景</strong>：减少数据库的负载，加速数据检索。</li><li><strong>实现</strong>：将经常查询的数据，如用户信息、商品详情等存储在 Redis 中。当数据被请求时，首先查询 Redis，如果找不到，再查询数据库，并将结果存回 Redis。</li></ul></li><li>会话存储（Session Store）：<ul><li><strong>场景</strong>：用于 Web 应用的用户会话管理。</li><li><strong>实现</strong>：将用户的会话信息存储在 Redis 中，由于 Redis 的读写速度快，可以快速处理大量并发的会话数据。</li></ul></li><li>消息队列：<ul><li><strong>场景</strong>：应用程序之间的消息传递和异步处理。</li><li><strong>实现</strong>：使用 Redis 的发布&#x2F;订阅功能或列表结构实现消息队列，支持生产者-消费者模型，实现数据的异步处理。</li></ul></li><li>排行榜&#x2F;计数器：<ul><li><strong>场景</strong>：用于实现社交网络、游戏等应用的排行榜功能。</li><li><strong>实现</strong>：利用 Redis 的有序集合（sorted set），可以快速添加、更新和获取排行榜数据。</li></ul></li><li>实时分析：<ul><li><strong>场景</strong>：网站的实时访问数据统计。</li><li><strong>实现</strong>：使用Redis的计数器功能，例如HyperLogLog来估计唯一访问者数量。</li></ul></li><li>地理空间数据处理：<ul><li><strong>场景</strong>：例如实现基于位置的服务，如查找附近的商店或用户。</li><li><strong>实现</strong>：使用Redis的地理空间索引功能，可以存储地理位置信息，并进行范围查询和距离计算。</li></ul></li><li>分布式锁：<ul><li><strong>场景</strong>：在分布式系统中同步不同进程或服务器之间的操作。</li><li><strong>实现</strong>：通过Redis的SETNX命令实现锁的机制，确保同一时间只有一个进程能执行特定的代码段。</li></ul></li><li>数据过期处理：<ul><li><strong>场景</strong>：自动删除过期的数据，如临时令牌或验证码。</li><li><strong>实现</strong>：利用Redis的键过期功能，可以为存储的数据设置生存时间。</li></ul></li></ol><h2 id="持久化机制"><a href="#持久化机制" class="headerlink" title="持久化机制"></a>持久化机制</h2><p>Redis 支持两种持久化机制，分别是RDB（Redis DataBase）和 AOF（Append Only File）。这两种机制可以单独使用，也可以同时使用，以便在不同的场景下平衡性能和数据安全性。</p><h3 id="RDB（默认持久化机制）"><a href="#RDB（默认持久化机制）" class="headerlink" title="RDB（默认持久化机制）"></a>RDB（默认持久化机制）</h3><ul><li><p><strong>介绍</strong>：</p><p>  RDB 持久化是通过<strong>创建数据集</strong>的快照来实现的。</p></li><li><p><strong>工作原理</strong>：</p><ul><li>在指定的时间间隔内，Redis自动创建当前数据的快照，并将其保存在一个紧凑的二进制文件中（默认为 <code>dump.rdb</code>）。Redis 在默认情况下只有一个 dump.rdb 文件，意味着每次创建新的RDB快照时，都会覆盖现有的<code>dump.rdb</code>文件。快照的创建可以通过自动或手动触发。自动触发基于配置的时间间隔和数据变化的次数。如果因为服务器宕机死机重启，那么内存中的数据就没了，但是他会从rdb中进行恢复。</li><li><strong>如果只开启了 RDB 持久化</strong>：<ul><li>那么每次启动或重启 Redis 时，它都会寻找 <code>dump.rdb</code> 文件。</li><li>如果找到了，就会加载该文件，将数据恢复到内存中。这个过程会阻塞服务器，直到加载完成。</li><li>如果没有找到 <code>dump.rdb</code> 文件，Redis 就会作为一个空实例启动。</li></ul></li><li><strong>如果同时开启了 RDB 和 AOF 两种持久化</strong>：<ul><li>在这种情况下，当 Redis 启动时，它会<strong>优先加载 AOF 文件</strong> (<code>appendonly.aof</code>) 来恢复数据，因为 AOF 文件通常保存了比 RDB 文件更完整、更新的数据。</li><li>此时，即使 <code>dump.rdb</code> 文件存在，<strong>它也会被忽略</strong>，Redis 不会加载它。</li></ul></li></ul></li><li><p><strong>优点</strong>：</p><ul><li><strong>高效性能</strong>：RDB是一个非常高效的方式来保存大量数据的快照。</li><li><strong>灾难恢复</strong>：由于RDB文件是压缩的二进制文件，适用于需要<code>定期备份数据</code>的情况，非常适合灾难恢复。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>数据丢失风险</strong>：如果Redis崩溃，自上次快照以来的所有数据都可能丢失。</li><li><strong>性能开销</strong>：在大数据集的情况下，保存快照可能会对性能产生短暂的影响。</li></ul><p>  在 RDB 持久性机制下，Redis 确实提供了两种快照（snapshot）保存方式：<code>SAVE</code> 和 <code>BFSAVE</code>。这两种命令都用于生成当前 Redis 数据库状态的快照，但它们在执行方式上有显著的不同。</p><p>  在 Redis 的配置文件 <code>redis.conf</code> 中，有类似这样的 <code>save</code> 配置项（默认开启）：</p><pre class="line-numbers language-language-json"><code class="language-language-json">save 900 1save 300 10save 60 10000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>  当 Redis 因为满足了上述 <code>save</code> 配置的条件而<strong>自动触发</strong>持久化时，它<strong>执行的操作等同于 <code>BGSAVE</code></strong>，而不是阻塞的 <code>SAVE</code>。Redis 会在后台创建一个子进程来完成快照的生成，以避免阻塞主服务。</p></li></ul><ol><li>SAVE<ul><li><strong>执行方式</strong>：<code>SAVE</code>命令会创建一个快照并将其保存在磁盘上，但这个过程是同步进行的。这意味着在 <code>SAVE</code>命令执行期间，Redis将停止处理其他命令。</li><li><strong>使用场景</strong>：由于<code>SAVE</code>会堵塞所有其他客户端请求，它通常不推荐在生产环境中使用。它更适用于低流量的时段或维护期间，例如，当需要确保数据完全同步到磁盘时。</li></ul></li><li>BGSAVE（ Redis 自动持久化时默认采用的机制）<ul><li><strong>执行方式</strong>：<code>BGSAVE</code>命令会在后台创建一个快照。具体来说，Redis会先创建一个子进程，然后子进程负责将快照写入磁盘，而父进程（即原始的Redis服务器进程）可以继续处理客户端请求。</li><li><strong>使用场景</strong>：由于<code>BGSAVE</code>不会堵塞主服务进程，它更适合生产环境中使用，尤其是在需要定期快照但又不希望影响服务性能的场合。</li></ul></li></ol><h3 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h3><ul><li><p><strong>介绍</strong>：</p><p>  AOF（Append-Only File）是一种将Redis操作命令以追加方式写入日志文件的机制，是追加式备份。它以文本格式记录Redis的写操作（新增、修改、删除），都会记录在这个AOF日志里。</p><p>  需要注意的是，<strong>redis是先执行写操作指令，随后再把指令追加进AOF文件中</strong>。</p></li><li><p><strong>工作原理</strong>：</p><ul><li>每个写操作命令都会追加到AOF文件的末尾。</li><li>类似于记录日志，把所有的写操作追加到文件。追加的形式是append，逐个命令追加，不是修改。<ul><li>比如说 <code>set key1 abc</code>, <code>set key1 123</code>，虽然两次设置key1的值，但不会合并，而是追加命令。</li><li>redis恢复的时候先恢复AOF，如果AOF有问题（比如破损），则再恢复RDB。</li><li>redis恢复的时候是读取AOF中的命令，从头到尾读一遍，然后数据恢复。</li></ul></li><li>Redis启动时，通过重新执行这些命令来重建原始数据。</li><li>AOF 是通过Redis主线程执行的，因此每个写操作都会导致磁盘I&#x2F;O，当然这是为了确保数据的持久性。</li></ul></li><li><p><strong>优点</strong>：</p><ul><li><strong>数据完整性</strong>：与RDB相比，AOF可以提供更好的数据完整性和安全性。</li><li><strong>易于阅读</strong>：AOF文件以文本格式存储，易于阅读和维护。</li><li><strong>灵活性</strong>：提供多种同步策略，如<code>每秒同步</code>、<code>每修改同步</code>等。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>文件大小</strong>：AOF文件可能会比RDB文件大很多，因为它保存了所有的写操作。</li><li><strong>性能开销</strong>：特别是在每次修改同步的配置下，可能会对写入性能产生影响。</li></ul></li></ul><p><strong>重写机制</strong>：</p><p>AOF 的重写机制是Redis用来优化AOF文件大小和性能的重要机制。随着操作的不断累积，AOF文件可能会变得非常大，包含许多已经不再需要的命令。<strong>AOF重写机制就会创建一个新的AOF文件，其中包含了与当前数据库相同的数据，但是采用更紧凑的格式，通常比原始AOF文件要小得多。这有助于减少AOF文件的大小，提高Redis性能，以及降低恢复速度。</strong></p><p><strong>工作原理</strong>：</p><ul><li><strong>创建新的AOF文件</strong>：在AOF重写过程中，Redis会创建一个新的AOF文件。并且<strong>不是去分析和重放旧的 AOF 日志文件</strong>，而是<strong>直接读取当前数据库内存中的数据</strong>，然后为这些数据生成一套最精简的写入命令。</li><li><strong>最小命令集</strong>：新的AOF文件仅包含使数据库达到当前状态的最小命令集。例如，如果一个键被修改多次，新的AOF文件只会包含这个键的最终状态。</li></ul><p><strong>重写模式</strong>：</p><p>AOF重写有两种模式，其中一种是<code>混合模式</code>，另一种是<code>纯AOF模式</code>。</p><ul><li><p><strong>混合模式（Mixed Mode）:</strong></p><ul><li>在混合模式下AOF重写生成的新AOF文件既包含AOF格式的写命令，也包含RDB快照的数据。</li><li>首先，redis会把当前所有的数据以rdb形式存入到AOF文件中，这些都是二进制文件，数据量小，随后新的数据会以AOF格式追加到这个AOF文件中。</li><li>恢复过程会更快，因为只需要加载一个文件。</li></ul></li><li><p><strong>纯AOF模式</strong>：</p><ul><li>AOF重写生成的新AOF文件仅包含AOF格式的写命令，不包含RDB快照的数据。但是会将一些重复的，没有意义的指令给去除掉，减少文件体积。</li></ul></li><li><p><strong>触发机制</strong></p><ul><li><strong>手动触发</strong>：可以通过执行<code>BGREWRITEAOF</code>命令手动触发AOF重写。</li><li><strong>自动触发</strong>：Redis还可以配置为在AOF文件增长到一定大小时自动触发重写。这是通过配置文件中的<code>auto-aof-rewrite-percentage</code> 和 <code>auto-aof-rewrite-min-size</code> 指令来控制的。</li></ul></li><li><p><strong>过程细节</strong></p><ul><li><strong>使用子进程</strong>：类似于<code>BGSAVE</code>命令，AOF重写也是在一个子进程中进行的，以避免堵塞主进程。</li><li><strong>追加写入期间的命令</strong>：在重写过程中，对Redis数据库进行的所有写操作同时会被追加到旧的和新的AOF文件中，确保数据一致性。</li><li><strong>切换文件</strong>：一旦新的AOF文件创建完成，Redis会使用新文件替换旧的AOF文件，并从此刻开始只向新文件追加新的写命令。</li></ul></li><li><p><strong>优点</strong></p><ul><li><strong>减少磁盘占用</strong>：通过删除命令，AOF重写能显著减少AOF文件的大小。</li><li><strong>提高重启速度</strong>：更小的AOF文件意味着重启时重放命令的速度更快。</li></ul></li><li><p><strong>注意事项</strong></p><ul><li><strong>性能影响</strong>：尽管AOF重写是非堵塞的，但它可能会增加磁盘I&#x2F;O负担，因此在高负载的系统上运行时需要小心。</li><li><strong>内存影响</strong>：与<code>BGSAVE</code>类似，AOF重写也会临时增加Redis的内存使用，因为它需要创建一个当前数据库状态的副本。</li></ul><p>  <strong>AOF小结</strong>：</p><ol><li>AOF写入：<ul><li>AOF操作本身是将每个写命令追加到AOF文件的过程。这种追加操作通常是非堵塞的，但它的行为取决于具体的配置。</li><li>在AOF配置中，有一个<code>appendfsync</code>选项，它控制着操作系统刷新数据到磁盘的时机，这个选项有三个设置：<ul><li><strong>always</strong>（每修改同步）：每个写命令都同步写入磁盘，这可能导致堵塞，尤其是在磁盘I&#x2F;O性能较差的时候。</li><li><strong>everysec</strong>（每秒同步）：在默认情况下，大多数Redis设置会使用这个选项。大约每秒同步一次，这是一种平衡性能和数据安全性的做法，通常不会引起显著的堵塞。</li><li><code>no</code>：交给操作系统决定何时进行数据写入，这种方式下写入操作是非堵塞的，但在系统崩溃的情况下可能会丢失更多数据。</li></ul></li></ul></li><li>AOF重写：<ul><li>AOF重写操作是非堵塞的。在执行AOF重写时，Redis会启动一个子进程来进行重写工作，而主进程继续处理客户端的请求。</li><li>由于AOF重写是在子进程中完成的，它不会堵塞正在进行的客户端命令处理。不过，它可能会对系统的整体性能产生影响，主要是因为磁盘I&#x2F;O和额外的CPU负载。</li></ul></li></ol></li></ul><h3 id="两种持久化机制如何选择"><a href="#两种持久化机制如何选择" class="headerlink" title="两种持久化机制如何选择"></a>两种持久化机制如何选择</h3><p>选择RDB还是AOF持久化取决于应用的需求。通常情况下，两者可以结合使用以获得更好的性能和持久性。例如，可以启用AOF来记录最近的写操作，并同时使用RDB来提供定期的全数据快照。</p><p>总的来说，RDB适合对<code>定期备份敏感</code>、<code>数据集较大</code>的场景，而AOF适合对<code>实时性要求较高</code>、<code>数据恢复性要求非常高</code>的场景。具体选择应该根据应用程序的性质和需求来确定。</p><h2 id="淘汰策略"><a href="#淘汰策略" class="headerlink" title="淘汰策略"></a>淘汰策略</h2><p>Redis 的数据淘汰策略是<strong>指当内存使用达到一定阈值时，Redis如何选择删除一些数据以释放内存的方法</strong>。这些策略主要用于当Redis被用作缓存时，帮助管理内存的使用。</p><ol><li><strong>noeviction</strong>（无淘汰策略）：当内存使用达到限制时，对写入操作返回错误，但允许读操作。这是<strong>默认策略</strong>。</li><li><strong>allkeys-lru</strong>（最近最少使用）：在内存达到限制时，在所有键中移除最近最少使用的键。适用于通用缓存场景。</li><li><strong>volatile-lru</strong>（过期时间中最少使用）：仅淘汰设置了过期时间的键中的最近最少使用的键。</li><li><strong>allkeys-random</strong>（随机）：在内存达到限制时，在所有键中随机移除键。</li><li><strong>volatile-random</strong>（过期时间中随机）：仅随机移除设置了过期时间的键。</li><li><strong>allkeys-lfu</strong>：在所有键中移除最不经常使用的键。</li><li><strong>volatile-lfu</strong>（最近最少频繁使用）：从已设置过期时间的键中，移除最不经常使用的键。</li><li><strong>volatile-ttl</strong>（最短剩余时间）：从已设置过期的键中，移除即将到期的键。</li></ol><p><strong>应用场景：</strong></p><p>选择哪种淘汰策略取决于具体的使用场景和需求。例如，如果使用Redis作为缓存，并且希望在内存不足时自动删除老旧数据，可以选择<strong>allkeys-lru</strong>策略。</p><p>对于关键数据，可能更倾向于使用<strong>noeviction</strong>策略，并在应用层面控制内存使用。</p><h2 id="非阻塞I-O模型"><a href="#非阻塞I-O模型" class="headerlink" title="非阻塞I&#x2F;O模型"></a>非阻塞I&#x2F;O模型</h2><p>在此之前，我们先了解下什么是I&#x2F;O模型。I&#x2F;O（输入&#x2F;输出）模型描述的是程序如何处理输入和输出操作。在计算机系统中，I&#x2F;O操作通常是指与外部设备（如硬盘、网络接口等）的数据交换。I&#x2F;O模型决定了程序在等待I&#x2F;O操作完成时的行为，这对程序的性能和响应能力有重要影响。</p><p>主要有以下几种I&#x2F;O模型：</p><ol><li>堵塞I&#x2F;O（Blocking I&#x2F;O）：<ul><li>在这种模型中，应用程序发起I&#x2F;O请求后，必须等待数据准备就绪并完成操作，期间应用程序被堵塞，不能执行其他任务。</li><li>例如，读取文件操作会一直等待，直到有数据可以读取。</li></ul></li><li>非堵塞I&#x2F;O（New I&#x2F;O）：<ul><li>应用程序发起I&#x2F;O请求后，如果数据未准备好，操作系统会立即返回一个错误（通常是“资源暂时不可用”），应用程序可以继续执行其他任务。</li><li>应用程序需要不断地询问操作系统数据是否准备好，这个过程为“轮询（polling）”。</li></ul></li><li>I&#x2F;O复用（I&#x2F;O Multiplexing）：<ul><li>应用程序通过一个API（如select、poll、epoll）监控多个I&#x2F;O流，一旦其中一个或多个I&#x2F;O流准备好，操作系统通知应用程序。</li><li>这种模型允许单个线程同时管理多个I&#x2F;O操作，而不是为每个I&#x2F;O操作创建单独的线程。</li></ul></li><li>信号驱动I&#x2F;O（Signal-driver I&#x2F;O）：<ul><li>应用程序告诉操作系统启动一个操作，并让操作系统在数据准备好时通过信号来通知它。</li><li>与非堵塞I&#x2F;O不同，信号驱动I&#x2F;O不需要应用程序不断地检查数据是否准备好。</li></ul></li><li>异步I&#x2F;O（Asynchronous I&#x2F;O）：<ul><li>应用程序发起I&#x2F;O操作后，可以立即开始执行下一个指令。操作系统将完成整个I&#x2F;O操作（包括数据传输）并在操作完成后通知应用程序。</li><li>这种模型下，应用程序无需等待I&#x2F;O操作的完成。</li></ul></li></ol><p><strong>接下来让我们通过一个关于Redis如何利用非阻塞I&#x2F;O处理客户端请求的例子来更好地理解非堵塞I&#x2F;O模型概念。</strong></p><p><strong>场景：客户端请求处理</strong></p><h3 id="传统堵塞I-O模型的限制"><a href="#传统堵塞I-O模型的限制" class="headerlink" title="传统堵塞I&#x2F;O模型的限制"></a><strong>传统堵塞I&#x2F;O模型的限制</strong></h3><p>在传统的堵塞I&#x2F;O模型下，当Redis服务器接收来自一个客户端的请求时，它必须等待整个请求的处理完全完成（包括等待所有必要的数据被读取或写入），在此期间，它不能处理来自其他客户端的任何其他请求。这意味着如果某个请求的处理需要一些时间（例如，一个复杂的查询或大量数据的读取），其他客户端必须等待，这降低了整体的响应性和吞吐量。</p><h3 id="Redis的非堵塞I-O模型"><a href="#Redis的非堵塞I-O模型" class="headerlink" title="Redis的非堵塞I&#x2F;O模型"></a><strong>Redis的非堵塞I&#x2F;O模型</strong></h3><p>现在再来看看Redis是如何使用非堵塞I&#x2F;O来优化这个过程的：</p><ol><li><strong>多个客户端同时连接</strong>：多个客户端同时向Redis服务器发送请求。</li><li>非堵塞I&#x2F;O操作：<ul><li>当Redis服务器接收到一个请求时，它会开始处理这个请求。如果在处理过程中需要进行I&#x2F;O操作（比如读取磁盘上的数据），Redis服务器不会在这个操作完成前被堵塞。</li><li>相反，如果数据尚未准备好，Redis可以暂时停止处理这个请求，并转而处理其他客户端的请求。</li></ul></li><li>I&#x2F;O多路复用：<ul><li>在后台，Redis使用I&#x2F;O多路复用技术（如epoll）来有效地监控所有活跃的客户端连接。</li><li>一旦某个请求的I&#x2F;O操作完成（例如，所需数据已准备好读取），I&#x2F;O多路复用机制会通知Redis服务器，然后Redis可以继续处理这个请求。</li></ul></li><li>高效并发处理：<ul><li>通过这种方式，Redis可以在单个线程中高效地处理多个并发请求，而无需为每个请求或连接创建单独的线程。</li><li>这提高了服务器的响应性和吞吐量，即使在面对大量并发请求时也能保持高性能。</li></ul></li></ol><p><strong>结论</strong>：Redis 正是使用了<strong>非堵塞I&#x2F;O</strong>和<strong>I&#x2F;O多路复用</strong>，才能够快速、高效地处理成千上万的并发连接和请求，而无需创建和管理多个线程，从而大大提高了资源利用率和性能。</p><h3 id="事件驱动模型"><a href="#事件驱动模型" class="headerlink" title="事件驱动模型"></a>事件驱动模型</h3><ul><li><strong>非堵塞的事件循环模型</strong>：</li><li>Redis 采用了非堵塞的事件循环模型，允许多个客户端并发请求。虽然Redis在任何给定时间点只能处理一个请求，但它可以快速轮询多个客户端请求，以确保高吞吐量。<ul><li>事件驱动：<ul><li>Redis的<strong>核心运行机制</strong>是<code>基于事件驱动</code>的。意味着Redis的主要功能是接收和处理客户端请求、执行命令、数据持久化等，都是通过响应各种事件来完成的。</li><li>在事件驱动模型中，Redis不需要为每个任务或请求创建新的线程，而是在单线程中异步处理这些事件。这种方式使得Redis能够高效地处理大量并发请求，同时保持简单的架构和低延迟。</li></ul></li><li><strong>事件循环</strong>：Redis的核心是一个<code>事件循环</code>，也称为<code>事件驱动循环</code>。这个事件循环不断地检查并处理发生的事件，而不会堵塞整个系统。它会轮询各个已注册的事件，如客户端连接事件、套接字可读事件、套接字可写事件等。</li><li><strong>事件监听</strong>：Redis使用操作系统提供的多路复用机制，如<code>epoll（Linux）</code>、<code>kqueue（BSD）</code>、<code>select</code>等，来监听多个套接字上的事件。这使得Redis能够同时处理多个客户端连接而无需为每个连接创建一个新线程。</li><li><strong>协程和事件处理</strong>：在一些版本的Redis中，引入了协程（Coroutine）和事件处理机制，可以更有效地处理多个客户端请求和数据库操作，提高了并发性能。</li></ul></li></ul><h2 id="Redis的操作是同步还是异步"><a href="#Redis的操作是同步还是异步" class="headerlink" title="Redis的操作是同步还是异步"></a>Redis的操作是同步还是异步</h2><p>先说结论，<strong>基于内存操作是同步，基于网络I&#x2F;O或磁盘I&#x2F;O是异步</strong>。</p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a><strong>同步</strong></h3><ul><li>当Redis接收到像<code>get key1, set key1</code>这样的命令时，如果数据在内存中（即不需要从磁盘加载），Redis会立即处理这个命令并同步返回结果。即使是在这种同步情况下，操作也是非常迅速的，因为它是在内存中进行的，几乎没有什么延迟。</li><li>因此，对于大多数常规操作，Redis的处理是足够快的，以至于即使这些操作都是同步进行的，也不会对性能造成显著影响。</li></ul><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a><strong>异步</strong></h3><ul><li>在网络I&#x2F;O方面，Redis使用<code>非堵塞I/O</code>机制。这意味着Redis可以在等待一个网络请求的数据准备好时，同时处理其他网络请求或内部任务。</li><li>在磁盘I&#x2F;O方面，需要区分不同类型的操作：<ul><li><strong>RDB持久化</strong>：当执行如<code>BGSAVE</code>命令进行RDB持久化时，Redis会在后台创建一个数据快照。这是一个异步操作，不会堵塞主事件循环。</li><li><strong>AOF持久化</strong>：对于AOF持久化，其行为取决于配置。如果配置为<code>appendfsync always</code>，则每次写入都会同步到磁盘，这可能是同步的。如果配置为<code>appendfsync everysec</code>或<code>appendfsync no</code>，则写入操作是异步的，因为实际的磁盘写入是延迟或由操作系统管理的。</li><li><strong>磁盘读取</strong>：对于磁盘读取（如重启时从RDB或AOF恢复数据），这通常是同步进行的，因为Redis需要这些数据来恢复其状态。</li></ul></li></ul><h2 id="Redis-单线程为什么这么快"><a href="#Redis-单线程为什么这么快" class="headerlink" title="Redis 单线程为什么这么快"></a>Redis 单线程为什么这么快</h2><p>Redis 采用单线程模型的优势在于它能够避免多线程带来的复杂性，降低了线程切换和锁竞争的开销，以及更容易实现一些关键操作的原子性。</p><ol><li>内存操作<ul><li><strong>快速数据访问</strong>：Redis是一个基于内存的数据存储系统。内存操作比磁盘操作快得多，避免了磁盘寻址和磁盘I&#x2F;O的开销。</li></ul></li><li>高效的数据结构<ul><li><strong>优化的实现</strong>：Redis内部使用高效的数据结构（如哈希表、跳跃表等），这些结构经过优化，能够快速执行数据操作，如添加、删除、查找和访问。</li></ul></li><li>单线程模型<ul><li><strong>避免上下文切换</strong>：多线程程序需要处理上下文切换的开销。Redis的单线程模型避免了这种开销，从而提高了效率。</li><li><strong>无需锁机制</strong>：Redis是单线程的，不需要担心数据同步和锁的问题。</li></ul></li><li>非堵塞I&#x2F;O<ul><li><strong>I&#x2F;O多路复用</strong>：Redis利用I&#x2F;O多路复用技术（如epoll、kqueue）来同时监听多个网络连接，从而提高网络通信效率。</li><li><strong>事件驱动模型</strong>：Redis使用基于非堵塞I&#x2F;O的事件驱动模型。这意味着即使在执行I&#x2F;O操作（如网络请求）时，Redis也不会被堵塞，而是能够继续处理其他任务。</li></ul></li><li>优化的命令执行<ul><li><strong>快速命令处理</strong>：Redis的大多数命令非常简单，可以在常数时间内完成（例如O(1)或O(log n)）。</li><li><strong>管道化和批量操作</strong>：Redis支持管道化，允许客户端一次性发送多个命令，然后Redis以此快速处理，减少了网络往返延迟。</li></ul></li></ol><h2 id="缓存雪崩、缓存击穿、缓存穿透"><a href="#缓存雪崩、缓存击穿、缓存穿透" class="headerlink" title="缓存雪崩、缓存击穿、缓存穿透"></a>缓存雪崩、缓存击穿、缓存穿透</h2><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h3><p><strong>原理</strong>：缓存雪崩是指在缓存系统中，大量的缓存数据几乎同时失效（过期），导致所有的请求都直接落到数据库上，从而可能引起数据库压力过大、甚至宕机的问题。 </p><p><strong>解决方法：</strong></p><ol><li><strong>设置不同的过期时间</strong>：为缓存数据设置略微不同的过期时间，防止大量数据同时过期。</li><li><strong>使用持久化</strong>：确保缓存层有持久化机制，这样即使缓存服务器重启，也可以从持久化存储中恢复数据。</li><li><strong>设置备用缓存</strong>：建立备用缓存或多级缓存策略，当主缓存不可用时，可以使用备用缓存。</li><li><strong>限流和降级</strong>：在系统架构中实现限流策略，以及在高负载时启用服务降级策略。</li></ol><h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h3><p><strong>原理</strong>：缓存击穿是指对某个热点 key 非常频繁的访问，在这个 key 突然失效的瞬间，大量请求直接达到数据库上，可能导致数据库短时间内承受巨大压力。</p><p> <strong>解决方法：</strong></p><ol><li><strong>设置热点数据永不过期</strong>：对于这些非常热门的数据，可以将它们设置为永不过期。</li><li><strong>使用互斥锁</strong>：当缓存失效时，不是所有请求都去数据库加载数据，而是使用某种互斥锁机制确保只有一个请求去请求数据库加载数据库并回填到缓存中。</li><li><strong>提前更新</strong>：监控这些热点key的访问频率和模式，根据预测在它们即将过期时提前更新它们的值。</li></ol><h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h3><p><strong>原理</strong>：缓存穿透是指查询一个<strong>一定不存在</strong>的数据，由于缓存不会命中，每次都要到数据库去查询，可能会被恶意利用，对数据库造成压力。 </p><p><strong>解决方法：</strong></p><ol><li><strong>布隆过滤器</strong>：在缓存之前使用布隆过滤器，它可以快速判断一个数据是否在数据集中，如果不存在，则无需查询数据库，直接返回空响应即可。</li><li><strong>缓存空对象</strong>：即使某个数据在数据库中不存在，也将这个“空”结果缓存起来，避免对同一个不存在的数据发起多次查询。</li><li><strong>参数校验</strong>：增加严格的参数校验，避免非法参数查询。</li></ol><h2 id="布隆过滤器是什么"><a href="#布隆过滤器是什么" class="headerlink" title="布隆过滤器是什么"></a>布隆过滤器是什么</h2><p>布隆过滤器是一种空间效率很高的概率型数据结构，用于测试一个元素是否是一个集合的成员。它的主要特点是：</p><ol><li><strong>如果布隆过滤器判断元素不存在</strong>：那么该元素一定不存在于集合中。</li><li><strong>如果布隆过滤器判断元素存在</strong>：则该元素可能存在于集合中。也就是说，存在一定的误判概率，即布隆过滤器可能会错误地判断某个不存在的元素为存在（称为假阳性）。</li></ol><p>布隆过滤器的这种特性使其非常适合用于那些不需要100%准确性但对空间效率有较高要求的场景，如网络应用中的缓存穿透问题、垃圾邮件检测等。</p><h3 id="原理简述"><a href="#原理简述" class="headerlink" title="原理简述"></a><strong>原理简述</strong></h3><p>布隆过滤器通过多个独立的哈希函数对元素进行处理。当添加一个元素时，它会被所有的哈希函数分别哈希，然后在对应的位置上做标记。在查询时，会对元素使用相同的哈希函数，检查所有对应的位置是否都被标记过。如果所有位置都被标记，布隆过滤器判断元素“<code>可能存在</code>”；如果任何一个位置未被标记，则元素“<code>一定不存在</code>”。</p><p><strong>场景</strong></p><p><strong>应用：缓存穿透问题</strong></p><pre><code>**问题**：缓存穿透是指缓存和数据库中都没有的数据，但请求者故意进行重复请求的现象。如这些请求数据由于不存在，每次请求都要访问数据库然后返回，这将导致数据库压力过大。**措施**：布隆过滤器可以用来防止缓存穿透。方法是将所有可能查询的数据哈希到一个足够大的布隆过滤器中。查询时，先查询布隆过滤器，如果布隆过滤器说数据不存在，那么肯定不存在，请求可以拒绝，从而避免对底层数据源的查询压力。如果布隆过滤器认为数据可能存在，请求才会被进一步的查询数据库或缓存系统。</code></pre><h2 id="Redis和数据库如何做到一致性"><a href="#Redis和数据库如何做到一致性" class="headerlink" title="Redis和数据库如何做到一致性"></a>Redis和数据库如何做到一致性</h2><ol><li>缓存失效策略<ul><li><strong>主动失效</strong>：每当数据库更新时，立即删除或更新缓存中的相应数据。</li><li><strong>延迟双删</strong>：<ol><li><strong>先删除缓存</strong></li><li><strong>再更新数据库</strong></li><li><strong>休眠一段时间（例如几百毫秒）</strong></li><li><strong>再次删除缓存</strong></li></ol></li></ul></li><li>读写分离和最终一致性<ul><li>对于某些非关键性应用，可以接受最终一致性而不是强一致性。</li><li>通过设置合理的缓存过期时间，可以在一定时间内自动更新缓存，减少数据不一致的时间窗口。</li></ul></li><li>事务和锁机制<ul><li>在更新数据时使用事务和锁来确保数据库操作和缓存操作的原子性。</li><li>这种方法适用于要求严格一致性的场景，但可能会影响系统的性能。</li></ul></li></ol><h2 id="Redis-分布式锁"><a href="#Redis-分布式锁" class="headerlink" title="Redis 分布式锁"></a>Redis 分布式锁</h2><h3 id="实现分布式锁"><a href="#实现分布式锁" class="headerlink" title="实现分布式锁"></a>实现分布式锁</h3><p>Redis 分布式锁底层实现的关键点在于 Redis <strong>提供的原子命令</strong>。</p><p>Redis 分布式锁的原理主要基于它的命令， <code>SETNX</code> （SET if not exists），Redis 使用单线程处理命令，因此在执行 SETNX 命令期间不会发生竞态条件（线程竞争），即使多个客户端同时尝试设置同一个键，Redis也会确保只有一个客户端成功设置。这使得 SETNX 命令成为实现分布式锁的理想选择，因为它可以安全地用于多个客户端之间的协调。</p><ol><li><strong>SETNX（SET if Not exists）命令</strong>：Redis中通常使用 SETNX 命令来尝试设置一个键的值，但仅在该键不存在时才设置成功。这一特性使得可以将键视为锁，当 SETNX 成功时标识锁被获取。</li><li>内部执行过程：<ul><li><strong>检查键是否存在</strong>：在执行 <code>SETNX</code> 命令时，Redis首先会检查指定的键是否已经存在于数据库中。</li><li><strong>如果键不存在，设置键的值</strong>：如果检查发现键不存在，Redis会执行设置操作，将指定的键设置为指定的值。这个设置操作是原子的，即Redis确保在多个客户端同时尝试设置相同键时，只有一个客户端会成功设置，而其他客户端将失败。</li><li><strong>返回结果</strong>：SETNX命令会根据操作的结果返回一个<code>布尔值</code>，通常是标识为<code>1</code>代表成功获取锁，<code>0</code>代表锁已被其他客户端持有。客户端可以根据这个返回值来确定是否成功获取锁或设置键的值。</li></ul></li></ol><p><strong>正确的方式：使用 <code>SET</code> 命令的扩展选项</strong></p><p>现代 Redis 推荐使用一条 <strong><code>SET</code> 命令来原子地完成加锁和设置超时。</strong></p><pre class="line-numbers language-language-bash"><code class="language-language-bash">SET lock_key "unique_random_value" NX PX 30000<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>lock_key</code>：锁的唯一标识。</li><li><code>unique_random_value</code>：<strong>一个唯一的随机字符串，用作锁的持有者凭证，用于安全地释放锁。</strong></li><li><code>NX</code>：只在 <code>lock_key</code> 不存在时才设置，等同于 <code>SETNX</code> 的效果。</li><li><code>PX 30000</code>：设置 <code>lock_key</code> 的过期时间为 30000 毫秒（30秒），避免死锁。</li></ul><p>这个操作是<strong>原子</strong>的，即 Redis 确保在多个客户端同时尝试设置相同键时，只有一个客户端会成功。命令返回 “OK” 代表成功获取锁，返回 “nil” 代表锁已被其他客户端持有。</p><p><strong>锁的释放：</strong></p><p>锁的持有者完成其任务后应该释放锁。<strong>为了防止错误地释放了别人的锁，释放操作必须是原子的“先比较、后删除”。<strong>这通常通过</strong>Lua 脚本</strong>来实现。</p><h3 id="为什么Redis适合做分布式锁"><a href="#为什么Redis适合做分布式锁" class="headerlink" title="为什么Redis适合做分布式锁"></a>为什么Redis适合做分布式锁</h3><ul><li><strong>原子性操作</strong>：Redis的<code>SETNX</code>命令和<code>EXPIRE</code>命令是原子操作，这意味着它们在执行时不会受到竞态条件的影响。这使得在多个客户端之间安全地实现分布式锁非常容易。</li><li><strong>高性能</strong>：Redis是一个内存数据库，可以在微秒级别执行操作，因此在获取和释放锁时非常快速。这使得Redis适合用作分布式锁。</li><li><strong>可用性</strong>：Redis支持主从复制和分片，因此即使某个Redis节点发生故障，其他节点仍然可以提供服务。这确保了分布式锁的可用性。</li><li><strong>超时和自动释放</strong>：Redis可以为锁设置超时时间，确保锁在一段时间后自动释放，防止死锁情况的发生。</li><li><strong>简单易用</strong>：Redis的API自带了SETNX和EXPIRE命令，创建锁和管理锁非常容易。</li></ul><h2 id="Redis-6-0-多线程的改动"><a href="#Redis-6-0-多线程的改动" class="headerlink" title="Redis 6.0 多线程的改动"></a>Redis 6.0 多线程的改动</h2><p>从 Redis 6.0版本开始，Redis开始引入了多线程模型来处理网络I&#x2F;O。这是Redis架构中的一个重要变化，但需要注意的是，<strong>这个多线程模型主要用于网络请求的读写操作，并不涉及到数据的实际读写操作，也就是说，数据的实际读写还是单线程在处理。</strong></p><p><strong>Redis多线程模型的特点</strong></p><ol><li><strong>限于网络I&#x2F;O</strong>：<strong>多线程在Redis中主要用于处理客户端请求的接受和响应的发送，即网络I&#x2F;O操作</strong>。数据的读取和写入仍然在主线程中单线程执行，以保证原子性和一致性。</li><li><strong>配置可调整</strong>：用户可以配置线程数量，以优化网络I&#x2F;O的性能。默认情况下，多线程不会启用，需要用户配置。</li></ol><p><strong>为什么采用这种多线程设计</strong></p><p>通过使用多线程处理网络I&#x2F;O，Redis可以更好地利用现代多核CPU的能力，提高在高并发情况下的性能，尤其是在处理大量网络请求时。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL基础巩固</title>
      <link href="/2024/05/03/mysql/"/>
      <url>/2024/05/03/mysql/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL-复习"><a href="#MySQL-复习" class="headerlink" title="MySQL 复习"></a>MySQL 复习</h1><p>在MySQL中，索引和数据都存储在磁盘中。当我们通过索引进行查询时，首先需要进行磁盘I&#x2F;O，将索引页加载到内存中。通过内存中的索引定位到数据所在的物理位置后，还需要再次进行磁盘I&#x2F;O来读取相应的数据行。由于磁盘I&#x2F;O相对于内存操作是非常耗时的，因此查询过程中发生的I&#x2F;O次数是决定查询性能的关键。因此数据库优化的一个核心目标就是减少查询时的磁盘I&#x2F;O次数。</p><p>这里说的”磁盘I&#x2F;O“指的是<strong>计算机的CPU和内存，与磁盘之间进行数据读取和写入的过程。</strong></p><h2 id="索引的目的"><a href="#索引的目的" class="headerlink" title="索引的目的"></a>索引的目的</h2><p><strong>加速查询</strong>：索引允许数据库快速定位到特定的数据行，显著提高了查询的效率，避免全表扫描。</p><p><strong>影响更新操作</strong>：索引需要在数据插入、更新、删除时进行维护，这可能会影响写操作的性能。</p><h2 id="索引的优势和劣势"><a href="#索引的优势和劣势" class="headerlink" title="索引的优势和劣势"></a>索引的优势和劣势</h2><p><strong>优势</strong>：</p><ol><li><strong>快速定位数据</strong>。</li><li><strong>优化排序和分组操作，因为B+树索引是有序的</strong>。</li><li><strong>提高大型表的查询效率</strong>。</li></ol><p><strong>劣势</strong>：</p><ol><li><strong>维护成本</strong>：影响数据的插入、更新、删除性能。</li><li><strong>存储开销</strong>：索引需要额外的存储空间。</li></ol><h3 id="索引的适用场景"><a href="#索引的适用场景" class="headerlink" title="索引的适用场景"></a>索引的适用场景</h3><ul><li>频繁作为查询条件的字段，特别是当查询返回的结果集为表中较小的子集时。</li></ul><h2 id="SQL语句可能不使用索引的情况"><a href="#SQL语句可能不使用索引的情况" class="headerlink" title="SQL语句可能不使用索引的情况"></a>SQL语句可能不使用索引的情况</h2><ol><li>使用函数或者算术运算，对索引列使用函数（如 <code>DATE_FORMAT(date_col, ...)</code>）或运算（如 <code>age + 1 = 30</code>），会导致数据库放弃使用索引。因为索引保存的是原始值，计算后的结果无法直接在索引树中查找。</li><li>使用 <code>NOT IN (...)</code> 或 <code>&lt;&gt;</code> &#x2F; <code>!=</code></li><li>在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。</li><li>查询优化器评估全表扫描比索引扫描更高效</li><li>表的数据量小，全表扫描会更快</li><li>like 查询中，使用前置通配符，即%符号在前。（like ‘%value’）<ul><li><strong>当%在前时</strong>：由于通配符在前，查询需要检查所有的记录来查找匹配项，因为<strong>索引是基于字段值的顺序构建的</strong>，所以当查询的开始部分是不确定的，数据库无法利用索引快速定位数据，只能执行全表扫描来查找匹配的行，导致索引失效。</li><li><strong>当%在后时</strong>：通配符在后，查询只需要匹配以<strong>关键字开头</strong>的任意字符，这种情况下，数据库可以利用索引快速定位到关键词开始的第一条记录，然后顺序扫描，直到不再匹配为止。</li></ul></li><li>数据类型不匹配：<ul><li>如 <code>WHERE column = &#39;123&#39;</code> 而不是 <code>WHERE column = 123</code>，这可能导致索引不被使用。</li></ul></li><li>NULL 值查询：<ul><li>查询涉及 <code>IS NULL</code> 或 <code>IS NOT NULL</code> 时，索引的使用可能会受到限制，尤其是在某些数据库配置下。</li></ul></li></ol><h2 id="COUNT-用法的性能分析和排序："><a href="#COUNT-用法的性能分析和排序：" class="headerlink" title="COUNT 用法的性能分析和排序："></a><code>COUNT</code> 用法的性能分析和排序：</h2><ol><li><code>COUNT(*)</code>：<ul><li><code>COUNT(*)</code> 统计表中的行数，不忽略任何行（包括含有 NULL 值的行）。它不关注表中的任何特定列，只是简单地计数所有行。</li><li>在多数数据库系统中，<code>COUNT(*)</code> 被特别优化，因为它只需要遍历索引或数据行来计算总行数，不需要查看列值。</li><li>统计所有行，包括包含 NULL 的行。</li></ul></li><li><code>COUNT(1)</code>：<ul><li><code>COUNT(1)</code> 实质上与 <code>COUNT(*)</code> 功能相同，因为 <code>1</code> 是一个常量表达式，表示“对每行计数”，与 <code>COUNT(*)</code> 一样，它不涉及任何列的内容。</li><li>在大多数现代数据库中，<code>COUNT(1)</code> 和 <code>COUNT(*)</code> 的性能非常接近，因为优化器识别这种计数模式并执行相同的操作。</li><li>统计所有行，包括包含 NULL 的行。</li></ul></li><li><code>COUNT(主键字段)</code>：<ul><li><code>COUNT(主键字段)</code> 统计主键字段非 NULL 的行数。由于主键字段不允许有 NULL 值，因此 <code>COUNT(主键字段)</code> 实际上与 <code>COUNT(*)</code> 一样，都是统计所有行。</li><li>这种方式的性能通常与 <code>COUNT(*)</code> 接近，但如果数据库优化器不充分识别这一点，可能略微有额外开销，因为它需要检查主键字段。</li><li>由于主键字段不能为 NULL，所以其结果和 <code>COUNT(*)</code> 相同，即统计所有行。</li></ul></li><li><code>COUNT(字段)</code>：<ul><li><code>COUNT(字段)</code> 统计指定字段非 NULL 的行数。这需要访问具体的列数据，并检查每个列值是否为 NULL。</li><li>如果该字段不是索引的一部分，这可能导致较慢的性能，因为数据库需要加载每一行的实际数据来检查该字段。</li></ul></li></ol><ul><li>只统计那些指定字段不为 NULL 的行。</li></ul><h2 id="MySQL索引的数据结构："><a href="#MySQL索引的数据结构：" class="headerlink" title="MySQL索引的数据结构："></a>MySQL索引的数据结构：</h2><ol><li><p>B+Tree（B+ 树）索引：</p><ol><li>MySQL主要的索引类型。用于 InnoDB、MyISAM、Memory 等存储引擎。</li><li>支持全值匹配、范围查询和前缀查找。</li><li><strong>在InnoDB存储引擎中</strong>，主键索引是聚簇索引，数据直接存在叶子节点上。</li><li><strong>在MyISAM存储引擎中</strong>，主键索引和普通索引一样，都是非聚簇索引，叶子节点存的是数据的物理地址指针。</li></ol></li><li><p>Hash（哈希）索引：</p><ol><li>基于哈希表，适用于等值查询。</li><li>适用于 Memory 存储引擎。</li><li><strong>当某个索引值被频繁访问时，InnoDB会在B-Tree索引上再创建一个Hash 索引，这使得某些查找操作更加高效</strong>。</li><li><strong>查找速度非常快，<strong>Hash索引的插入和删除速度</strong>非常快</strong>，因为哈希函数定位到存储位置通常是O(1)的操作，比B+Tree的查找再插入&#x2F;删除要直接。</li></ol><p> <strong>缺点是</strong>：</p><ol><li><strong>不支持范围查询</strong>：<code>&gt;</code>、<code>&lt;</code>、<code>BETWEEN</code>、<code>LIKE</code>前缀匹配都无法使用哈希索引。</li><li><strong>不支持排序</strong>：<code>ORDER BY</code>无法利用哈希索引。</li><li><strong>哈希冲突问题</strong>：一旦大量数据有相同的哈希值（哈希冲突），索引的性能会急剧下降，从O(1)退化为O(n)，查询效率还不如全表扫描。</li></ol></li></ol><h2 id="MySQL的索引结构为什么要选B-树？"><a href="#MySQL的索引结构为什么要选B-树？" class="headerlink" title="MySQL的索引结构为什么要选B+树？"></a>MySQL的索引结构为什么要选B+树？</h2><ol><li><p><strong>二分查找树</strong>是一个天然的二分结构，能很好的利用二分查找快速定位数据，但是它存在一种极端的情况，每当插入的元素都是树内最大&#x2F;最小的元素，就会导致二分查找树退化成一个链表，此时查询复杂度就会从O(logn)降低为O(n)。</p></li><li><p><strong>自平衡二叉树</strong>保证了查询操作的时间复杂度就会一直维持在O(logn)。但是它本质上还是一个二叉树，每个节点只能有2个子节点，随着元素的增多，树的高度会越来越高。</p><blockquote><p>树的高度通常决定了查询时潜在的磁盘I&#x2F;O操作次数，因为树结构是存储在磁盘上的，并且访问树中的每个节点都可能对应一次磁盘I&#x2F;O操作，尤其是当节点数据不在内存中时。因此，树的高度越高，每次查询可能涉及的磁盘I&#x2F;O次数越多，从而可能影响查询的性能。然而，使用内存缓冲等技术可以减少这种影响。</p></blockquote><p> 而B树和B+树都是通过多叉树的方式，会将树的高度变矮，所以这两个数据结构非常适合检索存于磁盘中的数据。</p><p> 但是MySQL默认的存储引擎InnoDB采用的是B+树作为索引的数据结构，原因有：</p><ul><li>B+树的非叶子节点不存放实际的记录数据，仅存放索引，因此在数据量相同的情况下，相比存储即存索引又存记录的B树，B+树的非叶子节点可以存放更多的索引，因此B+树可以比B树更[矮胖]，查询底层节点的磁盘I&#x2F;O次数会更少。</li><li>B+树有大量的冗余节点（所有非叶子节点可以存放更多的索引），这些冗余索引让B+树在输入、删除的效率都更高，比如删除根节点的时候，不会像B树那样发生复杂的树的变化。</li><li>B+树叶子节点之间用链表连接了起来，有利于范围查询，而B树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及到多个节点的磁盘I&#x2F;O操作，范围查询效率不如B+树。</li></ul></li></ol><h2 id="聚簇索引和非聚簇索引的区别"><a href="#聚簇索引和非聚簇索引的区别" class="headerlink" title="聚簇索引和非聚簇索引的区别"></a>聚簇索引和非聚簇索引的区别</h2><p>在InnoDB引擎下，聚簇索引理解为主键索引，非聚簇索引理解为普通索引。</p><h3 id="聚簇索引-VS-非聚簇索引"><a href="#聚簇索引-VS-非聚簇索引" class="headerlink" title="聚簇索引 VS 非聚簇索引"></a>聚簇索引 VS 非聚簇索引</h3><ol><li>聚簇索引<ul><li>索引和数据行都是直接存储在索引的叶子节点上。</li><li>一个表只能有一个聚簇索引，通常是主键。</li></ul></li><li>非聚簇索引<ul><li>索引和数据行分开存储，索引项包含指向数据行的指针。</li><li>一个表可以有多个非聚簇索引。</li><li>查询可能会导致回表操作，即通过索引找到数据行的实际位置。</li></ul></li></ol><h2 id="MySQL的全表扫描"><a href="#MySQL的全表扫描" class="headerlink" title="MySQL的全表扫描"></a>MySQL的全表扫描</h2><p>全表扫描是指<strong>数据库执行查询操作时，不通过索引，逐一读取表中的每一行来确定哪些行符合查询条件</strong>。</p><p><strong>何时会触发全表扫描</strong>：</p><ol><li>表中没有建立索引或者查询的时候没有使用到索引；</li><li>使用了函数或表达式；</li><li>查询优化器觉得全表扫描更高效。</li></ol><p><strong>全表扫描的优势</strong>：</p><ol><li><strong>简单</strong>：不需要考虑索引维护和构建的复杂性。</li><li><strong>适用于小表</strong>：对于非常小的表，全表扫描很可能比建立和维护索引更快。</li><li><strong>避免索引的开销</strong>：没有索引，就没有更新索引的开销。</li></ol><p><strong>全表扫描的劣势</strong>：</p><ol><li><strong>对于大表来讲，效率较低</strong></li><li><strong>I&#x2F;O开销大，全表扫描会导致大量的磁盘I&#x2F;O</strong>。</li></ol><h2 id="MySQL-回表"><a href="#MySQL-回表" class="headerlink" title="MySQL 回表"></a>MySQL 回表</h2><p><strong>场景</strong>：当通过非聚簇索引查询数据时，从返回的数据里面，无法获取所需的全部数据（即查询的列不是索引的一部分），<strong>系统就需要再次通过主键索引（聚簇索引）去查询剩下这部分数据</strong>。这个<code>额外查询的过程</code>被称为”回表“。回表是一个附加的步骤，可能会导致额外的I&#x2F;O操作。</p><p><strong>性能影响</strong>：回表操作涉及至少两次索引查找，第一次是通过非聚簇索引查询数据时，第二次是通过聚簇索引（即主键索引）查询数据时。在数据行分散或索引不是完全覆盖查询需求的情况下，回表操作可能成为性能瓶颈，尤其在数据量大或查询复杂时。</p><p><strong>发生回表的场景</strong>：</p><ul><li>当查询的列不在索引中时，即查询的列超出了索引所覆盖的范围。</li><li>使用 <code>select *</code> 查询时，因为表示所有列，而非聚簇索引只包含索引列和主键列，所有需要回表获取其他列的数据。</li><li>当查询条件（WHERE子句）使用了非聚簇索引进行筛选，但返回的结果集（SELECT子句）中包含了非索引列。</li></ul><h3 id="避免和解决回表问题-覆盖索引"><a href="#避免和解决回表问题-覆盖索引" class="headerlink" title="避免和解决回表问题 - 覆盖索引"></a>避免和解决回表问题 - 覆盖索引</h3><p>覆盖索引是非聚簇索引，它包含了查询所有列。<strong>当查询只涉及索引中的列时，就可以直接从索引中获取数据，无需回表</strong>。例如，如果你经常查询name和age，你可以创建一个包含name、age和id(主键)的索引。</p><p>在使用覆盖索引时，列的顺序很重要，它影响了索引的使用方式和效率。如果你为 <code>age</code> 和 <code>name</code> 建立了一个复合索引，索引的创建顺序会影响查询的性能。这里是几个关键点：</p><ol><li><strong>索引的顺序</strong>：如果索引是按 <code>(age, name)</code> 的顺序建立的，那么在查询时首先使用 <code>age</code> 作为过滤条件的查询将能高效地利用这个索引。例如，<code>SELECT age, name FROM table WHERE age = 30</code> 能够充分利用该索引。但如果查询主要基于 <code>name</code> 而非 <code>age</code>，如 <code>SELECT age, name FROM table WHERE name = &#39;John&#39;</code>，那么这个索引可能不会被优化器选择，因为 <code>name</code> 是索引的第二部分。</li><li><strong>最左前缀原则</strong>：在使用复合索引时，数据库遵循<strong>最左前缀原则</strong>。这意味着查询条件<strong>必须包含</strong>索引最左边的列，索引才能被高效地用于查找。例如，对于 <code>INDEX(age, name)</code>，只要 <code>WHERE</code> 子句中包含了对 <code>age</code> 的筛选条件，<strong>无论该条件写在何处</strong>，优化器都能利用该索引进行快速定位。但如果查询直接使用 <code>name</code> 而忽略 <code>age</code>，索引将无法用于定位，其效率便会大大降低。</li><li><strong>多条件查询</strong>：如果查询涉及多个条件且这些条件对应于索引中的列，那么索引的效率取决于这些条件是如何使用索引列的。例如，<code>SELECT age, name FROM table WHERE age = 30 AND name = &#39;John&#39;</code> 可以完美地利用 <code>(age, name)</code> 索引。</li></ol><p>注：在 InnoDB 存储引擎中，每个表都有一个聚簇索引，通常是主键。如果你没有明确指定一个主键，InnoDB 会选择一个唯一的非空索引作为聚簇索引。如果表中没有这样的索引，InnoDB会为表生成一个隐藏的、包含6字节长度的聚簇索引。</p><h3 id="为什么回表是通过主键索引，速度却还是这么慢？"><a href="#为什么回表是通过主键索引，速度却还是这么慢？" class="headerlink" title="为什么回表是通过主键索引，速度却还是这么慢？"></a>为什么回表是通过主键索引，速度却还是这么慢？</h3><p>核心就在于：<strong>“回表”不是一次性的动作，而是“一行数据，回一次表”。<strong>如果你的查询通过二级索引找到了</strong>多行</strong>数据，那么MySQL就需要<strong>多次</strong>执行回表操作。</p><p>我们来看一个非常经典的场景：</p><p><strong>表结构</strong>: <code>users</code> 表，有 <code>id</code> (主键), <code>status</code> (普通索引), <code>name</code>, <code>email</code> 等字段。<br>查询语句：<strong>SELECT * FROM users WHERE status &#x3D; ‘active’;</strong></p><p>假设数据库里有10,000个用户的<code>status</code>是<code>&#39;active&#39;</code>。</p><p><strong>MySQL的执行流程是这样的：</strong></p><ol><li><strong>第一次查找（在<code>status</code>索引上）</strong>：<ul><li>MySQL在<code>status</code>索引这棵B+树上进行范围查找，定位到第一条<code>status = &#39;active&#39;</code>的记录。</li><li>这个索引的叶子节点上存的是 <code>(status值, 主键id)</code>。比如，它找到了 <code>(&#39;active&#39;, 101)</code>。</li></ul></li><li><strong>第一次回表</strong>：<ul><li>拿到主键<code>id = 101</code>。</li><li>去主键索引（聚簇索引）这棵B+树里，查找<code>id = 101</code>的完整数据行。</li><li>把这行数据放入结果集。</li></ul></li><li><strong>第二次查找（在<code>status</code>索引上）</strong>：<ul><li>MySQL在<code>status</code>索引上继续向后查找，找到了下一条<code>status = &#39;active&#39;</code>的记录。</li><li>比如，它找到了 <code>(&#39;active&#39;, 105)</code>。</li></ul></li><li><strong>第二次回表</strong>：<ul><li>拿到主键<code>id = 105</code>。</li><li>再次去主键索引里查找<code>id = 105</code>的完整数据行。</li><li>把这行数据放入结果集。</li></ul></li><li><strong>…这个过程会循环往复…</strong></li><li><strong>第10,000次回表</strong>：<ul><li>…</li></ul></li></ol><p><strong>这个过程会一直重复，直到所有<code>status = &#39;active&#39;</code>的记录都被找到为止。</strong></p><p>回表操作之所以慢，不在于单次回表的开销，而在于当查询返回大量数据时，它会<strong>按行、高并发地、重复地</strong>发生，每一次都可能伴随着随机磁盘I&#x2F;O，最终导致整个查询的性能雪崩。</p><p>这也是为什么“<strong>覆盖索引</strong>”如此重要的原因，因为它能让数据库在二级索引上找到所有需要的数据，从根本上消除了这成千上万次回表的可能性。</p><h2 id="MySQL-页"><a href="#MySQL-页" class="headerlink" title="MySQL 页"></a>MySQL 页</h2><p>在 MySQL 中，“页” 通常是指<strong>数据库存储引擎中数据存储的基本单位</strong>。在 InnoDB 存储引擎中，页的默认大小是 16KB。<strong>数据页用于存储表中的行数据，索引页用于存储索引数据。</strong></p><ol><li>数据页和事务日志：<ul><li>InnoDB 存储引擎将数据存储在数据页中。每当数据被修改（如通过 insert、update或delete操作）时，InnoDB 不仅会更改磁盘上的数据页，而且还会在事务日志（称为redo log）中记录这些更改。这是为了确保在系统崩溃后能够恢复数据。</li></ul></li><li>回滚段（Undo Logs）：<ul><li>当事务进行修改时，原始数据会存储在所谓的回滚段中。这些回滚段实际上也存储在数据页中。它们允许数据库在事务失败或明确地回滚时恢复到原始状态。同时，回滚段支持MVCC，使得不同的事务可以看到数据的不同历史版本。</li></ul></li><li>事务的一致性视图：<ul><li>InnoDB 通过在每个事务开始时创建一个一致性视图来实现 MVCC。这意味着每个事务可以看到在其启动时刻之前已经提交的数据的快照。这个机制依赖于回滚段中的数据来提供旧版本的数据，允许对同一数据的并发读写操作。</li></ul></li><li>锁定和并发控制：<ul><li>页面级的锁定（虽然 InnoDB 主要使用行级锁）可以用于管理对共享数据的并发访问。这有助于处理死锁和提高并发事务的性能。</li></ul></li></ol><h2 id="MySQL-的-InnoDB-和-MyISAM-的区别"><a href="#MySQL-的-InnoDB-和-MyISAM-的区别" class="headerlink" title="MySQL 的 InnoDB 和 MyISAM 的区别"></a>MySQL 的 InnoDB 和 MyISAM 的区别</h2><table><thead><tr><th>区别点</th><th>InnoDB</th><th>MyISAM</th></tr></thead><tbody><tr><td>事务支持</td><td>支持ACID事务，有提交和崩溃回滚功能</td><td>不支持</td></tr><tr><td>锁定机制</td><td>采用行级锁，进行数据修改时只锁定所涉及的行</td><td>采用表级锁，进行写操作时会锁定整个表</td></tr><tr><td>外键</td><td>支持外键</td><td>不支持</td></tr><tr><td>崩溃恢复</td><td>由于其事务日志、双写缓冲区和自动崩溃恢复功能，InnoDB在系统崩溃后可以恢复到一致的状态</td><td>没有事务日志，因此在崩溃后可能需要修复或重建表</td></tr><tr><td>性能</td><td>通常在需要高并发写入的应用中表现更好，因为它使用行级锁定</td><td>在只读或大量读的应用中可能会更快</td></tr><tr><td>MVCC (多版本并发控制)</td><td>支持</td><td>不支持</td></tr></tbody></table><p><strong>总结</strong>：因为MyISAM相对简单所以在效率上要优于InnoDB，如果系统读多写少，同时表数据量不是很大，并对原⼦性要求低，那么MyISAM最好的选择。且MyISAM恢复速度快。可直接⽤备份覆盖恢复。如果系统读少，写多的时候，尤其是并发写⼊⾼的时候。InnoDB就是⾸选了。</p><h2 id="MySQL-的主备同步（主从复制）"><a href="#MySQL-的主备同步（主从复制）" class="headerlink" title="MySQL 的主备同步（主从复制）"></a>MySQL 的主备同步（主从复制）</h2><p>主从复制是一种常用的数据备份和读取扩展策略。它允许数据从一个 MySQL 数据库服务器（主服务器）复制到一个或多个MySQL 数据库服务器（从服务器）。</p><p>MySQL 主备复制实现分成三个步骤：</p><ol><li>master 将所有数据更改（如 INSERT、UPDATE、DELETE 命令）都会记录在二进制日志中（binary log）中</li><li>slave 将 master 的binary log，简称binlog，拷贝到它的中继日志（relay log）</li><li>salve 重做中继日志的事件，将改变反映它自己的数据</li></ol><h2 id="SQL-语句执⾏顺序"><a href="#SQL-语句执⾏顺序" class="headerlink" title="SQL 语句执⾏顺序"></a>SQL 语句执⾏顺序</h2><ol><li><p><strong>FROM</strong> 子句</p><ul><li>首先执行FROM子句，确定查询的数据源，可能是一张表、多张表的连接或子查询。</li></ul></li><li><p><strong>ON</strong> 子句</p><ul><li>如果涉及JOIN操作，接下来将执行ON子句以确定如何连接表。</li></ul></li><li><p><strong>JOIN</strong> 子句</p><ul><li>执行JOIN操作，根据ON子句的条件确定如何连接表。</li></ul></li><li><p><strong>WHERE</strong> 子句</p><ul><li>接着执行WHERE子句，对FROM子句确定的数据源进行行级过滤。</li></ul></li><li><p><strong>GROUP BY</strong> 子句</p><ul><li>执行GROUP BY子句，将数据分组以供聚合函数使用。</li></ul></li><li><p><strong>HAVING</strong> 子句</p><ul><li>在分组后执行HAVING子句，对分组后的数据进行过滤，HAVING子句通常用于聚合函数的条件过滤。</li></ul></li><li><p><strong>SELECT</strong> 子句</p><ul><li>然后执行SELECT子句，选取特定的列。如果使用了聚合函数，此时将计算聚合值。</li></ul></li><li><p><strong>DISTINCT</strong> 子句</p><ul><li>如果使用了DISTINCT关键字，将在此阶段对结果进行去重。</li></ul></li><li><p><strong>ORDER BY</strong> 子句</p><ul><li>执行ORDER BY子句，对结果集进行排序。</li></ul></li><li><p><strong>LIMIT &#x2F; OFFSET</strong> 子句</p><ul><li>最后执行LIMIT&#x2F;OFFSET子句，限制返回的结果数量或跳过一定数量的行。</li></ul><pre class="line-numbers language-language-sql"><code class="language-language-sql">SELECT DISTINCT column1FROM table1JOIN table2 ON table1.id = table2.idWHERE column2 > 10GROUP BY column1HAVING COUNT(*) > 1ORDER BY column1LIMIT 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行顺序为：</p><ol><li>FROM table1</li><li>JOIN table2 ON <a href="http://table1.id/">table1.id</a> &#x3D; <a href="http://table2.id/">table2.id</a></li><li>WHERE column2 &gt; 10</li><li>GROUP BY column1</li><li>HAVING COUNT(*) &gt; 1</li><li>SELECT DISTINCT column1</li><li>ORDER BY column1</li><li>LIMIT 10</li></ol></li></ol><h2 id="SQL优化"><a href="#SQL优化" class="headerlink" title="SQL优化"></a>SQL优化</h2><ol><li>避免使用 select * ，只选择需要查询的列</li><li>避免在列上使用函数或者算术运算</li><li>正确使用索引<ul><li>为经常用于查询条件的列创建索引</li><li>定期检查并优化索引，删除不再需要的索引</li><li>避免过度索引，因为索引会增加写操作的开销</li></ul></li><li>限制查询的数目以及返回的结果条数</li><li>使用合适的数据类型，字段尽量使⽤not null，因为NULL值列表会占用 1 字节空间。</li><li>尽量避免在查询中使用子查询，考虑使用连接查询（JOIN）</li><li>调整数据库配置，当表过大时，及时做好数据归档或分库分表</li><li>使用 EXPLAIN 分析查询<ul><li>使用 EXPLAIN 命令可以查看MySQL如何执行特定的SQL查询</li><li>它会查询的执行计划，包括哪些索引被使用、表的扫描方式、估计的行数等</li><li>通过分析EXPLAIN的输出，你可以确定哪些部分的查询可能导致性能问题</li></ul></li><li>监控慢查询日志，找出需要优化的查询。</li></ol><h2 id="MySQL的分库分表"><a href="#MySQL的分库分表" class="headerlink" title="MySQL的分库分表"></a>MySQL的分库分表</h2><p>分库分表是数据库水平扩展的常用策略，用于解决单一数据服务器承载压力过大的问题。</p><h3 id="分库分表的场景"><a href="#分库分表的场景" class="headerlink" title="分库分表的场景"></a>分库分表的场景</h3><ol><li>数据量巨大：当单表的数据量达到数千万、数亿甚至更多时，查询性能可能会下降，此时需要考虑分表</li><li>高并发请求：当数据库的读写请求较高，超出了单一数据库服务器的处理能力，可能需要分库来分散读写压力</li><li>业务扩展</li><li>数据存储限制</li><li>提高数据安全性</li></ol><h3 id="分库分表的策略"><a href="#分库分表的策略" class="headerlink" title="分库分表的策略"></a>分库分表的策略</h3><ol><li>垂直分库&#x2F;分表<ul><li>根据业务功能将表分组，每个组放在不同的数据库中</li><li>例如，用户相关的表放在一个数据库中，订单相关的表放在另一个数据库中</li><li>这种方式可以将不同的业务负载分散在不同的数据库服务器上</li></ul></li><li>水平分库&#x2F;分表<ul><li>将表中的数据按某种规则分散到多个相同结构的表中</li><li>常见的分表键包括：用户ID、订单ID、时间等</li><li>例如，根据用户ID的范围将数据分散到不同的表中</li></ul></li><li>基于范围的分库<ul><li>根据分表键的范围将数据分散到不同的数据库或表中</li><li>例如，根据时间范围将数据分散到不同的表中</li></ul></li></ol><h2 id="事务的ACID属性是什么？"><a href="#事务的ACID属性是什么？" class="headerlink" title="事务的ACID属性是什么？"></a>事务的ACID属性是什么？</h2><p>A：原子性，原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生</p><p>C：一致性，事务前后数据的完整性必须保持一致。</p><p>I：隔离性，事务的隔离性是指多个用户并发访问数据库时，一个用户的事务不能被其他用户的事务所干扰，多个并发事务之间数据要相互隔离</p><p>D：持久性，事务一旦被提交，它对数据库中数据的改变就是永久性，接下来即使数据库发生故障也不应该对其有任何影响。</p><pre class="line-numbers language-language-java"><code class="language-language-java">原子性和一致性有点类型，但是两者的侧重点不同，原子性关注状态，要么全部成功，要么全部失败，不存在部分成功的状态。而一致性关注数据的可见性，中间状态的数据对外部不可见，只有最初状态和最终状态的数据对外可见。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="什么是死锁？如何预防和解决死锁？"><a href="#什么是死锁？如何预防和解决死锁？" class="headerlink" title="什么是死锁？如何预防和解决死锁？"></a>什么是死锁？如何预防和解决死锁？</h2><p>当涉及到数据库时，死锁通常是指<strong>两个或多个事务在等待对方释放资源，从而导致它们都无法继续执行的情况。</strong></p><h3 id="死锁的四个必要条件"><a href="#死锁的四个必要条件" class="headerlink" title="死锁的四个必要条件"></a>死锁的四个必要条件</h3><ol><li><strong>互斥条件</strong>：资源不能被多个事务同时占用。</li><li><strong>请求与保持条件</strong>：一个事务在请求新的资源时保持对其他资源的占用。</li><li><strong>不剥夺条件</strong>：已经分配给一个事务的资源不能被其他事务强行剥夺，只有该事务使用完毕后才释放资源。</li><li><strong>循环等待条件</strong>：存在一个等待循环，即事务A等待事务B释放资源，事务B等待事务C释放资源，事务C又等待事务A释放资源，形成一个闭环。</li></ol><h3 id="如何预防死锁"><a href="#如何预防死锁" class="headerlink" title="如何预防死锁"></a>如何预防死锁</h3><ol><li><strong>超时</strong>：为事务设置一个超时时间。</li><li><strong>死锁检测</strong>：数据库系统可以定期检查是否存在死锁。</li><li><strong>预先分配</strong>：在事务开始时请求所有必需的资源。</li></ol><h3 id="如何解决死锁"><a href="#如何解决死锁" class="headerlink" title="如何解决死锁"></a>如何解决死锁</h3><ol><li><strong>手动干预</strong>：DBA可以手动终止或回滚某些事务，从而释放资源并打破死锁。</li><li><strong>使用死锁检测工具</strong>：许多数据库系统都提供了死锁检测工具，可以帮助DBA找到并解决死锁问题。</li></ol><h2 id="描述数据库的隔离级别以及它们之间的区别"><a href="#描述数据库的隔离级别以及它们之间的区别" class="headerlink" title="描述数据库的隔离级别以及它们之间的区别"></a>描述数据库的隔离级别以及它们之间的区别</h2><ol><li><strong>未提交读</strong>：事务最低的隔离级别，它允许另外一个事务可以看到另一个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻读</li><li><strong>已提交读</strong>：保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。可能避免脏读，但可能出现不可重复读和幻读。</li><li><strong>可重复读</strong>：可以防止脏读和不可重复读。但是可能出现幻读。除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免不可重复读。</li><li><strong>可串行化</strong>：最高事务隔离级别。事务被处理为顺序执行，除了防止脏读，不可重复读，还避免了幻读。</li></ol><table><thead><tr><th>隔离级别</th><th>脏读</th><th>不可重复读</th><th>幻读</th></tr></thead><tbody><tr><td>未提交读</td><td>可能</td><td>可能</td><td>可能</td></tr><tr><td>已提交读</td><td>不可能</td><td>可能</td><td>可能</td></tr><tr><td>可重复读</td><td>不可能</td><td>不可能</td><td>可能</td></tr><tr><td>可串行化</td><td>不可能</td><td>不可能</td><td>不可能</td></tr></tbody></table><h2 id="MySQL-的MVCC-是什么"><a href="#MySQL-的MVCC-是什么" class="headerlink" title="MySQL 的MVCC 是什么"></a>MySQL 的MVCC 是什么</h2><p>MVCC 是一种<code>多版本并发控制</code>机制，通过事务的可见性看到自己预期的数据，能降低其系统开销。（RC和RR工作）</p><p>InnoDB 的MVCC，是通过在每行记录后面保存系统版本号（可以理解为事务的ID），每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID。这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的，防止幻读的产生。</p><ol><li>MVCC 手段只适用于MySQL隔离级别中的读已提交（Read commited）和可重复读（Repeatable Read）。</li><li>Read uncommited 由于存在脏读，即能读到未提交事务的数据行，所以不适用MVCC。</li><li>简单的select快照读不会加锁，删改及select for update 等需要当前读的场景会加锁。</li></ol><p>原因是MVCC的创建版本和删除版本只要在事务提交后才会产生。客观上，MySQL使用的是乐观锁的一种实现方式，就是每行都有版本号，保存时根据版本号觉得是否成功。InnoDB的MVCC使用到快照存储在Undo日志中，该日志通过回滚指针把一个数据行所有快照连接起来。</p><h2 id="什么是视图（View）？它与普通表有什么不同？"><a href="#什么是视图（View）？它与普通表有什么不同？" class="headerlink" title="什么是视图（View）？它与普通表有什么不同？"></a>什么是视图（View）？它与普通表有什么不同？</h2><p>视图是数据库中的一个虚拟表，是基于一个或多个实际表的结果集的表现形式。视图并不存储数据，只是保存了一个SQL查询。当视图被引用时，该查询会被执行，从而生成视图的数据。</p><table><thead><tr><th>区别</th><th>视图</th><th>普通表</th></tr></thead><tbody><tr><td>数据存储</td><td>不存储数据。它只是一个基于实际表的查询的定义。每次查询视图时，都会执行其定义的查询</td><td>实际存储数据。数据在物理磁盘上有实际的存储位置</td></tr><tr><td>更新性</td><td>不是所有的视图都可以更新。只有当视图基于一个单一的表，并且没有使用某些聚合函数、DISTINCT关键字等时，才可以更新。</td><td>可以直接更新</td></tr><tr><td>复杂性和灵活性</td><td>可以基于多个表，并使用复杂的SQL逻辑，如连接、过滤和聚合。这使得视图可以提供一个简化或特定的数据视图，隐藏复杂的SQL细节</td><td>表示实际的数据结构，通常不包含复杂的逻辑或计算</td></tr><tr><td>用途</td><td>抽象和隐藏复杂的SQL查询</td><td>用于持久存储数据</td></tr><tr><td>性能</td><td>由于视图是在每次查询时动态生成的，所以可能会有性能问题，特别是当视图基于多个表或包含复杂逻辑时</td><td>直接查询普通表通常更快，因为数据是实际存储的</td></tr></tbody></table><h2 id="如何备份和恢复MySQL数据库？以及什么是数据库的冷备份和热备份？"><a href="#如何备份和恢复MySQL数据库？以及什么是数据库的冷备份和热备份？" class="headerlink" title="如何备份和恢复MySQL数据库？以及什么是数据库的冷备份和热备份？"></a>如何备份和恢复MySQL数据库？以及什么是数据库的冷备份和热备份？</h2><p>使用 mysqldump 工具，mysqldump 是MySQL提供的一个命令行工具，用于导出数据库为SQL文件。</p><p>示例：备份数据库</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">mysqldump -u [username] -p[password] [database_name] > [dump_file.sql]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>示例：备份特定的表</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">mysqldump -u [username] -p[password] [database_name] [table_name] > [dump_file.sql]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="如何恢复MySQL数据库"><a href="#如何恢复MySQL数据库" class="headerlink" title="如何恢复MySQL数据库"></a>如何恢复MySQL数据库</h2><p>可以使用MySQL命令行客户端直接执行SQL文件来恢复数据库</p><pre class="line-numbers language-language-sql"><code class="language-language-sql">mysql -u [username] -p[password] [database_name] < [dump_file.sql]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="冷备份与热备份"><a href="#冷备份与热备份" class="headerlink" title="冷备份与热备份"></a>冷备份与热备份</h2><h3 id="冷备份（Code-Backup）"><a href="#冷备份（Code-Backup）" class="headerlink" title="冷备份（Code Backup）"></a>冷备份（Code Backup）</h3><p><strong>特点</strong></p><ol><li>冷备份是在数据库处于关闭状态时进行的备份。</li><li>在备份期间，数据库不可用，不能进行读写操作。</li><li>冷备份通常更简单，因为它不需要处理并发的数据修改。备份的数据是一致的，不需要额外的日志或恢复步骤。</li></ol><p><strong>优点</strong>：数据一致性得到保证，恢复过程简单快速。</p><p><strong>缺点</strong>：需要停机，可能导致业务中断。</p><p><strong>实现方式</strong>：冷备份的实现相对简单。通常只需停止数据库服务，然后复制数据库文件（例如数据文件、日志文件等）到备份位置。完成后，再重新启动数据库服务。</p><h3 id="热备份（Hot-Backup）"><a href="#热备份（Hot-Backup）" class="headerlink" title="热备份（Hot Backup）"></a>热备份（Hot Backup）</h3><ol><li>热备份是在数据库处于运行状态时进行的备份，不需要停机。</li><li>在备份期间，数据库仍然可用，可用进行读写操作。</li><li>热备份通常需要更复杂的策略，因为需要确保备份的数据一致性。这通常涉及到使用日志文件来记录备份期间的所有更改，以便在恢复时应用这些更改。</li><li>MySQL的InnoDB存储支持热备份。</li></ol><p><strong>优点</strong>：不需要停机，数据库始终可用。可以实现近实时或增量备份，减少备份时间和存储需求。</p><p><strong>缺点</strong>：可能需要额外的存储和计算资源。恢复过程可能比冷备份复杂，特别是当需要应用大量的日志更改时。</p><p><strong>实现方式</strong>：热备份的实现较为复杂。通常需要专门的备份工具或软件。这些工具可以在数据库运行时捕获数据的一致性快照，并记录备份期间的所有更改。恢复时，首先恢复数据快照，然后应用备份期间的所有更改。</p><h2 id="MySQL的InnoDB存储引擎能够支持热备份的原因主要与其内部的设计和特性有关"><a href="#MySQL的InnoDB存储引擎能够支持热备份的原因主要与其内部的设计和特性有关" class="headerlink" title="MySQL的InnoDB存储引擎能够支持热备份的原因主要与其内部的设计和特性有关"></a>MySQL的InnoDB存储引擎能够支持热备份的原因主要与其内部的设计和特性有关</h2><ol><li><strong>事务日志（Redo Logs）</strong>：InnoDB 使用事务日志来确保数据的持久性和恢复能力。当数据发生变化时，这些变化首先被记录在redo日志中，然后再异步地写入磁盘。在热备份期间，即使数据正在变化，也可以使用这些日志来捕获备份开始后的所有更改，确保数据的一致性。</li><li><strong>多版本并发控制（MVCC）</strong>:InnoDB 使用MVCC来支持高并发。这意味着每个事务都看到一个数据的“快照”，而不是实时数据。这使得在热备份期间可以为备份创建一个数据快照，而不影响正在运行的事务。</li><li><strong>一致性非锁定读取</strong>：InnoDB允许在不锁定整个表或数据库的情况下进行一致性读取。这意味着备份过程可以读取数据，而不会堵塞其他事务。</li></ol><p>总的来说，选择冷备份还是热备份取决于业务需求。如果可以承受短时间的停机，冷备份可能是一个简单的选择。如果需要24&#x2F;7的可用性，热备份可能是更好的选择，但可能需要复杂的备份和恢复策略。</p><h2 id="如何确保数据库的高可用性和灾难恢复"><a href="#如何确保数据库的高可用性和灾难恢复" class="headerlink" title="如何确保数据库的高可用性和灾难恢复"></a>如何确保数据库的高可用性和灾难恢复</h2><p><strong>高可用性</strong>是确保数据库在出现故障时仍然可用的能力。</p><p><strong>策略</strong>：</p><ol><li><strong>主从复制</strong>：一个主数据库处理写操作，一个或多个从数据库处理读操作。如果主数据库出现故障，一个从数据库可以被提升为新的主数据库。</li><li><strong>负载均衡</strong>：使用负载均衡器分发数据库请求，确保没有单点故障。</li></ol><p><strong>灾难恢复</strong>是在发生灾难性事件后恢复数据和服务的能力。</p><p><strong>策略</strong>：</p><ol><li><strong>定期备份</strong>：定期将数据库备份到安全的位置。</li><li><strong>异地备份</strong>：将备份存储在与生产数据库不同的地理位置。</li><li><strong>备份验证</strong>：定期验证备份的完整性和可恢复性。</li></ol><h2 id="慢查询"><a href="#慢查询" class="headerlink" title="慢查询"></a>慢查询</h2><p><strong>含义</strong>：慢查询是数据库中查询时间超过指定阈值的SQL，指的是在日志中记录运行比较慢的SQL语句。</p><h3 id="启用慢查询日志"><a href="#启用慢查询日志" class="headerlink" title="启用慢查询日志"></a>启用慢查询日志</h3><ul><li>确保MySQL 数据库的慢查询日志（slow query log）被启用。可以通过设置 slow_query_log 和 long_query_time 参数来开启和配置慢查询日志。</li></ul><h3 id="分析慢查询日志"><a href="#分析慢查询日志" class="headerlink" title="分析慢查询日志"></a>分析慢查询日志</h3><ul><li>日志文件分析：定期检查慢查询日志文件，查找执行时间长、频率高的SQL语句。</li><li>使用工具：使用 mysqldumpslow 或 pt-query-digest 这样的工具来分析MySQL 的慢查询日志。</li></ul><h3 id="实时性能分析"><a href="#实时性能分析" class="headerlink" title="实时性能分析"></a>实时性能分析</h3><ul><li>使用 show processlist：在MySQL中，可以使用 show processlist 命令来查看当前执行的查询及其运行时间。</li></ul><h3 id="导致慢查询的原因"><a href="#导致慢查询的原因" class="headerlink" title="导致慢查询的原因"></a>导致慢查询的原因</h3><ol><li><strong>缺乏适当的索引</strong>：没有为查询中的条件列创建索引，导致数据库执行全表扫描。</li><li><strong>索引不当</strong>：虽然存在索引，但可能不是最优的，所以优化器没有走索引查询。</li><li><strong>查询设计不当</strong>：使用了低效的SQL语句，如使用子查询代替连接。</li><li><strong>数据量巨大</strong>：随着数据的增长，未优化的查询可能会变得越来越慢。</li><li><strong>硬件限制</strong>：如磁盘I&#x2F;O、CPU或内存瓶颈。</li><li><strong>使用了大量的JOIN操作</strong>：特别是在大表上。</li><li><strong>使用了复杂的聚合函数</strong>：如GROUP BY、COUNT、SUM等。</li><li><strong>网络延迟</strong>：尤其是在分布式数据库系统中。</li></ol><h3 id="如何避免慢查询"><a href="#如何避免慢查询" class="headerlink" title="如何避免慢查询"></a>如何避免慢查询</h3><ol><li><strong>优化查询</strong>：确保SQL语句尽可能简单高效。</li><li><strong>使用索引</strong>：为经常用于查询条件的列创建索引。</li><li><em>避免在查询中使用‘</em>’ **：只选择需要的列。</li><li><strong>减少JOIN操作的数据</strong>：尤其是在大表上。</li><li><strong>使用数据库的查询优化工具</strong>：如MySQL的<code>EXPLAIN</code>。</li><li><strong>定时维护数据库</strong>：如优化表、更新统计信息等。</li></ol><h3 id="如何解决慢查询"><a href="#如何解决慢查询" class="headerlink" title="如何解决慢查询"></a>如何解决慢查询</h3><ol><li><strong>分析慢查询日志</strong>：找出最慢的查询。</li><li><strong>使用EXPLAIN命令</strong>：查看查询的执行计划，找出瓶颈。</li><li><strong>优化查询</strong>：根据EXPLAIN的结果，重写SQL语句。</li><li><strong>添加或调整索引</strong>：确保查询使用了最优的索引。</li><li><strong>考虑硬件升级</strong>：如增加内存、使用更快的磁盘或升级CPU。</li><li><strong>考虑使用缓存</strong>：考虑在程序中使用缓存，如redis或memcached，减少对数据库的请求。</li></ol><h2 id="为什么JOIN会影响查询缓慢，导致慢查询（基于大表的情况下）"><a href="#为什么JOIN会影响查询缓慢，导致慢查询（基于大表的情况下）" class="headerlink" title="为什么JOIN会影响查询缓慢，导致慢查询（基于大表的情况下）"></a>为什么JOIN会影响查询缓慢，导致慢查询（基于大表的情况下）</h2><p>首先 JOIN 操作是关系型数据库中的一个核心操作，它允许从两个或多个表中组合数据。</p><ol><li><strong>数据量增加</strong>：当两个表进行 JOIN 操作时，结果集的大小可能会显著增加，特别是在进行笛卡尔积（每个表中的每一行与另一个表中的每一行组合）时。</li><li><strong>全表扫描</strong>：如果没有适当的索引支持JOIN操作，数据库可能需要对一个或两个表进行全表扫描，这在大表上是非常耗时的。</li><li><strong>复杂的数据处理</strong>：JOIN操作可能需要数据库进行复杂的数据处理，如排序、过滤和聚合，这会增加查询的执行时间。</li><li><strong>I&#x2F;O开销</strong>：JOIN操作可能导致大量的磁盘I&#x2F;O，特别是当数据不在内存中时。</li><li><strong>临时表</strong>：某些JOIN操作可能需要数据库创建临时表来处理查询结果，这回增加额外的I&#x2F;O开销和处理时间。</li><li><strong>多表连接</strong>：连接多个表会增加查询的复杂性和处理时间。每增加一个表，都可能导致性能下载。</li></ol><p>为了避免由于JOIN操作导致的性能问题，可以采取以下策略</p><ol><li><strong>使用适当的索引</strong>：确保JOIN操作的列都有索引，并且这些索引是最优的</li><li><strong>减少结果集的大小</strong>：在JOIN操作之前使用WHERE子句过滤数据，以减少需要处理的数据量。</li><li><strong>避免笛卡尔积</strong>：始终在JOIN操作中使用明确的连接条件，以避免不必要的数据组合。</li><li><strong>只连接所需的表</strong>：避免不必要的表连接，只连接真正需要的表。</li><li><strong>使用EXPLAIN命令分析</strong></li></ol><p>笛卡尔积：简单的说就是两个集合相乘的结果。</p><p>假设集合A&#x3D;{a, b}，集合B&#x3D;{0, 1,2}，则两个集合的笛卡尔积为{(a, 0),(a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}。</p><h2 id="分布式数据库与传统数据库的主要区别"><a href="#分布式数据库与传统数据库的主要区别" class="headerlink" title="分布式数据库与传统数据库的主要区别"></a>分布式数据库与传统数据库的主要区别</h2><table><thead><tr><th>区别点</th><th>分布式数据库</th><th>传统数据库</th></tr></thead><tbody><tr><td>数据存储位置</td><td>数据被分布在多个物理位置上。</td><td>数据通常存储在单一的物理位置上。</td></tr><tr><td>可扩展性</td><td>可以通过添加更多的节点来水平扩展，以支持更大的数据量和更高的查询负载。</td><td>通常依赖于垂直扩展，即增加单一服务器的硬件资源。</td></tr><tr><td>可用性和容错性</td><td>由于数据在多个节点上有多个副本，即使某些节点出现故障，数据仍然可用。</td><td>如果单一的数据库服务器出现故障，可能会导致数据不可用。</td></tr><tr><td>数据一致性</td><td>可能使用不同的数据一致性模型，如最终一致性，这意味着在某些情况下，不同的节点可能会看到数据的不同版本。</td><td>通常使用严格的ACID事务模型，确保数据的强一致性。</td></tr><tr><td>复杂性</td><td>由于需要处理节点之间的通信、数据同步和故障恢复等问题，所以通常比传统数据库更复杂。</td><td>由于所有数据都在一个地方，所以管理和维护相对简单。</td></tr><tr><td>查询性能</td><td>可以并行处理查询，因为数据分布在多个节点上。但跨节点的查询可能会受到网络延迟的影响。</td><td>查询性能取决于单一服务器的硬件和数据库的优化。</td></tr><tr><td>事务处理</td><td>处理跨多个节点的事务可能会更复杂，并可能需要特殊的协议，如两阶段提交。</td><td>事务处理通常更简单和直接，因为所有数据都在同一个地方。</td></tr></tbody></table><h2 id="Explain关键字详解"><a href="#Explain关键字详解" class="headerlink" title="Explain关键字详解"></a>Explain关键字详解</h2><p>EXPLAIN是MySQL中的一个关键字，用于显示MySQL如何执行SQL查询。通过Explain，可以了解查询的执行计划，从而找出可能的性能瓶颈并进行优化。</p><p><strong>使用方法</strong>：你只需在你的SELECT、INSERT、UPDATE或DELETE查询前加上EXPLAIN关键字：</p><p>EXPLAIN SELECT * FROM _table WHERE _column &#x3D; ‘some_value’;</p><h3 id="Explain-返回的参数及其意义："><a href="#Explain-返回的参数及其意义：" class="headerlink" title="Explain 返回的参数及其意义："></a>Explain 返回的参数及其意义：</h3><ol><li>id：查询的标识符。如果查询中包含子查询，每个子查询都会有一个不同的id。</li><li>select_type：查询的类型。常见的值有：<ul><li>SIMPLE：简单的SELECT查询，不包含子查询或JOIN。</li><li>PRIMARY：查询中最外层的SELECT。</li><li>SUBQUERY：在SELECT或WHERE子句中的子查询。</li><li>DERIVED：在FROM子句中的子查询。</li></ul></li><li>table：输出结果集的表。</li><li>type：连接类型。性能从好到差依次是 <code>system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL</code>。<ul><li>system：表只有一行（等于const）</li><li>const：通过主键或者唯一索引查找一行。</li><li>eq_ref：通过主键或者唯一索引查找多行。</li><li>ref：通过非唯一索引查找。</li><li>range：通过索引查找某个范围的行。</li><li>index：全索引扫描。</li><li>ALL：全表扫描。</li></ul></li><li>possible_keys：可能使用的索引。</li><li>key：实际使用的索引。如果这里是<code>NULL</code>，说明MySQL没找到可用的索引。</li><li>key_len：使用的索引的长度。</li><li>ref：显示索引的哪一列被使用。</li><li>rows：预估扫描的行数。这个数字越小越好。如果一个查询只返回10行，但这里预估扫描了100万行，那问题就非常大了</li><li>Extra：额外信息，这是<strong>诊断的精髓所在</strong>：<ol><li><code>Using filesort</code>：一个巨大的警报。说明MySQL无法利用索引完成排序，不得不在内存或磁盘上进行额外的排序操作，非常消耗性能。</li><li><code>Using temporary</code>：另一个巨大的警报。说明MySQL需要创建一个临时表来处理查询，通常发生在复杂的<code>GROUP BY</code>或<code>UNION</code>操作中。</li><li><code>Using index</code>：一个极好的信号。表示查询触发了<strong>覆盖索引</strong>，无需回表，性能极高。</li><li><code>Using where</code>：表示MySQL在存储引擎层找到数据后，还需要在Server层进行额外的条件过滤。</li></ol></li></ol><p>通过理解EXPLAIN的输出，开发者可以更好地了解查询的执行方式，找出可能的性能瓶颈，并进行相应的优化。例如，如果type列显示为ALL，这意味着进行了全表扫描，这通常是一个性能瓶颈，可能需要添加或调整索引来改善性能。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud Alibaba-Nacos</title>
      <link href="/2024/04/23/spring-cloud-alibaba-nacos-fu-xi/"/>
      <url>/2024/04/23/spring-cloud-alibaba-nacos-fu-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring-Cloud-Alibaba-Nacos"><a href="#Spring-Cloud-Alibaba-Nacos" class="headerlink" title="Spring Cloud Alibaba Nacos"></a>Spring Cloud Alibaba Nacos</h1><h2 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h2><p>Nacos 是由阿里巴巴开源的一个<strong>动态服务发现</strong>、<strong>配置</strong>和<strong>服务管理平台</strong>，专为微服务架构设计。它帮助开发者实现云原生应用的动态服务发现和配置管理，提供了轻量级的服务注册和发现机制以及动态的服务配置功能。</p><h3 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h3><ol><li>服务发现与注册：<ul><li>Nacos 支持 DNS-based 和 RPC-based 服务发现，提供实时健康检查，确保服务的可用性。服务实例上线或下线时，Nacos 会自动处理服务列表，客户端通过订阅服务列表获取最新的服务实例信息。</li></ul></li><li>动态配置管理：<ul><li>Nacos 提供中心化的配置管理服务，允许动态地管理应用配置，而无需重启服务。它支持配置自动更新，服务在运行时可以根据配置变化进行自适应调整。</li></ul></li><li>服务元数据和流量管理：<ul><li>Nacos 支持存储服务的元数据，如权重、负载均衡策略等，这些元数据可以用于服务调度和流量管理。</li></ul></li><li>支持多种配置格式：<ul><li>Nacos 支持多种配置格式，包括但不限于 properties、YAML、JSON等。</li></ul></li><li>支持多种环境：<ul><li>Nacos 支持多环境、多租户的配置管理，助力于微服务的多环境隔离。</li></ul></li></ol><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h2><p>服务注册信息主要存储在服务器的内存中，以便快速响应服务发现的请求。这部分是服务注册的核心存储机制。</p><p>对于配置管理，Nacos 提供了两种存储方式：</p><ul><li><strong>内存中</strong>：为了快速访问，配置数据也会被缓存于内存中。</li><li><strong>持久化存储</strong>：在持久化模式下，Nacos 支持使用外部数据库来存储配置数据。</li></ul><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><h3 id="1、Nacos-是怎么发现新进来的微服务实例的？"><a href="#1、Nacos-是怎么发现新进来的微服务实例的？" class="headerlink" title="1、Nacos 是怎么发现新进来的微服务实例的？"></a>1、Nacos 是怎么发现新进来的微服务实例的？</h3><p>当一个微服务实例启动时，首先在 yml 文件中配置 Nacos 注册中心的地址，然后在启动类上添加服务发现 @EnableDiscoveryClient 注解。这样它会自动向 Nacos Server 注册当前服务实例。这个过程通常是通过调用 Nacos Server 的 API，将自己的服务名称、实例ID、IP地址、端口号等信息发送给 Nacos Server。注册成功之后，Nacos 并能感知到这个实例的存在。</p><h3 id="2、Nacos-是如何发现一个服务不可用的？"><a href="#2、Nacos-是如何发现一个服务不可用的？" class="headerlink" title="2、Nacos 是如何发现一个服务不可用的？"></a>2、Nacos 是如何发现一个服务不可用的？</h3><p>Nacos Server 会定时发送心跳检查请求到每一个已注册的服务实例，以确认它们是否还存活。如果在一定时间内（默认是30秒）没有收到某个服务实例的心跳响应，Nacos 会认为这个服务实例已经不可用，并将其从服务列表中剔除。</p><h3 id="3、当一个服务要调用另一个服务时，流程是哪样子的？"><a href="#3、当一个服务要调用另一个服务时，流程是哪样子的？" class="headerlink" title="3、当一个服务要调用另一个服务时，流程是哪样子的？"></a>3、当一个服务要调用另一个服务时，流程是哪样子的？</h3><ul><li>当服务 A 想要调用服务 B 时，服务 A 首先会向服务注册中心（Nacos）请求服务 B 的实例信息。</li><li>服务注册中心接收到响应，然后返回服务 B 所有可用实例列表给服务A。</li><li>服务 A 通过集成的客户端负载均衡器（如Ribbon），根据某种负载均衡策略（如轮询、随机、权重等）选择一个服务B的实例。</li><li>服务 A 通过选定的服务 B 实例的网络地址发起对服务 B 的调用。通常通过 HTTP 或其他 RPC 协议实现。</li><li>如果调用失败，服务A可以选择另一个服务B的实例进行重试。</li><li>被调用的服务 B 实例接收请求，处理业务逻辑，并返回响应给服务 A。</li></ul><h3 id="4、灰度发布和蓝绿部署是什么意思？"><a href="#4、灰度发布和蓝绿部署是什么意思？" class="headerlink" title="4、灰度发布和蓝绿部署是什么意思？"></a>4、灰度发布和蓝绿部署是什么意思？</h3><ul><li>灰度发布：是指新版本的服务逐渐替代旧版本的过程，初期只有少部分用户使用新版本，如果新版本稳定，再逐渐扩大到所有用户。这样可以确保新版本的稳定性，并减少因新版本引入的问题对所有用户的影响。</li><li>蓝绿部署：是指同时部署两个版本的服务，蓝色代表旧版本，绿色代表新版本。所有的流量首先路由到蓝色版本，当绿色版本准备好并经过测试后，流量可以切换到绿色版本。这样可以快速回滚到蓝色版本，如果绿色版本有问题。</li></ul><h3 id="5、Nacos-和-Gateway-之间是怎么样的关系？"><a href="#5、Nacos-和-Gateway-之间是怎么样的关系？" class="headerlink" title="5、Nacos 和 Gateway 之间是怎么样的关系？"></a>5、Nacos 和 Gateway 之间是怎么样的关系？</h3><ul><li>Nacos 主要负责<code>服务的注册与发现</code>以及<code>配置管理</code>。而Gateway 是<code>API网关</code>，负责处理外部请求并将其路由到相应的微服务。</li><li>当 Gateway 需要路由请求到某个服务时，它会向服务注册中心查询这个服务的实例列表，然后通过集成的负载均衡器，根据负载均衡策略选择一个实例进行调用。</li></ul><h3 id="6、Nacos-集群之间是怎么同步的？"><a href="#6、Nacos-集群之间是怎么同步的？" class="headerlink" title="6、Nacos 集群之间是怎么同步的？"></a>6、Nacos 集群之间是怎么同步的？</h3><p>Nacos 集群中的数据同步主要依靠 Raft 协议来保证一致性。这是一种分布式计算中用于实现多节点间一致性的协议，常用于多副本数据的一致性保障。</p><ol><li>领导选举（Leader Election）：<ul><li>当 Nacos 集群启动时，或者领导者节点失效时，集群会通过 Raft 协议进行新的领导者选举。</li><li>所有的写操作（比如更新配置信息或服务注册信息）都必须通过领导者来进行。</li></ul></li><li>日志复制（Log Replication）：<ul><li>当领导者节点接收到一个更新请求（比如配置更新或服务注册），它首先将这个请求作为一个新的日志条目添加到它的日志中。</li><li>然后，领导者将这个日志条目发送给其他的追随者节点。</li><li>追随者节点将该日志条目添加到它们各自的日志中，并向领导者确认接收成功。</li></ul></li><li>数据提交：<ul><li>一旦领导者节点从大多数追随者那里收到了确认，它就会提交该日志条目，并将更新应用到系统状态上。</li><li>领导者之后会通知追随者已经提交了哪些日志条目，追随者随后也会将这些条目提交并应用到各自的系统状态。</li></ul></li><li>读取操作：<ul><li>对于读操作，可以直接由任何一个节点处理，但为了保证读到的数据是最新的，通过读操作也会通过领导者节点来进行。</li></ul></li></ol><h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><p>CAP 理论指出，一个分布式系统不可能同时满足以下三点：</p><ul><li><strong>一致性（Consistency）</strong>：每次读取都能得到最新的写入或错误响应。</li><li><strong>可用性（Availability）</strong>：每次请求都能得到响应，无论成功或失败。</li><li><strong>分区容错性（Partition tolerance）</strong>：系统中任何信息的丢失或失败都不会影响系统的继续运行。</li></ul><h3 id="Nacos-的-CAP-配置"><a href="#Nacos-的-CAP-配置" class="headerlink" title="Nacos 的 CAP 配置"></a>Nacos 的 CAP 配置</h3><p>Nacos 提供了两种运行模式，可以根据需要在一致性和可用性之间进行选择，确保系统按需求正确运行：</p><p>在CAP理论中，Nacos可以配置为更偏向于“一致性（C）”和“分区容错性（P）”，也可以配置为更偏向于“可用性（A）”和“分区容错性（P）”，这取决于具体的使用场景和配置。</p><ul><li>不同的运行模式：<ul><li>AP 模式：<ul><li>当运行在 AP 模式下，Nacos 更偏向于高可用性和分区容错性。这在大多数服务发现场景是可取的，因为在这种情况下，即使在网络分区或部分系统故障的情况下，服务仍然需要被发现和消费。</li><li>在 AP 模式下，Nacos 可能会牺牲一致性来保证高可用性。也就是说，服务的注册和发现可能不会立即反映最新状态，但服务的发现和路由功能仍然是可用的。</li><li>作为服务注册中心：<ul><li>当Nacos用作服务注册中心时，它通常运行在AP模式。这是因为在服务发现的场景中，可用性和分区容错性通常被视为更重要。即使在网络分区或部分服务不可用的情况下，服务注册和发现功能仍然可以正常工作，尽管这可能会导致短暂的数据不一致。</li></ul></li></ul></li><li>CP 模式：<ul><li>当运行在 CP 模式下，Nacos 更注重一致性和分区容错性。这适用于配置管理场景，因为配置信息的正确性和一致性是非常重要的。</li><li>在 CP 模式下，Nacos 会确保配置的一致性，即使这可能意味着在网络分区或其他问题发生时，对配置的访问可能会受到影响。</li><li>作为配置中心：<ul><li>当Nacos用作配置中心时，它通常运行在CP模式。在配置管理场景下，一致性是非常关键的，因为配置信息需要在所有服务实例之间保持一致。Nacos 确保即使在网络分区或故障的情况下，所有服务实例也能获取到一致的配置信息。</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud Alibaba </tag>
            
            <tag> Nacos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud-Ribbon</title>
      <link href="/2024/04/23/springcloud-ribbon/"/>
      <url>/2024/04/23/springcloud-ribbon/</url>
      
        <content type="html"><![CDATA[<p>Ribbon 是一个客户端负载均衡器，它可以在调用微服务时提供<strong>负载均衡</strong>的功能。在Spring Cloud 中，Ribbon 通常与 Eureka、Nacos 等其他服务发现组件结合使用，以实现在客户端进行服务的动态查找和负载均衡。Ribbon <strong>主要用于控制 HTTP 和 TCP 客户端的行为</strong>。</p><h2 id="核心特性"><a href="#核心特性" class="headerlink" title="核心特性"></a>核心特性</h2><ol><li><strong>服务发现集成</strong>：当 Ribbon 客户端启动时，它会从服务组件（如Eureka、Nacos等）获取可用的服务实例列表，并对这些实例进行负载均衡策略。</li><li><strong>负载均衡策略</strong>：Ribbon 内置了多种负载均衡策略，如<strong>轮询</strong>、<strong>随机</strong>、<strong>权重</strong>等（默认是轮询）。用户可以选择合适的策略，也可以自定义策略。</li><li><strong>容错机制</strong>：Ribbon 提供了失败重试机制，可以在调用失败时自动重试其他实例。这增加了调用的健壮性。</li><li><strong>客户端缓存和批处理</strong>：Ribbon 可以缓存客户端请求，支持请求的批量发送，优化网络使用等。</li><li><strong>可配置性</strong>：Ribbon 允许开发者通过配置文件定制和调整其行为，包括超时设置、重试策略、连接池大小等。</li></ol><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li><strong>服务发现</strong>：首先从服务注册中心获取可用的服务实例列表。</li><li><strong>选择服务实例</strong>：根据配置的负载均衡策略，从可用服务实例中选择一个。</li><li><strong>服务调用</strong>：对选定的服务实例进行网络请求。</li><li><strong>错误处理和重试</strong>：在发生错误时，根据配置的策略进行重试或失败回退。</li></ol><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ol><li><strong>微服务间的调用</strong>：在微服务架构中，服务实例可能会动态地上下线，Ribbon可以帮助客户端自动发现可用的服务实例，并进行负载均衡。</li><li><strong>消除单点故障</strong>：通过Ribbon的负载均衡，请求可以被分散到多个服务实例，从而消除单点故障。</li><li><strong>通过系统吞吐量</strong>：通过将请求均匀分配到多个服务实例，可以提高系统的整体吞吐量。</li></ol><h3 id="故障处理：使用-Ribbon-的失败重试和回退机制"><a href="#故障处理：使用-Ribbon-的失败重试和回退机制" class="headerlink" title="故障处理：使用 Ribbon 的失败重试和回退机制"></a>故障处理：使用 Ribbon 的失败重试和回退机制</h3><p><strong>失败重试</strong>：</p><p>Ribbon 允许配置自动重试机制，这意味着当服务调用失败时，Ribbon 可以自动重新发送请求到同一个或不同的服务实例。这在临时网络问题或服务瞬时故障时特别有用。</p><p><strong>配置示例</strong>：</p><p>在<code>application.properties</code>文件中，可以设置重试次数和条件:</p><pre><code>ribbon:  MaxAutoRetries=1                 # 同一个服务实例的最大重试次数  MaxAutoRetriesNextServer=2       # 尝试另一个服务实例的最大重试次数  OkToRetryOnAllOperations=true    # 允许对所有请求操作进行重试</code></pre><p><strong>回退策略</strong>：</p><p>回退策略允许在调用失败时提供一个默认的响应，这通常通过集成 Hystrix 来实现。Hystrix 提供了断路器功能，当服务不可用时可以自动切换到预定义的回退逻辑。</p><p><strong>实现示例</strong>：</p><p>使用Hystrix 在 Ribbon 调用中添加回退逻辑，例如在使用 <code>RestTemplate</code>进行服务调用时：</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class MyService &#123;    @Autowired    private RestTemplate restTemplate;    @HystrixCommand(fallbackMethod = "reliable")    public String getData(String serviceId) &#123;        return restTemplate.getForObject("http://" + serviceId + "/data", String.class);    &#125;    public String reliable(String serviceId) &#123;        return "Default Data";    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="性能优化：调整-Ribbon-的性能设置"><a href="#性能优化：调整-Ribbon-的性能设置" class="headerlink" title="性能优化：调整 Ribbon 的性能设置"></a>性能优化：调整 Ribbon 的性能设置</h3><p>在使用 Ribbon 进行服务调用时，适当的性能优化可以提高响应速度和系统吞吐量。</p><p><strong>客户端超时设置</strong></p><p>调整请求超时设置可以防止服务调用过长时间等待，影响用户体验和资源使用效率。</p><pre><code>ribbon:  ReadTimeout=5000     # 读取超时时间（毫秒）  ConnectTimeout=1000  # 连接超时时间（毫秒）</code></pre><p><strong>请求批处理</strong></p><p>如果 Ribbon 用于大量的请求调用，可以采用请求批处理策略，合并短时间内的多个请求，减少网络往返次数，提高效率。</p><p><strong>连接池配置</strong></p><p>配置连接池可以重用 HTTP 连接，减少频繁建立连接的开销。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Beanpublic ClientHttpRequestFactory clientHttpRequestFactory() &#123;    HttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory();    factory.setReadTimeout(5000);    factory.setConnectTimeout(1000);    factory.setHttpClient(httpClient()); // 自定义配置 HttpClient    return factory;&#125;private HttpClient httpClient() &#123;    PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager();    connectionManager.setMaxTotal(200);    // 最大连接数    connectionManager.setDefaultMaxPerRoute(50); // 每个路由的默认最大连接数    return HttpClientBuilder.create().setConnectionManager(connectionManager).build();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
            <tag> Ribbon </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud Alibaba-Sentinel</title>
      <link href="/2024/04/23/springcloud-alibaba-sentinel/"/>
      <url>/2024/04/23/springcloud-alibaba-sentinel/</url>
      
        <content type="html"><![CDATA[<p>Sentinel 是一个开源的流量控制组件，由阿里巴巴开源。它主要用于微服务的<strong>稳定性</strong>和<strong>可靠性</strong>保障，提供了丰富的功能来应对各种服务和应用的高可用需求。Sentinel 特别适合用来处理微服务架构中的<strong>流量控制</strong>、<strong>熔断降级</strong>、<strong>系统自适应保护</strong>等问题。</p><h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><ol><li>流量控制：<ul><li>Sentinel 可以通过定义流量控制规则来限制访问频率，保护系统不被高流量压垮。这些规则可以基于QPS（每秒查询率）或并发线程数设置。</li></ul></li><li>熔断降级：<ul><li>在服务不稳定（如响应时间过长或异常率过高）时会自动进行熔断，快速返回错误响应或者调用回退逻辑，防止服务雪崩效应。</li></ul></li><li>系统自适应保护：<ul><li>根据系统的负载情况（如CPU使用率、总体平均响应时间等）自动调整流量控制规则，确保系统在安全的负载下运行。</li></ul></li><li>热点参数限流：<ul><li>支持对经常访问的“热点”数据进行流量控制，比如对热门商品的请求过于频繁时自动进行限流。</li></ul></li><li>实时监控和动态规则配置：<ul><li>Sentinel 提供了实时监控功能，可以实时观察系统的运行状态和规则的执行效果（流量、熔断、响应时间等信息）。同时，支持通过控制台动态修改流量控制规则，无需重启应用。</li></ul></li><li>规则的优先级：<ul><li>Sentinel 支持多种规则，如流量控制规则、降级规则、系统保护规则等。这些规则有不同的优先级，当多个规则同时触发时，高优先级的规则会先被执行。</li></ul></li></ol><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ol><li><strong>微服务流量控制</strong>：在微服务架构中，可以使用Sentinel对服务间的调用进行流量控制。</li><li><strong>API网关流量控制</strong>：在API网关中，可以使用Sentinel对外部的请求进行流量控制。</li><li><strong>分布式系统熔断降级</strong>：在分布式系统中，可以使用Sentinel对故障的服务进行熔断降级。</li><li><strong>系统负载保护</strong>：当系统的负载过高时，可以使用Sentinel进行流量控制，保护系统。</li></ol><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ol><li><strong>流量控制</strong>：保护系统免受恶意攻击和流量峰值。</li><li><strong>熔断控制</strong>：在服务出现问题时，自动进行熔断降级，保护系统。</li><li><strong>系统负载保护</strong>：在系统负载过高时，自动进行流量控制，保护系统。</li><li><strong>实时监控</strong>：提供实时的监控和统计功能，帮助运维人员了解系统的状态。</li></ol><h3 id="底层原理"><a href="#底层原理" class="headerlink" title="底层原理"></a>底层原理</h3><ul><li>Sentinel 使用<code>滑动窗口算法</code>来统计请求的数量和响应时间。这使得 Sentinel 可以实时地获取系统的运行状态，并根据这些状态做出限流、降级和熔断的决策。</li><li>Sentinel 的核心是一系列的处理器和规则。处理器负载统计数据和执行控制逻辑，而规则定义了何时触发控制逻辑。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud Alibaba </tag>
            
            <tag> Sentinel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud Gateway</title>
      <link href="/2024/04/22/spring-cloud-gateway-fu-xi/"/>
      <url>/2024/04/22/spring-cloud-gateway-fu-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Spring-Cloud-Gateway-复习"><a href="#Spring-Cloud-Gateway-复习" class="headerlink" title="Spring Cloud Gateway 复习"></a>Spring Cloud Gateway 复习</h2><p>Gateway是Spring Cloud的一个组件，用于构建API网关。API网关是微服务架构中的一个关键组件，它负责<strong>路由请求转发</strong>、<strong>负载均衡</strong>、<strong>断路器</strong>、<strong>安全</strong>、<strong>跨域</strong>、<strong>请求头</strong>和<strong>响应头的修改</strong>等。</p><h3 id="三大特性"><a href="#三大特性" class="headerlink" title="三大特性"></a>三大特性</h3><ul><li><strong>Routes</strong>：路由</li><li><strong>Filters</strong>：过滤器</li><li><strong>Predicates</strong>：断言</li></ul><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ul><li><strong>路由</strong>：客户端向 Spring Cloud Gateway 发出请求后，Gateway Handler Mapping 查找与请求相匹配的路由。</li><li>过滤器：在请求达到实际服务之前，Gateway 可以使用过滤器来修改传入的 HTTP 请求；同理，返回的 HTTP 响应也可以被过滤器处理。<ul><li>有两种类型的过滤器，<code>pre 过滤器</code>和<code>post 过滤器</code>。pre 过滤器在路由请求之前执行，post 过滤器在路由请求之后执行。</li></ul></li><li><strong>转发</strong>：最后请求将被转发到实际的服务。</li></ul><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ul><li><strong>路由和负载均衡</strong>：将请求路由到合适的微服务实例。</li><li><strong>安全</strong>：如身份验证和授权。</li><li><strong>限流</strong>：限制请求的速率。</li><li><strong>缓存</strong>：缓存请求的响应。</li><li><strong>断路</strong>：在某个微服务实例出现问题时，快速失败。</li><li><strong>跨域</strong>：处理跨域资源共享（CORS）请求。</li><li><strong>请求和响应的修改</strong>：如添加、删除或修改头信息。</li></ul><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ul><li><strong>统一入口</strong>：为微服务提供一个统一的访问入口，简化客户端调用。</li><li><strong>安全防护</strong>：提供安全机制，如身份验证、授权和防止恶意攻击。</li><li><strong>性能优化</strong>：如负载均衡、缓存和限流。</li><li><strong>故障隔离</strong>：如断路和重试。</li></ul><h3 id="定义路由规则"><a href="#定义路由规则" class="headerlink" title="定义路由规则"></a>定义路由规则</h3><p>Gateway 使用一组路由规则来确定如何处理传入的 HTTP 请求。每条路由规则定义了一个<strong>目标</strong> URI 、一组<strong>断言</strong>和一组<strong>过滤器</strong>。当断言为真时，请求会被路由到目标 URI ，并在路由之前和之后应用过滤器。</p><p>在 application.yml 中：</p><pre><code>spring:  cloud:    gateway:      routes:      ## 路由规则的唯一标识符      - id: user-service        uri: lb://user-service        predicates:        - Path=/user/**        filters:        - AddRequestHeader=X-Request-Foo, Bar</code></pre><p>在 Spring Cloud Gateway 的路由定义中：</p><ul><li><p><strong>id</strong>：这是路由的唯一标识符。</p><p>  它用于区分和标识不同的路由规则。在管理、监控或日志中，这个<code>id</code>可以帮助我们快速识别和引用特定的路由。</p></li><li><p>uri：这是路由的目标URI。</p><p>  当请求满足某个路由的断言时，它会被转发到这个URI</p><p>  这个URI可以是一个具体的地址，也可以是一个服务的逻辑名称。</p><ul><li>当URI的前缀是 <code>lb://&#123;&#123;微服务名称&#125;&#125;</code> 时，表示这是一个逻辑地址，需要使用客户端负载均衡器（如 Ribbon）来解析。在这个例子中，<code>lb://user-service</code> 表示请求会被负载均衡到名为 <code>user-service</code> 的服务实例。</li><li>如果 URI 是一个 HTTP 或 HTTPS 地址，lb:&#x2F;&#x2F;<a href="https://www.baidu.com,那么请求会被直接转发到这个地/">https://www.baidu.com，那么请求会被直接转发到这个地</a></li></ul></li><li><p>predicates 路由谓词：</p><ul><li>谓词用于匹配和过滤HTTP请求。</li><li>SpringCloud Gateway 提供了多种内置谓词，如：<ul><li><p>Path</p><p>  ：根据请求路径匹配。</p><ul><li><strong>Method</strong>：根据HTTP方法匹配。</li><li><strong>Header</strong>：根据请求头匹配。</li><li><strong>Query</strong>：根据查询参数匹配。</li></ul></li></ul></li></ul></li><li><p>filters 过滤器：</p><ul><li>Gateway 提供了多种过滤器，这些过滤器分为全局过滤器和路由过滤器。全局过滤器对所有请求都有效，而路由过滤器只对特定的路由有效。</li><li>过滤器的执行顺序是由其 order 属性决定的。请求首先会经过所有的 pre 类型的过滤器，然后路由到下游服务，最后再经过所有的 post 类型的过滤器。</li></ul></li></ul><p>上述配置定义了一个路由规则，当请求的路径匹配 <code>/user/**</code> 断言时，它会被路由到名为 <code>user-service</code> 的微服务，并在请求头中添加一个名为 X-Request-Foo 的头信息。</p><h3 id="定义过滤器"><a href="#定义过滤器" class="headerlink" title="定义过滤器"></a>定义过滤器</h3><pre class="line-numbers language-language-java"><code class="language-language-java">@Beanpublic RouteLocator customRouteLocator(RouteLocatorBuilder builder) &#123;    return builder.routes()        .route("path_route", r -> r.path("/get")            .filters(f -> f.addRequestHeader("Hello", "World"))            .uri("<http://httpbin.org>"))        .build();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述代码定义了一个路由规则，当请求的路径为 &#x2F;get 时，它会被路由到 <a href="http://httpbin.org/get">http://httpbin.org/get</a> ，并在请求头中添加一个名为 Hello 的头信息。</p><h3 id="SpringCloud-Gateway的应用"><a href="#SpringCloud-Gateway的应用" class="headerlink" title="SpringCloud Gateway的应用"></a>SpringCloud Gateway的应用</h3><h3 id="1-动态路由"><a href="#1-动态路由" class="headerlink" title="1. 动态路由"></a>1. 动态路由</h3><ul><li>Gateway 支持动态路由，允许您在运行时添加、修改或删除路由规则。</li><li>这是通过与Spring Cloud Config 或其他外部配置源的集成来实现的。</li><li>当路由配置发生变化时，Gateway 可以自动刷新路由规则，而无需重新启动应用。</li></ul><h3 id="2-API聚合"><a href="#2-API聚合" class="headerlink" title="2. API聚合"></a>2. API聚合</h3><p>网关可以用来聚合多个后端服务的API调用结果。例如，客户端可以只发送一个请求到网关，网关则分别向多个服务发请求，然后将结果聚合后返回给客户端。这可以减少客户端与服务间的通信次数，并简化客户端逻辑。</p><h3 id="3-全局过滤器"><a href="#3-全局过滤器" class="headerlink" title="3. 全局过滤器"></a>3. 全局过滤器</h3><p>除了标准的路由过滤器外，Spring Cloud Gateway 还提供全局过滤器，它对所有的路由有效。这些全局过滤器可以用来实现跨服务的逻辑，如权限校验、日志跟踪记录、安全检查等。</p><h3 id="4-断路器集成"><a href="#4-断路器集成" class="headerlink" title="4. 断路器集成"></a>4. 断路器集成</h3><ul><li>Spring Cloud GateWay 支持与 Hystrix 断路器的集成。当某个下游服务出现问题时，断路器可以防止请求继续传递给服务，从而防止系统雪崩。</li><li>通过配置，你可以为特定的路由定义断路器的行为，例如失败的阈值和回退的响应。</li></ul><h3 id="5-重试机制"><a href="#5-重试机制" class="headerlink" title="5. 重试机制"></a>5. 重试机制</h3><ul><li>如果某个微服务实例失败，Gateway 可以配置为自动重试其他实例。</li><li>重试可以基于不同的策略，如固定延迟、指数退避等。</li><li>这增加了系统的弹性和可用性。</li></ul><h3 id="6-请求速率限制"><a href="#6-请求速率限制" class="headerlink" title="6. 请求速率限制"></a>6. 请求速率限制</h3><p>通过与Redis等技术的集成，Spring Cloud Gateway 支持对客户端请求进行速率限制，这有助于防止API滥用并保护后端服务免受过载。</p><ul><li>例如，你可以为特定的用户或IP设置每秒的请求限制。</li><li>限流可以帮助您保护系统免受恶意攻击和意外的流量峰值。</li></ul><h3 id="7-集成与第三方服务发现"><a href="#7-集成与第三方服务发现" class="headerlink" title="7. 集成与第三方服务发现"></a>7. 集成与第三方服务发现</h3><ul><li>Gateway 可以与Eureka、Consul、Zookeeper等服务发现组件集成。</li><li>当使用 <code>lb://&#123;&#123;微服务名称&#125;&#125;</code> 前缀的逻辑地址时，Gateway 会从服务发现组件中自动获取可用的服务实例，并进行负载均衡。</li></ul><h2 id="关于底层的负载均衡器"><a href="#关于底层的负载均衡器" class="headerlink" title="关于底层的负载均衡器"></a>关于底层的负载均衡器</h2><p>在Spring Cloud的较早版本中，<strong>Ribbon</strong>是默认的客户端负载均衡器。但从Spring Cloud Greenwich版本开始，官方推荐使用<strong>Spring Cloud LoadBalancer</strong>，这是一个基于Spring Reactor的新的轻量级、反应式负载均衡器。</p><p>Spring Cloud LoadBalancer的特点：</p><ul><li>它是一个反应式的负载均衡器，并且完全集成在WebFlux环境中。</li><li>提供了一个简单的轮询和随机负载均衡策略的默认实现，也支持自定义策略。</li><li>它使用了Spring Boot 2.x中引入的HttpClient或WebClient来进行实际的服务调用。</li></ul><p>虽然Ribbon项目现在处于维护模式，不再建议用于新的项目，但现有项目可以继续使用Ribbon而无需迁移到Spring Cloud LoadBalancer。</p><p>在实际的Spring Cloud项目中，你应该确保使用的是兼容的Spring Cloud版本，并且对应地选择使用Ribbon还是Spring Cloud LoadBalancer。如果你的Spring Cloud版本较新（比如Hoxton版本及以后），应该使用Spring Cloud LoadBalancer作为客户端负载均衡器。</p><p>总的来说，SpringCloud Gateway 是一个功能强大的API网关，它提供了一系列的工具和特性，使得开发者可以轻松地构建和管理微服务应用的API网关。</p>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
            <tag> Gateway </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud-OpenFeign</title>
      <link href="/2024/04/22/springcloud-openfeign/"/>
      <url>/2024/04/22/springcloud-openfeign/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>OpenFeign 是一个声明式客户端（声明式客户端：专注于要调用服务的接口，而不是服务调用的底层细节），允许开发者定义一个接口，然后通过注解指定对应的微服务名称，当这个接口被调用时，就会自动构建一个 HTTP 请求并发送，并将服务的响应进行解码。</p><h2 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h2><ol><li><strong>简化 HTTP 客户端</strong>：使用 OpenFeign，开发者无需手动创建 HTTP 请求，只需定义接口和注解绑定服务。</li><li><strong>统一异常处理</strong>：OpenFeign 提供了统一的异常处理机制，使得服务调用失败时可以统一处理。</li><li><strong>集成负载均衡</strong>：与 Ribbon 结合，OpenFeign 可以实现客户端的负载均衡。</li><li><strong>支持多种 HTTP 客户端</strong>：Feign 默认使用 HttpURLConnection 进行 HTTP 调用，也可以配置为使用 Apache HttpClient、OkHttp 等其他库。</li></ol><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ol><li><strong>微服务调用解耦</strong>：在微服务架构中，服务间需要频繁地进行通信，OpenFeign 提供了一个简洁的方式来调用其他服务的 API，只需要像调用本地方法一样调用Feign 客户端接口，Feign 就会处理服务发现、负载均衡以及远程调用。</li><li><strong>服务封装</strong>：可以将所有与特定微服务通信的逻辑封装在一个 Feign 客户端接口中，便于管理和复用。</li><li><strong>错误处理和重试机制</strong>：Feign 可以配置重试机制以对应临时的网络波动或服务不稳定。</li></ol><h3 id="核心组件和工作流程"><a href="#核心组件和工作流程" class="headerlink" title="核心组件和工作流程"></a>核心组件和工作流程</h3><ol><li>接口定义：<ul><li>用户定义一个接口，并使用 Feign 提供的注解（如<code>@FeignClient</code>，<code>GetMapping</code>，<code>@PostMapping</code>等）标注接口方法。这些注解定义了如何将接口方法调用转换为 HTTP 请求。</li></ul></li><li>动态代理：<ul><li>当应用启动时，Feign 使用 Java 的动态代理技术为标注了 <code>@FeignClient</code>的接口生成代理对象。这些代理对象负责将调用转换为 HTTP 请求。</li></ul></li><li>请求构造：<ul><li>Feign 根据接口方法上的注解信息（如 URL、HTTP 方法和参数等）构造HTTP请求。这包括解析和替换URI中的变量，序列化请求体，添加必要的请求头等。</li></ul></li><li>发送请求：<ul><li>请求构造完毕后，Feign 使用配置的 HTTP 客户端（如默认的 HttpURLConnection, Apache HttpClient 或 OkHttp）发送请求到服务端。如果配置了客户端负载均衡（如 Ribbon），Feign 会查询服务发现组件（如Nacos）获取服务实例的列表，并根据负载均衡策略选择一个实例发送请求。</li></ul></li><li>响应处理：<ul><li>服务端响应后，Feign 根据配置的解码器（Decoder）处理响应数据。这通常涉及将响应体反序列化为 Java 对象。如果响应表示错误，Feign 可以使用 ErrorDecoder 来解码错误信息，并可能抛出异常。</li></ul></li><li>异常处理和回退：<ul><li>如果在请求过程中出现错误，如网络问题或服务端错误，Feign 可以配置回退逻辑（Fallback）来处理这些情况。</li></ul></li></ol><h3 id="x-Beanpublic-ClientHttpRequestFactory-clientHttpRequestFactory-HttpComponentsClientHttpRequestFactory-factory-new-HttpComponentsClientHttpRequestFactory-factory-setReadTimeout-5000-factory-setConnectTimeout-1000-factory-setHttpClient-httpClient-自定义配置-HttpClient-return-factory-​private-HttpClient-httpClient-PoolingHttpClientConnectionManager-connectionManager-new-PoolingHttpClientConnectionManager-connectionManager-setMaxTotal-200-最大连接数-connectionManager-setDefaultMaxPerRoute-50-每个路由的默认最大连接数-return-HttpClientBuilder-create-setConnectionManager-connectionManager-build-java"><a href="#x-Beanpublic-ClientHttpRequestFactory-clientHttpRequestFactory-HttpComponentsClientHttpRequestFactory-factory-new-HttpComponentsClientHttpRequestFactory-factory-setReadTimeout-5000-factory-setConnectTimeout-1000-factory-setHttpClient-httpClient-自定义配置-HttpClient-return-factory-​private-HttpClient-httpClient-PoolingHttpClientConnectionManager-connectionManager-new-PoolingHttpClientConnectionManager-connectionManager-setMaxTotal-200-最大连接数-connectionManager-setDefaultMaxPerRoute-50-每个路由的默认最大连接数-return-HttpClientBuilder-create-setConnectionManager-connectionManager-build-java" class="headerlink" title="x @Beanpublic ClientHttpRequestFactory clientHttpRequestFactory() {    HttpComponentsClientHttpRequestFactory factory &#x3D; new HttpComponentsClientHttpRequestFactory();    factory.setReadTimeout(5000);    factory.setConnectTimeout(1000);    factory.setHttpClient(httpClient()); &#x2F;&#x2F; 自定义配置 HttpClient    return factory;}​private HttpClient httpClient() {    PoolingHttpClientConnectionManager connectionManager &#x3D; new PoolingHttpClientConnectionManager();    connectionManager.setMaxTotal(200);    &#x2F;&#x2F; 最大连接数    connectionManager.setDefaultMaxPerRoute(50); &#x2F;&#x2F; 每个路由的默认最大连接数    return HttpClientBuilder.create().setConnectionManager(connectionManager).build();}java"></a>x @Beanpublic ClientHttpRequestFactory clientHttpRequestFactory() {    HttpComponentsClientHttpRequestFactory factory &#x3D; new HttpComponentsClientHttpRequestFactory();    factory.setReadTimeout(5000);    factory.setConnectTimeout(1000);    factory.setHttpClient(httpClient()); &#x2F;&#x2F; 自定义配置 HttpClient    return factory;}​private HttpClient httpClient() {    PoolingHttpClientConnectionManager connectionManager &#x3D; new PoolingHttpClientConnectionManager();    connectionManager.setMaxTotal(200);    &#x2F;&#x2F; 最大连接数    connectionManager.setDefaultMaxPerRoute(50); &#x2F;&#x2F; 每个路由的默认最大连接数    return HttpClientBuilder.create().setConnectionManager(connectionManager).build();}java</h3><p>OpenFeign 使用了 <code>Ribbon</code> 和 <code>Hystrix</code> 实现服务间通信。</p><ol><li>OpenFeign 客户端通过 Ribbon 确定具体的微服务实例。</li><li>确定微服务实例后，OpenFeign 客户端根据定义的服务接口和方法（包括URL、HTTP方法、请求参数等）构造一个HTTP请求，并通过HTTP客户端将这个请求发送到指定的微服务实例上。</li><li>当请求到达指定服务器进行处理之后并返回响应，Feign客户端会处理这个响应，并将结果反序列化（或转换）成接口方法的返回类型。</li></ol><p>在这个过程中，Ribbon 主要负责<code>负载均衡</code>和<code>服务发现</code>，而 Hystrix 则用于<code>短路</code>、<code>降级</code>等容错功能。Hystrix 可以提供强大的服务保护和监控能力，防止单个节点故障导致整体系统不可用。</p>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
            <tag> OpenFeign </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring SpringBoot 复习</title>
      <link href="/2024/04/10/spring-springboot-fu-xi/"/>
      <url>/2024/04/10/spring-springboot-fu-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring、SpringBoot"><a href="#Spring、SpringBoot" class="headerlink" title="Spring、SpringBoot"></a>Spring、SpringBoot</h1><h2 id="IOC（Inversion-of-Control）"><a href="#IOC（Inversion-of-Control）" class="headerlink" title="IOC（Inversion of Control）"></a>IOC（Inversion of Control）</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>IoC 即<strong>控制反转</strong>，是一种设计原则，它将应用程序的控制权从程序代码本身转移到外部容器或框架中。传统的程序设计中，程序内部直接控制程序流程和对象的创建与销毁，而在IoC中，对象的创建和管理由容器来完成，应用程序只需要描述组件之间的依赖关系，而不需要负责对象的创建和销毁。</p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>主要作用在于降低了组件之间的耦合度，使得应用程序更加灵活、可扩展和易于维护。通过将对象的创建和管理交给容器，可以更容易地替换、扩展和重用组件，同时也能更好地实现面向接口编程。</p><h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><p>IoC的实现原理主要通过<strong>依赖注入</strong>（Dependency Injection）来实现。</p><p>依赖注入是IoC的一种具体实现方式，它通过容器来动态地将组件之间的依赖关系注入到组件中，从而实现控制反转。</p><p>依赖注入有三种主要的方式：</p><p><strong>构造器注入</strong>(需要结合@Configuration来使用)</p><ul><li>通过构造函数来注入依赖对象。</li><li><strong>优势</strong>：明确表明了类的依赖关系，使得类的依赖关系更加明确和可见。</li><li><strong>劣势</strong>：当类有多个依赖关系时，构造函数的参数列表可能变得很长，增加了代码的复杂性。</li></ul><pre class="line-numbers language-language-java"><code class="language-language-java"> private final UserRepository userRepository;    // 构造器注入    public UserService(UserRepository userRepository) &#123;        this.userRepository = userRepository;    &#125;    ...    @Configuration    public class AppConfig &#123;    @Bean    public UserRepository userRepository() &#123;        return new UserRepository();    &#125;    @Bean    public UserService userService(UserRepository userRepository) &#123;        return new UserService(userRepository);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Setter注入（需要结合**@Autowired**来使用）</p><ul><li>通过Setter方法来注入依赖对象。</li><li>优劣势：同上</li></ul><pre class="line-numbers language-language-java"><code class="language-language-java">    private UserRepository userRepository;    // Setter注入    public void setUserRepository(UserRepository userRepository) &#123;        this.userRepository = userRepository;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字段注入</p><ul><li>通过字段直接注入依赖对象。</li><li><strong>优势</strong>：简洁明了，不需要额外的构造函数或Setter方法。</li><li><strong>劣势</strong>：对象的依赖关系被直接暴露在类的字段中，降低了类的封装性。</li></ul><pre class="line-numbers language-language-java"><code class="language-language-java">    @Autowired    private UserRepository userRepository;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="AOP（Aspect-Oriented-Programming）"><a href="#AOP（Aspect-Oriented-Programming）" class="headerlink" title="AOP（Aspect-Oriented Programming）"></a>AOP（Aspect-Oriented Programming）</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>AOP 即<strong>面向横面编程</strong>，是一种编程范式，它允许你将那些与核心业务逻辑无关，但又分散在代码各处的功能（比如日志记录、事务管理、安全检查）“横切”出来，集中管理。这样做的目的是提高代码的重用性、可维护性和可扩展性。</p><p>在Spring中，@Aspect 注解标记一个类后，Spring 会将其视为<strong>切面类</strong>，并在运行时自动为该类创建代理类对象，并将切面逻辑织入到代理对象中。就可以实现手动创建代理对象相似的功能，但更加简介和方便。</p><p>通过使用注解 @Aspect ，Spring 提供了一种声明式的方式来定义切面，并在AOP中应用它们，而无需手动编写代理逻辑和切面逻辑的代码。</p><ul><li><strong>@Before</strong>：在目标方法执行前执行切面逻辑。</li><li><strong>@After</strong>：在目标方法执行后（无论是否发生异常）执行切面逻辑。</li><li>@AfterReturing：在目标方法正常返回后执行切面逻辑。</li><li>@AfterThrowing：在目标方法抛出异常后执行切面逻辑。</li><li>@Around：在目标方法执行前后，控制目标方法的执行过程，可以自定义是否执行目标方法、执行前后的额外逻辑等。</li></ul><h3 id="实现原理-1"><a href="#实现原理-1" class="headerlink" title="实现原理"></a>实现原理</h3><p>在Spring中，AOP主要通过动态代理实现，具体而言有两种主要的实现方式：JDK动态代理和CGLIB动态代理。</p><ol><li><strong>JDK 动态代理：</strong><ul><li>JDK 动态代理是基于接口（UserService）的代理，它要求目标类（UserServiceImpl）必须实现一个接口。</li><li>当目标类实现了接口时，Spring 容器会自动使用 JDK 动态代理生成一个实现了该接口（UserService）的代理类，并在代理类中织入切面逻辑。</li></ul></li><li>CGLIB 动态代理：<ul><li>CGLIB 动态代理是基于继承的代理，它不要求目标必须实现接口。</li><li>当目标类没有实现接口时，Spring 容器会自动使用CGLIB动态代理生成一个目标类的子类，并在子类中织入切面逻辑。</li><li>CGLIB 动态代理通过字节码增强技术来实现。</li></ul></li></ol><h3 id="Spring-在什么情况下会选择-JDK-或-CGLIB？这个选择策略是一成不变的吗？"><a href="#Spring-在什么情况下会选择-JDK-或-CGLIB？这个选择策略是一成不变的吗？" class="headerlink" title="Spring 在什么情况下会选择 JDK 或 CGLIB？这个选择策略是一成不变的吗？"></a>Spring 在什么情况下会选择 JDK 或 CGLIB？这个选择策略是一成不变的吗？</h3><ul><li><strong>对于纯 Spring Framework</strong>：Spring 框架会进行“智能”检测。<ul><li>如果一个 Bean <strong>实现了接口</strong>，Spring AOP <strong>优先使用 JDK 动态代理</strong>。</li><li>如果一个 Bean <strong>没有实现任何接口</strong>，Spring AOP 别无选择，<strong>只能使用 CGLIB</strong>。</li></ul></li><li><strong>对于 Spring Boot (2.0 及以后版本)</strong>：这是一个关键区别！<ul><li><p><strong>默认使用 CGLIB</strong>。无论你的目标类是否实现了接口，Spring Boot 都默认创建基于 CGLIB 的子类代理。</p></li><li><p><strong>为什么这么做？</strong> 主要是为解决一个经典问题——<strong>“同类方法调用（Self-Invocation）”时的代理失效问题</strong>。</p></li><li><p><strong>如何修改？</strong> 如果你希望 Spring Boot 使用 JDK 动态代理，可以在 <code>application.properties</code> 或 <code>application.yml</code> 中进行配置：</p><pre class="line-numbers language-language-yaml"><code class="language-language-yaml">spring.aop.proxy-target-class=false<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  设置为 <code>false</code> 后，Spring Boot 就会退回到和纯 Spring 框架一样的策略：有接口用 JDK，没接口用 CGLIB。</p></li></ul></li></ul><h3 id="这两种代理方式各有什么优缺点或者说限制？"><a href="#这两种代理方式各有什么优缺点或者说限制？" class="headerlink" title="这两种代理方式各有什么优缺点或者说限制？"></a>这两种代理方式各有什么优缺点或者说限制？</h3><p><strong>JDK 动态代理</strong></p><ul><li><strong>优点</strong>：<ul><li>它是 Java 官方提供的，无需引入任何第三方库。</li><li>在某些情况下，它的代理对象创建速度可能比 CGLIB 稍快。</li></ul></li><li><strong>缺点&#x2F;限制</strong>：<ul><li><strong>必须实现接口</strong>：这是它最大的限制。它只能为实现了接口的类创建代理。</li><li><strong>代理的是接口，不是类</strong>：生成的代理对象只能强制类型转换为接口类型，而不能是实现类类型。</li></ul></li></ul><p><strong>CGLIB 动态代理 (Code Generation Library)</strong></p><ul><li><strong>优点</strong>：<ul><li><strong>无需实现接口</strong>：可以为任意一个普通类（非 <code>final</code> 类）创建代理。</li><li><strong>解决了“同类方法调用”问题</strong>：由于它代理的是类本身，因此注入的代理对象就是目标类的子类型，可以避免代理失效。</li></ul></li><li><strong>缺点&#x2F;限制</strong>：<ul><li><strong>无法代理 <code>final</code> 类和 <code>final</code> 方法</strong>：因为 CGLIB 的原理是生成目标类的子类，而 <code>final</code> 关键字阻止了继承和重写。如果你的类或方法被声明为 <code>final</code>，AOP 将无法生效。</li><li><strong>性能开销</strong>：在早期版本中，CGLIB 因为涉及字节码操作，其代理对象的创建和方法执行的开销比 JDK 代理稍高。不过随着版本迭代和 JVM 的优化，这种性能差异已经非常微小了。</li></ul></li></ul><h3 id="什么是“同类方法调用（Self-Invocation）”代理失效问题？"><a href="#什么是“同类方法调用（Self-Invocation）”代理失效问题？" class="headerlink" title="什么是“同类方法调用（Self-Invocation）”代理失效问题？"></a>什么是“同类方法调用（Self-Invocation）”代理失效问题？</h3><p>这是 AOP 领域最经典的一个“坑”，也是 CGLIB 被 Spring Boot 设为默认方式的重要原因。</p><p><strong>场景</strong>：假设你有一个 <code>UserServiceImpl</code> 类，它被 Spring AOP 代理了。类里面有两个方法 <code>methodA()</code> 和 <code>methodB()</code>，并且 <code>methodA()</code> 上加了事务注解 <code>@Transactional</code>。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class UserServiceImpl implements UserService &#123;    @Transactional    public void methodA() &#123;        System.out.println("Executing method A...");        // ... 业务逻辑 ...    &#125;    public void methodB() &#123;        System.out.println("Executing method B...");        this.methodA(); // 问题在这里！    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>问题</strong>：当外部代码调用 <code>userService.methodB()</code> 时，<code>methodB()</code> 本身没有事务。在 <code>methodB()</code> 内部，通过 <code>this.methodA()</code> 调用 <code>methodA()</code>，<strong>此时 <code>methodA</code> 的事务会生效吗？</strong></p><p><strong>答案</strong>：<strong>不会</strong>。</p><p><strong>原因</strong>：</p><ul><li>外部调用 <code>userService.methodB()</code> 时，调用的是 Spring 创建的<strong>代理对象</strong>。</li><li>但是，一旦进入了 <code>methodB()</code> 的方法体内部，<code>this</code> 关键字指向的是<strong>原始的目标对象（UserServiceImpl 实例）</strong>，而不是代理对象。</li><li>所以 <code>this.methodA()</code> 相当于一个普通的内部方法调用，它直接跳过了代理对象，AOP 的切面逻辑（比如事务）自然也就无法被织入。</li></ul><p><strong>如何解决？</strong></p><p><strong>使用 CGLIB (Spring Boot 默认)</strong>：因为 CGLIB 创建的是子类代理，并且 Spring Boot 默认 <code>proxy-target-class=true</code>，Spring 容器中注入的就是这个子类代理。这样，即使在类内部，对方法的调用也有机会被代理拦截（尽管 <code>this</code> 关键字的问题依然存在，但 Spring 通过更复杂的处理使得在某些情况下可以工作）。</p><ol><li><p><strong>自己注入自己</strong>：最标准的解决方式是避免使用 <code>this</code>。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class UserServiceImpl implements UserService &#123;    @Autowired    private UserService self; // 注入自身的代理对象    @Transactional    public void methodA() &#123; ... &#125;    public void methodB() &#123;        System.out.println("Executing method B...");        self.methodA(); // 通过代理对象调用    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>使用 <code>AopContext.currentProxy()</code></strong>：</p><pre class="line-numbers language-language-java"><code class="language-language-java">((UserService) AopContext.currentProxy()).methodA();<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h2 id="Spring-Bean-的生命周期"><a href="#Spring-Bean-的生命周期" class="headerlink" title="Spring Bean 的生命周期"></a>Spring Bean 的生命周期</h2><h3 id="第一阶段：实例化-Instantiation"><a href="#第一阶段：实例化-Instantiation" class="headerlink" title="第一阶段：实例化 (Instantiation)"></a>第一阶段：实例化 (Instantiation)</h3><ol><li><strong>容器启动，扫描 Bean 定义</strong>：Spring 容器（<code>ApplicationContext</code>）启动时，会读取配置（XML、注解等），找到所有被管理的 Bean 的“图纸”（<code>BeanDefinition</code>）。</li><li><strong>实例化 Bean</strong>：当容器需要一个 Bean 时（比如因为懒加载被首次请求，或者因为非懒加载在容器启动时就需要），它会根据 <code>BeanDefinition</code>，通过<strong>反射</strong>机制调用该类的构造函数，创建一个原始的、空的 Bean 对象实例。</li></ol><h3 id="第二阶段：属性填充-Populate"><a href="#第二阶段：属性填充-Populate" class="headerlink" title="第二阶段：属性填充 (Populate)"></a>第二阶段：属性填充 (Populate)</h3><ol><li><strong>依赖注入 (DI)</strong>：Spring 容器会检查这个 Bean 依赖的其他 Bean（通过 <code>@Autowired</code>, <code>@Resource</code> 等注解）。它会去容器中找到这些依赖的 Bean，并通过反射（调用 setter 方法或直接设置字段）将它们注入到当前的 Bean 实例中。</li></ol><h3 id="第三阶段：初始化-Initialization"><a href="#第三阶段：初始化-Initialization" class="headerlink" title="第三阶段：初始化 (Initialization)"></a>第三阶段：初始化 (Initialization)</h3><ol><li><p><strong>Aware 接口的注入</strong>：Spring 会检查 Bean 是否实现了特定的 <code>Aware</code> 接口（如 <code>BeanNameAware</code>, <code>BeanFactoryAware</code>, <code>ApplicationContextAware</code>）。如果实现了，就会调用相应的方法，将 Bean 的名字、所在的 Bean 工厂、应用上下文等环境信息注入给它。</p><ul><li><code>setBeanName()</code></li><li><code>setBeanFactory()</code></li><li><code>setApplicationContext()</code></li></ul></li><li><p><strong><code>BeanPostProcessor</code> 前置处理</strong>：调用所有已注册的 <code>BeanPostProcessor</code> 的 <code>postProcessBeforeInitialization()</code> 方法。这是一个非常强大的扩展点，它允许你在 Bean 的“正式初始化”之前，对 Bean 进行任意的修改或包装。<strong>AOP 代理就是在这个阶段通过一个特殊的 <code>BeanPostProcessor</code> 实现的</strong>。如果 Spring 发现这个 Bean 需要被代理，它就会在这里返回一个代理对象，替换掉原始的 Bean 对象。</p></li><li><p><strong><code>@PostConstruct</code> 注解</strong>：如果 Bean 的方法上标注了 <code>@PostConstruct</code> 注解，Spring 会调用这个方法。这是 JSR-250 规范定义的，是官方<strong>推荐</strong>的初始化方式，因为它不依赖于 Spring 的特定接口。</p></li><li><p><strong><code>InitializingBean</code> 接口</strong>：如果 Bean 实现了 <code>InitializingBean</code> 接口，Spring 会调用它的 <code>afterPropertiesSet()</code> 方法。这是 Spring 早期的初始化方式。</p></li><li><p><strong>自定义 <code>init-method</code></strong>：如果你在 XML 配置中通过 <code>init-method</code> 属性，或者在 <code>@Bean</code> 注解中通过 <code>initMethod</code> 属性指定了自定义的初始化方法，Spring 会在此时调用它。</p><blockquote><p>初始化顺序：@PostConstruct -&gt; InitializingBean.afterPropertiesSet() -&gt; init-method。推荐使用 @PostConstruct。</p></blockquote></li><li><p><strong><code>BeanPostProcessor</code> 后置处理</strong>：调用所有 <code>BeanPostProcessor</code> 的 <code>postProcessAfterInitialization()</code> 方法。这是另一个关键的扩展点，它允许你在 Bean 完全初始化之后，再次进行处理。AOP 代理的创建主要发生在前置处理，但后置处理也可能被用到。</p></li></ol><h3 id="第四阶段：Bean-可用"><a href="#第四阶段：Bean-可用" class="headerlink" title="第四阶段：Bean 可用"></a>第四阶段：Bean 可用</h3><ol><li><strong>Bean 准备就绪</strong>：经过了以上所有步骤，Bean 现在是一个完全初始化、功能完备的对象了。它被存放在 Spring 容器的单例池（<code>singletonObjects</code>）中，等待其他对象来调用它。</li></ol><h3 id="第五阶段：销毁-Destruction"><a href="#第五阶段：销毁-Destruction" class="headerlink" title="第五阶段：销毁 (Destruction)"></a>第五阶段：销毁 (Destruction)</h3><p>当 Spring 容器关闭时（比如应用停止），容器会销毁它管理的所有单例 Bean。</p><ol><li><p><strong><code>@PreDestroy</code> 注解</strong>：如果 Bean 的方法上标注了 <code>@PreDestroy</code> 注解，Spring 会在销毁前调用这个方法。同样，这是 JSR-250 规范，是<strong>推荐</strong>的销毁方式。</p></li><li><p><strong><code>DisposableBean</code> 接口</strong>：如果 Bean 实现了 <code>DisposableBean</code> 接口，Spring 会调用它的 <code>destroy()</code> 方法。</p></li><li><p><strong>自定义 <code>destroy-method</code></strong>：如果你指定了自定义的销毁方法，它会在这里被调用。</p><blockquote><p>销毁顺序：@PreDestroy -&gt; DisposableBean.destroy() -&gt; destroy-method。推荐使用 @PreDestroy。</p></blockquote></li></ol><h2 id="BeanFactory-和-ApplicationContext-的区别"><a href="#BeanFactory-和-ApplicationContext-的区别" class="headerlink" title="BeanFactory 和 ApplicationContext 的区别"></a>BeanFactory 和 ApplicationContext 的区别</h2><p>两者是 Spring 两个核心接口，都可以用来获取 Bean 实例，但在功能上有所不同。</p><ul><li>BeanFactory<ul><li><strong>提供了基本的依赖注入支持</strong>。</li><li><strong>延迟加载，只有在明确请求时才初始化Bean</strong>。</li></ul></li><li>ApplicationContext<ul><li><strong>完全初始化所有单例Bean</strong>。</li><li><strong>支持国际化（i18n）、事件传播、资源加载等</strong>。</li><li><strong>提供了AOP功能</strong>。</li><li><strong>通常在应用程序中使用 ApplicationContext</strong>。</li></ul></li></ul><h2 id="Spring-加载-Bean的-方式"><a href="#Spring-加载-Bean的-方式" class="headerlink" title="Spring 加载 Bean的 方式"></a>Spring 加载 Bean的 方式</h2><ol><li>基于 XML 的配置</li><li>基于注解的配置</li><li>配置类（@Configuration + @Bean）</li><li>通过 FactoryBean</li></ol><h2 id="Spring-的-bean-为什么是单例的呢，并且除了单例以外还有什么形式，如果是多例的话，会有什么影响"><a href="#Spring-的-bean-为什么是单例的呢，并且除了单例以外还有什么形式，如果是多例的话，会有什么影响" class="headerlink" title="Spring 的 bean 为什么是单例的呢，并且除了单例以外还有什么形式，如果是多例的话，会有什么影响"></a>Spring 的 bean 为什么是单例的呢，并且除了单例以外还有什么形式，如果是多例的话，会有什么影响</h2><p>Spring 框架中Bean的默认作用域是<code>单例（singleton）</code>，这是出于以下几个原因：</p><ol><li><strong>性能优化</strong>：创建对象通常是一个昂贵的过程，尤其是涉及到 I&#x2F;O 操作（如数据库连接）时。使用单例可以减少对象创建的次数，节省资源和提升性能。</li><li><strong>状态共享</strong>：单例模式允许在应用的不同部分共享同一个 Bean 实例，这对于状态共享和管理非常有用。</li><li><strong>资源管理</strong>：许多 Bean ，如数据源、会话工厂等，是自然的单例，因为它们封装了共享资源，如数据库连接池。</li></ol><p>除了单例模式，Spring 还提供其他几种 Bean 的作用域：</p><ul><li><strong>单例（Singleton）</strong>：在 Spring IoC 容器仅存在一个 Bean 实例，Bean 以单例方式存在。</li><li><strong>原型（prototype）</strong>：每次注入或通过 Spring 容器的 getBean() 请求时，都会创建一个新的Bean实例（这种模式就是多例）。</li><li><strong>请求（request）</strong>：每个 HTTP 请求都会创建一个新的 Bean ，该作用域仅在请求的处理过程中有效。</li><li><strong>会话（session）</strong>：在一个 HTTP 会话中，一个 Bean 定义对应一个 Bean 实例，该作用域同样仅在会话期间中有效。</li><li><strong>应用（application）</strong>：在一个 ServletContext 的生命周期内，一个 Bean 定义对应一个 Bean 实例，同样仅在 Web 应用的生命周期中有效。</li></ul><p>如果将 Bean 定义为多例（prototype）作用域，将会有以下影响：</p><ol><li><strong>资源使用增加</strong>：每次请求 Bean 时都会创建新实例，会增加内存和资源的使用。</li><li><strong>状态管理</strong>：多例 Bean 不会共享状态，每个 Bean 实例都有自己的状态。</li><li><strong>生命周期管理</strong>：Spring 不会管理 prototype Bean 的完整生命周期，也就是说，Spring 不会调用 prototype Bean 的销毁方法。</li><li><strong>复杂性增加</strong>：在使用多例 Bean 时，需要更加小心地管理其生命周期和依赖关系。</li></ol><p>总的来说，选择正确的作用域取决于具体的应用需求。单例作用域适合于<strong>需要共享状态的全局资源</strong>，而原型作用域<strong>适合于那些具有独立状态、生命周期较短或需要隔离的Bean、每次都需要一个新实例的情况</strong>。</p><h2 id="Spring-循环依赖"><a href="#Spring-循环依赖" class="headerlink" title="Spring 循环依赖"></a>Spring 循环依赖</h2><h3 id="1-定义与前提"><a href="#1-定义与前提" class="headerlink" title="1. 定义与前提"></a><strong>1. 定义与前提</strong></h3><p>首先，循环依赖指的是两个或多个Bean之间相互依赖，形成了一个闭环，例如A依赖B，同时B又依赖A。</p><p>Spring只解决了<strong>单例（Singleton）作用域</strong>下，并且是基于<strong>setter或字段注入</strong>的循环依赖。对于构造器注入和原型（Prototype）作用域的循环依赖，Spring是无法解决的，会直接抛出异常。</p><h3 id="2-核心思想：分离实例化与初始化"><a href="#2-核心思想：分离实例化与初始化" class="headerlink" title="2. 核心思想：分离实例化与初始化"></a><strong>2. 核心思想：分离实例化与初始化</strong></h3><p>Spring解决问题的核心思想，是把Bean的创建过程拆分成了两个主要阶段：</p><ul><li><strong>实例化 (Instantiation)</strong>：通过反射创建一个原始的Bean对象，但此时Bean的属性都是null。</li><li><strong>初始化 (Initialization)</strong>：为Bean的属性进行依赖注入（DI）和执行各种初始化回调。</li></ul><p>通过将这两个阶段分离，Spring有机会将一个虽然还未完成初始化，但已经被实例化的“半成品”Bean提前暴露出去，从而打破循环。</p><h3 id="3-实现机制：三级缓存"><a href="#3-实现机制：三级缓存" class="headerlink" title="3. 实现机制：三级缓存"></a><strong>3. 实现机制：三级缓存</strong></h3><p>为了实现“半成品”Bean的提前暴露，Spring使用了“三级缓存”机制。这三个缓存都是Map结构：</p><ul><li><strong><code>singletonObjects</code> (一级缓存)</strong>：用于存放已经<strong>完全初始化</strong>的Bean，我们称之为“成品Bean缓存”。</li><li><strong><code>earlySingletonObjects</code> (二级缓存)</strong>：用于存放<strong>提前暴露</strong>的Bean，即“半成品Bean缓存”。这些Bean已被实例化，但还未完成属性注入。</li><li><strong><code>singletonFactories</code> (三级缓存)</strong>：这是解决循环依赖最关键的缓存。它存放的不是Bean对象，而是创建Bean的<strong>工厂（ObjectFactory）</strong>。这个工厂负责在真正需要时，才生成那个“半成品Bean”，并且可以包含AOP代理逻辑。</li></ul><h3 id="4-解决流程（以A、B循环依赖为例）"><a href="#4-解决流程（以A、B循环依赖为例）" class="headerlink" title="4. 解决流程（以A、B循环依赖为例）"></a><strong>4. 解决流程（以A、B循环依赖为例）</strong></h3><ol><li><strong>创建A</strong>：<code>getBean(&quot;a&quot;)</code>被调用。Spring首先实例化A，得到一个原始对象。然后，它并不会立即初始化A，而是将一个能够产生A的<code>ObjectFactory</code>放入<strong>三级缓存</strong>。</li><li><strong>A注入B</strong>：Spring开始初始化A，发现它依赖B，于是调用<code>getBean(&quot;b&quot;)</code>。</li><li><strong>创建B</strong>：Spring去创建B，同样先实例化B，然后将能产生B的<code>ObjectFactory</code>放入<strong>三级缓存</strong>。</li><li><strong>B注入A（关键步骤）</strong>：Spring初始化B，发现它依赖A，于是调用<code>getBean(&quot;a&quot;)</code>。</li><li><strong>获取A的半成品</strong>：此时，Spring按顺序查找A：<ul><li>在一级缓存中查找，找不到（A未完全初始化）。</li><li>在二级缓存中查找，也找不到。</li><li>在<strong>三级缓存</strong>中，<strong>成功找到了A的<code>ObjectFactory</code></strong>。</li></ul></li><li><strong>提前暴露A</strong>：Spring调用这个<code>ObjectFactory</code>来生成一个A的“半成品”（如果A需要AOP代理，此时会生成代理对象），并将这个半成品A放入<strong>二级缓存</strong>，然后从三级缓存中移除A的工厂。</li><li><strong>B创建完成</strong>：B获取到了A的半成品引用，顺利完成了自己的初始化。之后，完整的B被放入<strong>一级缓存</strong>。</li><li><strong>A创建完成</strong>：回到A的创建流程，它现在可以从一级缓存中获取到完整的B对象，并完成自己的初始化。最后，完整的A也被放入<strong>一级缓存</strong>。</li></ol><p>至此，循环依赖被解决。</p><h3 id="5-为什么必须是三级缓存？"><a href="#5-为什么必须是三级缓存？" class="headerlink" title="5. 为什么必须是三级缓存？"></a><strong>5. 为什么必须是三级缓存？</strong></h3><p>使用三级缓存而不是二级，主要是为了<strong>延迟AOP代理对象的创建</strong>。如果只有二级缓存，那就必须在Bean实例化后立刻创建代理对象，但实际上这个Bean可能最终并不需要被代理。三级缓存通过<code>ObjectFactory</code>，将代理对象的创建推迟到了真正发生循环依赖、且有其他Bean需要注入它时，才去执行，这是一种更合理、更优化的设计。</p><h2 id="By-Type-和-By-Name-的区别（-Autowired和-Resource的区别）"><a href="#By-Type-和-By-Name-的区别（-Autowired和-Resource的区别）" class="headerlink" title="By Type 和 By Name 的区别（@Autowired和@Resource的区别）"></a>By Type 和 By Name 的区别（@Autowired和@Resource的区别）</h2><p><strong>@Autowired 基于类型的依赖注入（By Type）</strong>：</p><ul><li><p><strong>定义</strong>：在基于类型的注入中，Spring 容器使用要注入的属性或构造函数参数的类型来在容器中查找匹配的 Bean。</p></li><li><p>代码示例：</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Autowiredprivate MyService myService;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>  在这个例子中，Spring 会在其容器中查找 MyService 类型的 Bean，并进行注入。</p><p>  <strong>多个候选 Bean</strong>：如果存在多个同类型的 Bean，而没有其他限定信息，Spring 将无法决定使用哪一个，从而导致异常。这种情况下，可以使用 @Qualifier 注解来指定 Bean 的名称。</p></li></ul><p>虽然<code>@Autowired</code><strong>本身不提供直接的基于名称的注入，通过与</strong><code>@Qualifier</code><strong>结合使用，它可以非常灵活地实现</strong><code>基于名称的注入</code>相似的功能。</p><p><strong>代码示例</strong>：</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Autowired@Qualifier("mySpecificService")private MyService myService;...@Service("mySpecificService")public class MyServiceImpl implements MyService &#123;    // 实现细节&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子中，Spring 会在其容器中查找名为 <code>mySpecificService</code> 的Bean来注入。</p><p><strong>@Resource 基于名称的依赖注入（By Name）（默认）</strong></p><p><strong>定义</strong>：<code>@Resource</code>注解是基于 JSR-250 标准，它可以根据名称或类型来注入依赖。 <strong>代码示例</strong>：</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Resource(name = "mySpecificService")private MyService myService;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在这个例子中，<code>@Resource</code><strong>注解通过</strong><code>name</code>属性直接指定了要注入的 Bean 名称，从而实现了基于名称的注入。 <strong>@Resource 基于类型的依赖注入（By Type）</strong></p><pre class="line-numbers language-language-java"><code class="language-language-java">@Resourceprivate MyService myService;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="Spring-事务失效的经典场景"><a href="#Spring-事务失效的经典场景" class="headerlink" title="Spring 事务失效的经典场景"></a><strong>Spring 事务失效的经典场景</strong></h2><h3 id="1-最经典的失效场景：方法内部调用（Self-Invocation）"><a href="#1-最经典的失效场景：方法内部调用（Self-Invocation）" class="headerlink" title="1. 最经典的失效场景：方法内部调用（Self-Invocation）"></a>1. 最经典的失效场景：方法内部调用（Self-Invocation）</h3><ul><li><p><strong>场景描述：</strong></p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class OrderService &#123;    public void createOrder() &#123;        // ... 其他操作 ...        this.updateStock(); // this 调用，事务会失效！    &#125;    @Transactional    public void updateStock() &#123;        // ... 更新库存的数据库操作 ...    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>失效原因</strong>: 调用 <code>this.updateStock()</code> 时，使用的是原始的 <code>OrderService</code> 实例，而不是 Spring 生成的代理对象。调用直接发生在对象内部，完全绕过了代理对象，AOP 根本没有机会介入，自然无法开启事务。</p></li><li><p><strong>解决方案</strong>: 避免 <code>this</code> 调用。通过注入自身的代理对象来解决</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Autowired private OrderService self; public void createOrder() &#123;    // ... 其他操作 ...    self.updateStock();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="2-方法的访问权限问题（private-protected-等）"><a href="#2-方法的访问权限问题（private-protected-等）" class="headerlink" title="2. 方法的访问权限问题（private, protected 等）"></a>2. 方法的访问权限问题（<code>private</code>, <code>protected</code> 等）</h3><ul><li><p><strong>场景描述</strong>: 将 <code>@Transactional</code> 注解加在了一个在非 <code>public</code> 方法上。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class OrderService &#123;    @Transactional    private void processPayment() &#123; // private 方法，事务会失效！        // ...    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>失效原因</strong>: Spring AOP 在创建代理时，无法拦截（或重写）<code>private</code> 方法，因为它们在类的外部是不可见的。对于 <code>protected</code> 和 <code>package-private</code>（默认）方法，虽然 CGLIB 代理理论上可以，但也存在限制且不符合最佳实践。<strong>Spring 官方文档明确建议，只在 <code>public</code> 方法上使用 <code>@Transactional</code> 注解。</strong></p></li></ul><h3 id="3-异常类型不匹配（rollbackFor-设置错误）"><a href="#3-异常类型不匹配（rollbackFor-设置错误）" class="headerlink" title="3. 异常类型不匹配（rollbackFor 设置错误）"></a>3. 异常类型不匹配（<code>rollbackFor</code> 设置错误）</h3><ul><li><p><strong>场景描述</strong>: 事务方法中抛出了一个<strong>受检异常（Checked Exception）</strong>，但没有指定 <code>rollbackFor</code> 属性。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class UserService &#123;    @Transactional    public void register() throws Exception &#123;        // ... 数据库操作 ...        if (someCondition) &#123;            throw new Exception("一个受检异常"); // 事务默认不会回滚！        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>失效原因</strong>: Spring 的默认事务回滚策略是：<strong>只在遇到 <code>RuntimeException</code>（运行时异常）或 <code>Error</code> 时才回滚</strong>。对于普通的 <code>Exception</code>（受检异常），Spring 认为这可能是业务逻辑的一部分，需要开发者明确指示才回滚。因此，上述代码在抛出 <code>Exception</code> 后，事务会<strong>提交</strong>而不是回滚。</p></li><li><p><strong>解决方案</strong>: 在注解上明确指定回滚的异常类型：<code>@Transactional(rollbackFor = Exception.class)</code>。</p></li></ul><h3 id="4-方法内部吞掉（catch）了异常"><a href="#4-方法内部吞掉（catch）了异常" class="headerlink" title="4. 方法内部吞掉（catch）了异常"></a>4. 方法内部吞掉（<code>catch</code>）了异常</h3><ul><li><p><strong>场景描述</strong>: 在事务方法内部使用了 <code>try...catch</code> 块，并且在 <code>catch</code> 块中没有将异常重新抛出。</p><pre class="line-numbers language-language-java"><code class="language-language-java">@Servicepublic class ProductService &#123;    @Transactional    public void updateProduct() &#123;        try &#123;            // ... 执行数据库更新，期间发生 RuntimeException ...        &#125; catch (Exception e) &#123;            // 异常被捕获了，但没有重新抛出            System.out.println("发生了一个异常，但我处理掉了");        &#125;        // 方法正常结束，没有异常抛出    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>失效原因</strong>: 事务回滚的前提是，代理对象需要<strong>感知到</strong>有异常发生。在上述代码中，异常被 <code>catch</code> 块完全“消化”了，没有向方法外部传播。对于代理对象来说，<code>updateProduct</code> 方法是“正常返回”的，它根本不知道内部曾发生过异常，因此会正常提交事务。</p></li></ul><h3 id="5-Bean-没有被-Spring-容器管理"><a href="#5-Bean-没有被-Spring-容器管理" class="headerlink" title="5. Bean 没有被 Spring 容器管理"></a>5. Bean 没有被 Spring 容器管理</h3><ul><li><p><strong>场景描述</strong>: 你手动 <code>new</code> 了一个对象，然后调用它的事务方法。</p><pre class="line-numbers language-language-java"><code class="language-language-java">public void someBusinessLogic() &#123;    OrderService service = new OrderService(); // 手动 new    service.updateStock(); // 事务会失效&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>失效原因</strong>: 这个 <code>OrderService</code> 实例是一个普通的 Java 对象，它完全游离在 Spring 容器之外。Spring 根本不知道它的存在，自然也无法为它创建代理对象，所有与 Spring 相关的功能（包括 <code>@Transactional</code>）都将无效。</p></li></ul><h2 id="Spring-Boot-的核心思想是什么？"><a href="#Spring-Boot-的核心思想是什么？" class="headerlink" title="Spring Boot 的核心思想是什么？"></a>Spring Boot 的核心思想是什么？</h2><p>Spring Boot 的核心思想是**“约定优于配置”** (Convention over Configuration)。</p><p>这个思想的本质是，Spring Boot 团队认为，对于绝大多数项目，很多技术的配置方式都是固定和通用的。因此，Spring Boot 不应该让开发者每次都去重复编写这些样板化的配置，而是应该提供一套“<strong>约定好的、合理的默认配置</strong>”。</p><p>具体来说，这个核心思想体现在以下三个方面：</p><ol><li><strong>开箱即用</strong>: Spring Boot 致力于提供一种“just run”的开发体验。开发者引入相关依赖后，无需或只需少量配置，就能快速启动和运行一个功能完备的应用。</li><li><ul><li>opinionated (有主见的)**：它为项目提供了一套“有主见的”最佳实践集合。比如，当它检测到 <code>spring-webmvc</code> 在 classpath 中时，它会“主观地”认为你正在开发一个 Web 应用，因此会自动为你配置好 DispatcherServlet、内嵌的 Tomcat 服务器等。</li></ul></li><li><strong>可覆盖的默认值</strong>: 虽然它提供了强大的默认配置，但它绝不“绑架”开发者。如果你对默认的配置不满意，比如不想要 Tomcat 而是想用 Undertow，或者想自定义数据源的参数，你随时可以通过简单的配置（如在 <code>application.yml</code> 中修改）来覆盖掉它的默认行为。</li></ol><p>总而言之，Spring Boot 的核心思想是通过<strong>自动配置</strong>、<strong>起步依赖</strong>等手段，用一套“约定好的”默认值来代替繁琐的手动配置，从而极大简化项目搭建和开发过程，让开发者能更专注于业务逻辑本身。</p><h2 id="什么是-Starter？它和普通依赖有什么区别？"><a href="#什么是-Starter？它和普通依赖有什么区别？" class="headerlink" title="什么是 Starter？它和普通依赖有什么区别？"></a>什么是 Starter？它和普通依赖有什么区别？</h2><p><strong>Starter (起步依赖)</strong> 是 Spring Boot 的核心特性之一，它本质上是一个特殊的 <strong>Maven 依赖描述符（POM）</strong>，而不是一个包含具体代码的 JAR 包。</p><p>它的主要作用有两个：</p><ol><li><strong>聚合相关依赖</strong>: Starter 将构建某一特定功能（如 Web 开发、数据访问）所需的一整套相关依赖聚合在一起。例如，我们引入 <code>spring-boot-starter-web</code>，它就会自动将 Spring MVC、Jackson、Validation API、内嵌的 Tomcat 服务器等所有必要的库都间接引入进来。</li><li><strong>管理版本兼容性</strong>: Starter 依赖于 Spring Boot 的父项目 (<code>spring-boot-dependencies</code>)，这个父项目像一个“字典”一样，统一管理了所有常用第三方库的版本号。这确保了 Starter 引入的所有传递性依赖之间版本都是互相兼容的，开发者无需再手动管理版本，从而避免了“依赖地狱”。</li></ol><p><strong>它和普通依赖的核心区别在于</strong>：</p><ul><li><strong>目的不同</strong>: 普通依赖（如 <code>spring-core.jar</code>）的目的是提供具体的 API 和功能实现。而 Starter 的目的在于<strong>简化依赖管理</strong>，它本身不提供任何代码，只负责“带入”其他依赖。</li><li><strong>关注点不同</strong>: 使用普通依赖时，开发者需要自己去管理一系列相关依赖的版本和兼容性。而使用 Starter 时，开发者只需要关注“我需要什么功能”（如 <code>web</code>、<code>jpa</code>），而不需要关心“为了这个功能我需要引入哪些具体的库、版本是什么”。</li></ul><p>简单来说，Starter 是 Spring Boot 提供的一种“一站式”解决方案，让依赖管理从“手动挡”变成了“自动挡”。</p><h2 id="请详细解释一下-Spring-Boot-的自动配置原理。"><a href="#请详细解释一下-Spring-Boot-的自动配置原理。" class="headerlink" title="请详细解释一下 Spring Boot 的自动配置原理。"></a>请详细解释一下 Spring Boot 的自动配置原理。</h2><p>Spring Boot 的自动配置是其最核心的功能，其原理主要依赖于三个关键部分：<code>@EnableAutoConfiguration</code> 注解、<code>META-INF/spring.factories</code> 文件和 <code>@Conditional</code> 系列条件注解。</p><p>整个流程如下：</p><ol><li><p><strong>启动自动配置总开关</strong>: Spring Boot 项目的启动类上通常有一个 <code>@SpringBootApplication</code> 注解，它是一个复合注解，里面包含了 <code>@Configuration</code>、<code>@ComponentScan</code>、<code>@EnableAutoConfiguration</code>，其中 <strong><code>@EnableAutoConfiguration</code></strong>。这个注解是激活自动配置功能的总开关。</p></li><li><p><strong>扫描候选配置类</strong>: 启动应用时，<code>@EnableAutoConfiguration</code> 注解会借助 Spring 的 <code>SpringFactoriesLoader</code> 机制，去扫描所有引入的 JAR 包中类路径下的 <strong><code>META-INF/spring.factories</code></strong> 文件。这个文件中以键值对的形式列出了大量预定义的自动配置类，例如：</p><pre class="line-numbers language-language-java"><code class="language-language-java"># Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>按需装配（条件判断）</strong>: 候选人名单有了，但并不是所有的配置都会生效。每一个自动配置类（如 <code>DataSourceAutoConfiguration</code>）本身都是一个 <code>@Configuration</code> 配置类，并且它上面会使用大量的 <strong><code>@Conditional</code> 系列注解</strong>来进行条件判断。只有当所有条件都满足时，这个自动配置类才会生效。常见的条件注解有：</p><ul><li><strong><code>@ConditionalOnClass</code></strong>: 判断 classpath 中是否存在指定的类。例如，<code>DataSourceAutoConfiguration</code> 会检查是否存在 <code>DataSource.class</code>。</li><li><strong><code>@ConditionalOnMissingBean</code></strong>: 判断 Spring 容器中是否<strong>不</strong>存在指定类型的 Bean。这是允许用户覆盖默认配置的关键，例如，如果用户自己定义了一个 <code>DataSource</code> Bean，那么 Spring Boot 的默认 <code>DataSource</code> 配置就不会生效。</li><li><strong><code>@ConditionalOnProperty</code></strong>: 判断配置文件中是否存在指定的属性及其值。</li></ul></li><li><p><strong>注入 Bean</strong>: 一旦某个自动配置类的所有条件都满足，它就会被 Spring 容器加载，其内部使用 <code>@Bean</code> 注解定义的一系列 Bean（如 <code>DataSource</code>, <code>JdbcTemplate</code> 等）就会被创建并注入到容器中，从而完成了自动配置。</p></li></ol><p>总结来说，自动配置的原理就是 <strong>“扫描 -&gt; 过滤 -&gt; 装配”</strong>：通过 <code>spring.factories</code> <strong>扫描</strong>到所有可能的配置，然后通过 <code>@Conditional</code> <strong>过滤</strong>掉不满足条件的，最后把满足条件的配置<strong>装配</strong>到 Spring 容器中。</p><h2 id="如何禁用某个不想要的自动配置？"><a href="#如何禁用某个不想要的自动配置？" class="headerlink" title="如何禁用某个不想要的自动配置？"></a>如何禁用某个不想要的自动配置？</h2><p>通常有两种方式可以禁用 Spring Boot 中我们不想要的自动配置：</p><ol><li><p><strong>通过注解的 <code>exclude</code> 属性（推荐）</strong>:<br>这是最常用且最直接的方式。我们可以在启动类的 <code>@SpringBootApplication</code> 或 <code>@EnableAutoConfiguration</code> 注解中，通过 <code>exclude</code> 属性来指定要排除的自动配置类。</p><pre class="line-numbers language-language-java"><code class="language-language-java">import org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;// 禁用数据源的自动配置@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;)public class MyApplication &#123;    // ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>通过配置文件属性</strong>:<br>我们也可以在 <code>application.properties</code> 或 <code>application.yml</code> 文件中，通过 <code>spring.autoconfigure.exclude</code> 属性来禁用一个或多个自动配置。</p><pre class="line-numbers language-language-java"><code class="language-language-java"># 在 application.properties 中禁用spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h2 id="Spring-Boot-的配置文件加载顺序是怎样的？"><a href="#Spring-Boot-的配置文件加载顺序是怎样的？" class="headerlink" title="Spring Boot 的配置文件加载顺序是怎样的？"></a>Spring Boot 的配置文件加载顺序是怎样的？</h2><p>Spring Boot 会从多个位置加载配置文件（如 <code>application.properties</code> 或 <code>application.yml</code>），并且有一个明确的<strong>优先级顺序</strong>。高优先级的配置会覆盖低优先级的配置。一个常见的、简化的优先级从低到高如下：</p><ol><li><p><strong>JAR 包内部的配置文件</strong>: 项目内部resource目录下的 <code>application.properties</code> 或 <code>application.yml</code>。这是最低的优先级，通常作为默认配置。</p></li><li><p><strong>JAR 包外部的配置文件</strong>:</p><ul><li><p>在 JAR 包<strong>同级目录</strong>下的配置文件。</p><pre class="line-numbers language-language-java"><code class="language-language-java">/opt/app/├── myapp.jar└── application.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><strong>操作系统环境变量</strong>。</p><ul><li><p>在 Linux 或 macOS 上: <code>export SERVER_PORT=8081</code>，在 Windows 上: <code>set SERVER_PORT=8081</code></p></li><li><p>环境变量 <code>SERVER_PORT</code> 会被 Spring Boot 识别为配置属性 <code>server.port</code>；环境变量 <code>SPRING_DATASOURCE_URL</code> 会被识别为 <code>spring.datasource.url</code>。</p></li><li><p>在 Docker容器化环境中，通过环境变量来注入配置是标准做法。</p><pre class="line-numbers language-language-docker"><code class="language-language-docker">FROM openjdk:17-jdk-slimWORKDIR /appCOPY target/myapp.jar myapp.jarENTRYPOINT ["java", "-jar", "myapp.jar"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-language-bash"><code class="language-language-bash">docker run -p 9090:9090 -e "SERVER_PORT=9090" my-app<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li><p><strong>Java 系统属性</strong> (通过 <code>java -D&lt;key&gt;=&lt;value&gt;</code> 指定)。</p><ul><li><p><strong>定义</strong>: 这是专属于某一个<strong>Java 虚拟机 (JVM) 实例</strong>的变量。它在 <code>java</code> 命令启动时通过 <code>-D</code> 参数传入，只对当前这个启动的 Java 进程有效。</p></li><li><p><strong>Spring Boot 如何读取</strong>: Spring Boot 会直接读取 JVM 的所有系统属性，属性名是什么，它就识别成什么。</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">java -Dserver.port=8082 -jar myapp.jar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  <strong>示例</strong>: <code>-Dserver.port=8082</code> 就直接对应配置属性 <code>server.port</code>。</p></li></ul></li><li><p><strong>命令行参数</strong> (通过 <code>-&lt;key&gt;=&lt;value&gt;</code> 传递)。这是最高的优先级。</p><ul><li><p><strong>定义</strong>: 这是<strong>直接传递给 Spring Boot 应用本身</strong>的参数。它在 <code>java -jar myapp.jar</code> 命令的<strong>最后面</strong>，通过 <code>--</code> 形式添加。</p></li><li><p><strong>Spring Boot 如何读取</strong>: Spring Boot 会专门解析这些以 <code>--</code> 开头的参数，并直接将它们映射为配置属性。</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">java -jar myapp.jar --server.port=8083<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  <strong>示例</strong>: <code>--server.port=8083</code> 就直接对应配置属性 <code>server.port</code>。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 微服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商系统核心技术问题与解决方案</title>
      <link href="/2023/12/26/dian-shang-xi-tong-he-xin-ji-zhu-wen-ti-yu-jie-jue-fang-an/"/>
      <url>/2023/12/26/dian-shang-xi-tong-he-xin-ji-zhu-wen-ti-yu-jie-jue-fang-an/</url>
      
        <content type="html"><![CDATA[<h1 id><a href="#" class="headerlink" title></a></h1><h2 id="一、引言：电商系统的技术挑战"><a href="#一、引言：电商系统的技术挑战" class="headerlink" title="一、引言：电商系统的技术挑战"></a>一、引言：电商系统的技术挑战</h2><p>电商系统是典型的高并发、大数据量应用场景，面临着订单处理、库存管理、数据一致性、系统性能等多方面的挑战。本文整合了常见的电商系统技术问题及其解决方案，旨在为电商系统的设计与实现提供技术参考。</p><h2 id="二、高并发场景下的订单处理"><a href="#二、高并发场景下的订单处理" class="headerlink" title="二、高并发场景下的订单处理"></a>二、高并发场景下的订单处理</h2><h3 id="1-大量订单快速拉取方案"><a href="#1-大量订单快速拉取方案" class="headerlink" title="1. 大量订单快速拉取方案"></a>1. 大量订单快速拉取方案</h3><p>双11等大促期间存在大量订单需要拉取，如何保证系统可用性？</p><p><strong>解决方案</strong>：</p><ul><li><strong>线程池批量处理</strong>：采用Java语言中的线程池进行批量拉取，设置为异步操作</li><li><strong>读写分离</strong>：将数据库设置为主从同步，查询操作转发到从库，写操作转发到主库</li><li><strong>引入缓存中间件</strong>：使用Redis缓存热点数据，减轻数据库压力</li><li><strong>异步消息队列</strong>：使用Kafka或RabbitMQ等消息队列，将订单处理异步化</li></ul><h3 id="2-订单修改与取消的高效处理"><a href="#2-订单修改与取消的高效处理" class="headerlink" title="2. 订单修改与取消的高效处理"></a>2. 订单修改与取消的高效处理</h3><p>订单取消或修改频繁时，如何快速响应并准确更新相关数据？</p><p><strong>解决方案</strong>：</p><ul><li><p><strong>消息队列</strong>：利用消息队列异步处理订单的取消或修改请求，解耦请求和实际处理逻辑</p></li><li><p><strong>幂等性处理</strong>：确保订单的取消和修改操作具有幂等性，通过添加唯一标识符实现</p></li><li><p>前端防抖和节流</p><p>：</p><ul><li>防抖技术：确保在一定时间内多次触发只执行一次操作</li><li>节流技术：限制一个函数在一定时间内只能执行一次</li></ul></li><li><p><strong>后端限流措施</strong>：使用令牌桶或漏桶算法控制请求频率</p></li><li><p><strong>合理设置修改频率限制</strong>：对敏感信息设定合理的修改间隔或次数限制</p></li></ul><h3 id="3-订单数据一致性保证"><a href="#3-订单数据一致性保证" class="headerlink" title="3. 订单数据一致性保证"></a>3. 订单数据一致性保证</h3><p>在订单处理过程中，如何保证订单数据的一致性和准确性？</p><p><strong>解决方案</strong>：</p><ul><li><strong>数据库事务(ACID)</strong>：确保订单操作的原子性、一致性、隔离性和持久性</li><li><strong>接口幂等性</strong>：通过唯一标识、Token机制等确保重复请求不会导致数据异常</li><li><strong>乐观锁和悲观锁</strong>：根据场景选择合适的锁机制处理并发修改</li><li><strong>最终一致性</strong>：对于非关键操作，可采用最终一致性模型，通过异步补偿机制保证数据最终一致</li></ul><h2 id="三、库存管理系统设计"><a href="#三、库存管理系统设计" class="headerlink" title="三、库存管理系统设计"></a>三、库存管理系统设计</h2><h3 id="1-大批量库存调整方案"><a href="#1-大批量库存调整方案" class="headerlink" title="1. 大批量库存调整方案"></a>1. 大批量库存调整方案</h3><p><strong>场景</strong>：需要一次性调整1万个商品的库存，要求不出现负库存，任一失败则全部回滚。</p><p><strong>解决方案</strong>：</p><ul><li><strong>事务范围控制</strong>：将整个库存调整过程控制在一个事务内</li><li><strong>锁定库存记录</strong>：使用数据库行锁或表锁确保并发安全</li><li><strong>批量操作</strong>：使用批处理技术提高效率</li><li><strong>完整的错误处理</strong>：捕获异常并进行回滚</li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;public class InventoryAdjustment &#123;    private Connection connection;    public InventoryAdjustment(Connection connection) &#123;        this.connection = connection;    &#125;    /**     * 调整多个商品的库存。     * @param adjustments 商品的库存调整数据（商品ID和调整数量）     * @return 是否调整成功     */    public boolean adjustInventory(List<InventoryData> adjustments) &#123;        try &#123;            // 启动事务            connection.setAutoCommit(false);            // 锁定库存记录并检查库存            for (InventoryData data : adjustments) &#123;                if (!checkAndLockInventory(data.productId, data.adjustmentQuantity)) &#123;                    // 库存不足，回滚事务                    connection.rollback();                    return false;                &#125;            &#125;            // 提交事务            connection.commit();            return true;        &#125; catch (SQLException e) &#123;            e.printStackTrace();            try &#123;                // 发生异常，回滚事务                connection.rollback();            &#125; catch (SQLException ex) &#123;                ex.printStackTrace();            &#125;            return false;        &#125; finally &#123;            try &#123;                // 恢复自动提交                connection.setAutoCommit(true);            &#125; catch (SQLException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;    private boolean checkAndLockInventory(int productId, int adjustmentQuantity) throws SQLException &#123;        String sql = "SELECT stock FROM inventory WHERE product_id = ? FOR UPDATE";        try (PreparedStatement stmt = connection.prepareStatement(sql)) &#123;            stmt.setInt(1, productId);            ResultSet rs = stmt.executeQuery();            if (rs.next()) &#123;                int currentStock = rs.getInt("stock");                if (currentStock + adjustmentQuantity < 0) &#123;                    return false; // 库存不足                &#125; else &#123;                    // 更新库存                    updateInventory(productId, currentStock + adjustmentQuantity);                    return true;                &#125;            &#125;        &#125;        return false;    &#125;    private void updateInventory(int productId, int newStock) throws SQLException &#123;        String updateSql = "UPDATE inventory SET stock = ? WHERE product_id = ?";        try (PreparedStatement stmt = connection.prepareStatement(updateSql)) &#123;            stmt.setInt(1, newStock);            stmt.setInt(2, productId);            stmt.executeUpdate();        &#125;    &#125;&#125;class InventoryData &#123;    int productId;    int adjustmentQuantity;    public InventoryData(int productId, int adjustmentQuantity) &#123;        this.productId = productId;        this.adjustmentQuantity = adjustmentQuantity;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-实时库存更新策略"><a href="#2-实时库存更新策略" class="headerlink" title="2. 实时库存更新策略"></a>2. 实时库存更新策略</h3><p>如何实现实时库存更新，减少超卖或缺货情况？</p><p><strong>解决方案</strong>：</p><ul><li><strong>分布式锁</strong>：使用Redis或Zookeeper实现分布式锁，确保同一时间只有一个请求能修改库存</li><li><strong>预扣库存</strong>：下单时先预扣库存，支付成功后再实际扣减</li><li><strong>库存缓存</strong>：热点商品库存放入缓存，减少数据库访问</li><li><strong>定时补偿</strong>：定期检查库存与订单数据一致性，发现不一致时进行补偿</li></ul><h2 id="四、数据库优化与管理"><a href="#四、数据库优化与管理" class="headerlink" title="四、数据库优化与管理"></a>四、数据库优化与管理</h2><h3 id="1-慢查询优化方案"><a href="#1-慢查询优化方案" class="headerlink" title="1. 慢查询优化方案"></a>1. 慢查询优化方案</h3><p><strong>常见问题</strong>：项目中的数据库慢查询问题及解决方法。</p><p><strong>解决方案</strong>：</p><ul><li><p><strong>使用EXPLAIN分析</strong>：使用EXPLAIN关键字分析SQL执行计划，找出性能瓶颈</p></li><li><p><strong>限制返回行数</strong>：在代码中先统计查询返回行数，超出阈值时提示用户调整查询条件</p></li><li><p><strong>异步处理大数据量查询</strong>：对确实需要大量数据的场景，通过异步方式处理，处理完后统一返回或生成文件下载</p></li><li><p>分库分表</p><p>：将大表拆分成多个小表，减少单次查询的数据量</p><ul><li><strong>技术工具</strong>：Sharding-JDBC（应用层分片）、MyCat（数据库中间件）</li></ul></li></ul><h3 id="2-实时报表数据更新策略"><a href="#2-实时报表数据更新策略" class="headerlink" title="2. 实时报表数据更新策略"></a>2. 实时报表数据更新策略</h3><p><strong>场景</strong>：实时报表需要表的实时更新，如果采取先删除后更新策略，当数据量大时会出现数据暂时不完整的情况。</p><p><strong>解决方案</strong>：</p><ul><li><strong>增加临时表</strong>：更新数据时写入临时表，数据准备完成后切换表引用</li><li><strong>版本控制</strong>：为数据添加版本号，更新时增加版本而不是直接删除旧数据，报表始终展示最新版本</li><li><strong>增量更新</strong>：只对变更的数据进行更新，避免全量更新</li><li><strong>引入缓存</strong>：更新过程中使用缓存存储旧数据，新数据准备就绪后再更新缓存</li></ul><h3 id="3-大数据量导出方案"><a href="#3-大数据量导出方案" class="headerlink" title="3. 大数据量导出方案"></a>3. 大数据量导出方案</h3><p>如何导出500W的数据？</p><p><strong>解决方案</strong>：</p><ul><li><strong>分批次导出</strong>：将500W数据分割成多个小批次（如每批5万条）进行导出</li><li><strong>异步导出</strong>：将导出任务放入后台，用户可以继续其他操作，导出完成后通知用户</li><li><strong>使用专业工具</strong>：如DataX、Sqoop等ETL工具进行大数据量导出</li><li><strong>多线程并行导出</strong>：使用多线程技术并行处理多个数据分片</li><li><strong>导出格式优化</strong>：根据需求选择合适的导出格式（CSV、Excel、JSON等）</li></ul><h2 id="五、系统性能优化策略"><a href="#五、系统性能优化策略" class="headerlink" title="五、系统性能优化策略"></a>五、系统性能优化策略</h2><h3 id="1-高峰期系统性能保障"><a href="#1-高峰期系统性能保障" class="headerlink" title="1. 高峰期系统性能保障"></a>1. 高峰期系统性能保障</h3><p>用户请求高峰时，如何优化系统性能保证快速响应？</p><p><strong>解决方案</strong>：</p><ol><li><p><strong>负载均衡</strong>：使用Nginx等负载均衡器分散流量到多个服务器</p></li><li><p>缓存机制</p><p>：</p><ul><li><strong>应用级缓存</strong>：使用Redis、Memcached缓存热点数据</li><li><strong>数据库缓存</strong>：利用数据库内建缓存机制</li><li><strong>前端缓存</strong>：缓存静态资源减少重复请求</li></ul></li><li><p>数据库优化</p><p>：</p><ul><li><strong>索引优化</strong>：确保表使用合适的索引</li><li><strong>查询优化</strong>：优化SQL语句，避免复杂联接和全表扫描</li><li><strong>读写分离</strong>：查询和更新操作分布到不同服务器</li></ul></li><li><p><strong>异步处理</strong>：使用RabbitMQ、Kafka等消息队列处理非实时任务</p></li><li><p>扩展策略</p><p>：</p><ul><li><strong>水平扩展</strong>：增加更多服务器分散负载</li><li><strong>垂直扩展</strong>：提升单服务器处理能力</li></ul></li><li><p><strong>代码优化</strong>：优化循环、条件语句，使用高效算法和数据结构</p></li><li><p><strong>服务微服务化</strong>：将大型应用拆分为多个微服务，灵活管理负载并独立扩展</p></li></ol><h3 id="2-线程池满载处理策略"><a href="#2-线程池满载处理策略" class="headerlink" title="2. 线程池满载处理策略"></a>2. 线程池满载处理策略</h3><p>线程池队列满后任务失败或丢弃，如何解决？</p><p><strong>解决方案</strong>：</p><ul><li><p>自定义拒绝策略</p><p>：实现RejectedExecutionHandler接口</p><ol><li><strong>拒绝任务入库</strong>：将被拒绝的任务持久化到数据库中，保证不丢失任务</li><li><strong>任务重回队列</strong>：尝试将任务重新放回队列或等待有可用线程</li></ol></li></ul><p>示例代码：</p><pre class="line-numbers language-language-java"><code class="language-language-java">public class MyRejectedExecutionHandler implements RejectedExecutionHandler &#123;    @Override    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123;        System.out.println("任务被拒绝执行: " + r.toString());        // 这里可以添加更多的处理逻辑，比如保存任务信息到数据库        // 或尝试将任务重新加入到某个队列中    &#125;&#125;// 使用自定义拒绝策略创建线程池RejectedExecutionHandler handler = new MyRejectedExecutionHandler();ThreadPoolExecutor executor =     new ThreadPoolExecutor(corePoolSize, maxPoolSize, keepAliveTime, unit, workQueue, handler);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="六、接口设计与幂等性保证"><a href="#六、接口设计与幂等性保证" class="headerlink" title="六、接口设计与幂等性保证"></a>六、接口设计与幂等性保证</h2><h3 id="1-接口幂等性实现方法"><a href="#1-接口幂等性实现方法" class="headerlink" title="1. 接口幂等性实现方法"></a>1. 接口幂等性实现方法</h3><p>如何保证接口的幂等性？</p><p><strong>实现方法</strong>：</p><ol><li><p>唯一事务标识</p><p>：</p><ul><li>客户端生成唯一标识（如订单ID+用户ID+时间戳）</li><li>服务器根据标识判断操作是否已执行</li></ul></li><li><p>Token机制</p><p>：</p><ul><li>服务器向客户端发放唯一Token</li><li>客户端随请求发送Token，服务器执行后废弃该Token</li><li>重复请求到达时，Token已不存在，拒绝操作</li></ul></li><li><p>乐观锁</p><p>：</p><ul><li>通过版本号或时间戳确保数据未被其他操作修改</li><li>发现冲突时放弃当前操作</li></ul></li><li><p>悲观锁</p><p>：</p><ul><li>使用synchronized或ReentrantLock实现</li><li>确保同一时间只有一个请求处理，保证原子性</li></ul></li><li><p>数据库约束</p><p>：</p><ul><li>利用唯一约束和主键约束防止重复记录插入</li></ul></li></ol><h2 id="七、总结与最佳实践"><a href="#七、总结与最佳实践" class="headerlink" title="七、总结与最佳实践"></a>七、总结与最佳实践</h2><p>电商系统设计需要综合考虑高并发、大数据量、实时性等多方面因素，核心技术挑战包括：</p><ol><li><strong>高并发请求处理</strong>：通过负载均衡、缓存机制、异步处理等提高系统吞吐量</li><li><strong>数据一致性保证</strong>：合理使用事务、锁机制和幂等设计确保数据准确性</li><li><strong>实时性与性能平衡</strong>：在保证数据实时性的同时优化系统性能，如合理使用缓存、预计算等</li><li><strong>系统可扩展性</strong>：采用微服务架构、水平扩展等方式提高系统可扩展性</li><li><strong>异常情况处理</strong>：完善的异常处理机制，包括补偿机制、降级策略等</li></ol><p>通过合理的技术选型和架构设计，电商系统可以更好地应对各种复杂场景，提供稳定、高效的服务。</p>]]></content>
      
      
      <categories>
          
          <category> 消息队列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
